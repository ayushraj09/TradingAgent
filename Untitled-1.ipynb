{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c23055",
   "metadata": {},
   "source": [
    "# Part 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7575d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Project root: /Users/ayushraj/Documents/Python/FinRL\n"
     ]
    }
   ],
   "source": [
    "# System setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get relative paths dynamically\n",
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115fec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import threading\n",
    "import tempfile\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add FinRL to path\n",
    "finrl_path = PROJECT_ROOT / 'FinRL'\n",
    "if str(finrl_path) not in sys.path:\n",
    "    sys.path.append(str(finrl_path))\n",
    "\n",
    "# FinRL imports\n",
    "from finrl.config import INDICATORS\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.meta.data_processors.processor_alpaca import AlpacaProcessor\n",
    "\n",
    "# DRL imports\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Alpaca imports\n",
    "import alpaca_trade_api as tradeapi\n",
    "\n",
    "# Explainability imports\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "from explainable_drl.explainable_agent import ExplainableAgent\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ac452",
   "metadata": {},
   "source": [
    "# Part 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d77b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì API credentials loaded\n",
      "‚úì Tickers: 30\n",
      "‚úì Technical indicators: 8\n"
     ]
    }
   ],
   "source": [
    "# Load API credentials from .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(PROJECT_ROOT / '.env')\n",
    "\n",
    "API_KEY = os.getenv('ALPACA_API_KEY')\n",
    "API_SECRET = os.getenv('ALPACA_API_SECRET')\n",
    "API_BASE_URL = os.getenv('ALPACA_API_BASE_URL', 'https://paper-api.alpaca.markets')\n",
    "\n",
    "# DOW 30 tickers (excluding VIXY)\n",
    "TICKERS = [\n",
    "    'AAPL', 'AMGN', 'AXP', 'BA', 'CAT', 'CRM', 'CSCO', 'CVX', 'DIS', 'DOW',\n",
    "    'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM',\n",
    "    'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WBA', 'WMT'\n",
    "]\n",
    "\n",
    "TECH_INDICATORS = INDICATORS\n",
    "\n",
    "print(f\"‚úì API credentials loaded\")\n",
    "print(f\"‚úì Tickers: {len(TICKERS)}\")\n",
    "print(f\"‚úì Technical indicators: {len(TECH_INDICATORS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70238af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã CONFIGURATION\n",
      "================================================================================\n",
      "Model: trained_models/agent_ppo.zip\n",
      "Output: production_paper_trading_results\n",
      "Data CSV: production_paper_trading_data.csv\n",
      "State dim: 301, Action dim: 30\n",
      "Initial cash: $1,000,000\n",
      "Fine-tune: Every 2h\n",
      "Lookback: 48h\n",
      "Explainability: True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Main configuration\n",
    "CONFIG = {\n",
    "    # Alpaca API\n",
    "    'API_KEY': API_KEY,\n",
    "    'API_SECRET': API_SECRET,\n",
    "    'API_BASE_URL': API_BASE_URL,\n",
    "    \n",
    "    # Tickers\n",
    "    'TICKERS': TICKERS,\n",
    "    'STOCK_DIM': len(TICKERS),\n",
    "    \n",
    "    # Model paths\n",
    "    'TRAINED_MODEL': 'trained_models/agent_ppo.zip',\n",
    "    'OUTPUT_DIR': 'production_paper_trading_results',\n",
    "    \n",
    "    # Trading parameters\n",
    "    'INITIAL_CASH': 1_000_000,\n",
    "    'HMAX': 100,\n",
    "    'MAX_STOCK': 100,\n",
    "    'TRANSACTION_COST_PCT': 0.001,\n",
    "    'REWARD_SCALING': 1e-4,\n",
    "    'TURBULENCE_THRESHOLD': 500,\n",
    "    'MIN_ACTION_THRESHOLD': 10,\n",
    "    \n",
    "    # Trading timing\n",
    "    'TIME_INTERVAL_MIN': 1,  # Trade every minute\n",
    "    'INITIAL_TRADE_DELAY_MIN': 15,  # Wait 15 min after market open\n",
    "    \n",
    "    # Fine-tuning parameters\n",
    "    'FINETUNE_INTERVAL_HOURS': 2,\n",
    "    'FINETUNE_LOOKBACK_HOURS': 48,\n",
    "    'FINETUNE_LR': 1e-5,\n",
    "    'FINETUNE_STEPS': 2000,\n",
    "    'VALIDATION_SPLIT': 0.2,\n",
    "    'ROLLBACK_THRESHOLD': 0.95,\n",
    "    \n",
    "    # Data storage\n",
    "    'DATA_CSV': 'production_paper_trading_data.csv',\n",
    "    \n",
    "    # Explainability\n",
    "    'ENABLE_EXPLANATIONS': True,\n",
    "    'SHAP_BACKGROUND_SAMPLES': 500,\n",
    "}\n",
    "\n",
    "# Calculate state dimensions (matches StockTradingEnv)\n",
    "# State: 1 (cash) + 30 (prices) + 30 (stocks) + 240 (tech indicators) = 301\n",
    "state_dim = 1 + 2 * CONFIG['STOCK_DIM'] + len(TECH_INDICATORS) * CONFIG['STOCK_DIM']\n",
    "action_dim = CONFIG['STOCK_DIM']\n",
    "\n",
    "CONFIG['state_dim'] = state_dim\n",
    "CONFIG['action_dim'] = action_dim\n",
    "\n",
    "print(\"\\nüìã CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {CONFIG['TRAINED_MODEL']}\")\n",
    "print(f\"Output: {CONFIG['OUTPUT_DIR']}\")\n",
    "print(f\"Data CSV: {CONFIG['DATA_CSV']}\")\n",
    "print(f\"State dim: {state_dim}, Action dim: {action_dim}\")\n",
    "print(f\"Initial cash: ${CONFIG['INITIAL_CASH']:,}\")\n",
    "print(f\"Fine-tune: Every {CONFIG['FINETUNE_INTERVAL_HOURS']}h\")\n",
    "print(f\"Lookback: {CONFIG['FINETUNE_LOOKBACK_HOURS']}h\")\n",
    "print(f\"Explainability: {CONFIG['ENABLE_EXPLANATIONS']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f8f4e9",
   "metadata": {},
   "source": [
    "# Part 3: Data Management Functions\n",
    "\n",
    "CSV-based data storage with historical backfill and real-time updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a62da1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data management functions defined\n"
     ]
    }
   ],
   "source": [
    "def init_data_csv(csv_path):\n",
    "    \"\"\"Initialize CSV file for data collection.\"\"\"\n",
    "    if not Path(csv_path).exists():\n",
    "        # Create with all required columns\n",
    "        columns = ['date', 'tic', 'open', 'high', 'low', 'close', 'volume'] + TECH_INDICATORS\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"‚úì Initialized CSV: {csv_path}\")\n",
    "    else:\n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úì CSV exists: {csv_path} ({len(existing_df):,} records)\")\n",
    "\n",
    "\n",
    "def fetch_historical_data_to_csv(alpaca_api, csv_path, required_days=2):\n",
    "    \"\"\"\n",
    "    Fetch historical 1-min data from Alpaca and populate CSV.\n",
    "    Uses last 2 completed trading days to avoid API restrictions.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì• Fetching historical data for {required_days} trading days...\")\n",
    "    \n",
    "    # Check if CSV already has sufficient data\n",
    "    if Path(csv_path).exists():\n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "        if len(existing_df) > 0:\n",
    "            existing_df['date'] = pd.to_datetime(existing_df['date'])\n",
    "            span_hours = (existing_df['date'].max() - existing_df['date'].min()).total_seconds() / 3600\n",
    "            age_hours = (datetime.utcnow() - existing_df['date'].max()).total_seconds() / 3600\n",
    "            \n",
    "            if span_hours >= 12 and age_hours < 24:  # At least 12h of recent data\n",
    "                print(f\"‚úì CSV has {span_hours:.1f}h of data ({age_hours:.1f}h old)\")\n",
    "                print(f\"  Skipping historical fetch\")\n",
    "                return\n",
    "    \n",
    "    try:\n",
    "        # Calculate date range\n",
    "        today = datetime.utcnow().date()\n",
    "        end_date = today - timedelta(days=1)  # Yesterday\n",
    "        start_date = end_date - timedelta(days=2)  # 2 days before\n",
    "        \n",
    "        print(f\"  Download range: {start_date} to {end_date}\")\n",
    "        \n",
    "        # Initialize Alpaca processor\n",
    "        alpaca_processor = AlpacaProcessor(\n",
    "            API_KEY=CONFIG['API_KEY'],\n",
    "            API_SECRET=CONFIG['API_SECRET'],\n",
    "            API_BASE_URL=CONFIG['API_BASE_URL']\n",
    "        )\n",
    "        \n",
    "        # Download raw data\n",
    "        print(f\"  Downloading...\")\n",
    "        df_raw = alpaca_processor.download_data(\n",
    "            start_date=start_date.strftime('%Y-%m-%d'),\n",
    "            end_date=end_date.strftime('%Y-%m-%d'),\n",
    "            ticker_list=TICKERS,\n",
    "            time_interval='1Min'\n",
    "        )\n",
    "        \n",
    "        if df_raw is None or len(df_raw) == 0:\n",
    "            print(\"  ‚ö†Ô∏è  No data returned from Alpaca\")\n",
    "            return\n",
    "        \n",
    "        print(f\"  ‚úì Downloaded: {len(df_raw):,} records\")\n",
    "        \n",
    "        # Process data\n",
    "        if 'date' in df_raw.columns:\n",
    "            df_raw.rename(columns={'date': 'timestamp'}, inplace=True)\n",
    "        \n",
    "        df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'], utc=True, errors='coerce')\n",
    "        \n",
    "        # Set processor attributes for clean_data\n",
    "        alpaca_processor.start = start_date.strftime('%Y-%m-%d')\n",
    "        alpaca_processor.end = end_date.strftime('%Y-%m-%d')\n",
    "        alpaca_processor.time_interval = '1Min'\n",
    "        \n",
    "        # Clean and add indicators\n",
    "        df_clean = alpaca_processor.clean_data(df_raw)\n",
    "        df_clean = df_clean.sort_values(by=['timestamp', 'tic']).reset_index(drop=True)\n",
    "        df_clean = alpaca_processor.add_technical_indicator(df_clean, TECH_INDICATORS)\n",
    "        df_clean = df_clean.ffill().bfill()\n",
    "        \n",
    "        print(f\"  ‚úì Processed: {len(df_clean):,} records\")\n",
    "        \n",
    "        # Convert to timezone-naive\n",
    "        df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])\n",
    "        if df_clean['timestamp'].dt.tz is not None:\n",
    "            df_clean['timestamp'] = df_clean['timestamp'].dt.tz_localize(None)\n",
    "        \n",
    "        df_clean.rename(columns={'timestamp': 'date'}, inplace=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        required_cols = ['date', 'tic', 'open', 'high', 'low', 'close', 'volume'] + TECH_INDICATORS\n",
    "        df_clean = df_clean[required_cols]\n",
    "        df_clean.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Saved {len(df_clean):,} records to {csv_path}\")\n",
    "        print(f\"   Date range: {df_clean['date'].min()} to {df_clean['date'].max()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed to fetch historical data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def append_latest_data_to_csv(alpaca_processor, csv_path):\n",
    "    \"\"\"\n",
    "    Fetch latest 1-min data from Alpaca and append to CSV.\n",
    "    Returns: DataFrame of new data (or None if failed/duplicate)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch latest data with technical indicators\n",
    "        price, tech, turbulence = alpaca_processor.fetch_latest_data(\n",
    "            ticker_list=TICKERS,\n",
    "            time_interval='1Min',\n",
    "            tech_indicator_list=TECH_INDICATORS\n",
    "        )\n",
    "        \n",
    "        if price is None:\n",
    "            print(\"‚ö†Ô∏è  No data fetched\")\n",
    "            return None\n",
    "        \n",
    "        # Get current timestamp (rounded to minute)\n",
    "        current_time = datetime.utcnow().replace(second=0, microsecond=0)\n",
    "        \n",
    "        # Build DataFrame\n",
    "        records = []\n",
    "        for i, ticker in enumerate(TICKERS):\n",
    "            record = {\n",
    "                'date': current_time,\n",
    "                'tic': ticker,\n",
    "                'open': price[i],  # Using close as proxy\n",
    "                'high': price[i],\n",
    "                'low': price[i],\n",
    "                'close': price[i],\n",
    "                'volume': 0,\n",
    "            }\n",
    "            \n",
    "            # Add tech indicators\n",
    "            for j, tech_name in enumerate(TECH_INDICATORS):\n",
    "                idx = i * len(TECH_INDICATORS) + j\n",
    "                record[tech_name] = tech[idx] if idx < len(tech) else 0\n",
    "            \n",
    "            records.append(record)\n",
    "        \n",
    "        df_new = pd.DataFrame(records)\n",
    "        \n",
    "        # Check for duplicates\n",
    "        if Path(csv_path).exists():\n",
    "            existing_df = pd.read_csv(csv_path)\n",
    "            if len(existing_df) > 0:\n",
    "                existing_df['date'] = pd.to_datetime(existing_df['date'])\n",
    "                last_timestamp = existing_df['date'].max()\n",
    "                \n",
    "                if current_time <= last_timestamp:\n",
    "                    print(f\"  ‚ö†Ô∏è  Data already exists for {current_time}\")\n",
    "                    return None\n",
    "        \n",
    "        # Append to CSV\n",
    "        df_new.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "        print(f\"  üíæ Appended {len(df_new)} records at {current_time}\")\n",
    "        \n",
    "        return df_new\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error appending data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_recent_data_from_csv(csv_path, hours=48):\n",
    "    \"\"\"Load last N hours of data from CSV.\"\"\"\n",
    "    if not Path(csv_path).exists():\n",
    "        print(f\"‚ö†Ô∏è  CSV not found: {csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"‚ö†Ô∏è  CSV is empty\")\n",
    "        return None\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Get last N hours\n",
    "    cutoff = datetime.utcnow() - timedelta(hours=hours)\n",
    "    df_filtered = df[df['date'] >= cutoff].copy()\n",
    "    \n",
    "    # Filter to only expected tickers\n",
    "    df_filtered = df_filtered[df_filtered['tic'].isin(TICKERS)]\n",
    "    \n",
    "    # Filter to complete timestamps (all 30 stocks)\n",
    "    df_filtered = df_filtered.sort_values(['date', 'tic']).reset_index(drop=True)\n",
    "    timestamp_counts = df_filtered.groupby('date')['tic'].count()\n",
    "    complete_timestamps = timestamp_counts[timestamp_counts == CONFIG['STOCK_DIM']].index\n",
    "    df_filtered = df_filtered[df_filtered['date'].isin(complete_timestamps)]\n",
    "    \n",
    "    if len(df_filtered) == 0:\n",
    "        print(\"‚ö†Ô∏è  No complete timestamps found\")\n",
    "        return None\n",
    "    \n",
    "    # Create day index\n",
    "    unique_dates = sorted(df_filtered['date'].unique())\n",
    "    date_to_day = {date: idx for idx, date in enumerate(unique_dates)}\n",
    "    df_filtered['day'] = df_filtered['date'].map(date_to_day)\n",
    "    \n",
    "    time_span = (df_filtered['date'].max() - df_filtered['date'].min()).total_seconds() / 3600\n",
    "    print(f\"‚úì Loaded {len(df_filtered):,} rows ({len(unique_dates)} timestamps, {time_span:.1f}h)\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "print(\"‚úì Data management functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8e0e7",
   "metadata": {},
   "source": [
    "# Part 4: Trading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94d28ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Trading helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_sign(ary, thresh):\n",
    "    \"\"\"Sigmoid transformation for turbulence.\"\"\"\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x * np.e)) - 0.5\n",
    "    return sigmoid(ary / thresh) * thresh\n",
    "\n",
    "\n",
    "def submit_order(alpaca, qty, stock, side, resp):\n",
    "    \"\"\"Submit order to Alpaca.\"\"\"\n",
    "    if qty > 0:\n",
    "        try:\n",
    "            alpaca.submit_order(stock, qty, side, \"market\", \"day\")\n",
    "            print(f\"    ‚úì {side.upper()} {qty} {stock}\")\n",
    "            resp.append(True)\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚úó {side.upper()} {qty} {stock} failed: {e}\")\n",
    "            resp.append(False)\n",
    "    else:\n",
    "        resp.append(True)\n",
    "\n",
    "\n",
    "def get_state_from_alpaca(alpaca_processor):\n",
    "    \"\"\"\n",
    "    Get current state from Alpaca (Production format: 301 features).\n",
    "    \n",
    "    State vector: [cash(1)] + [prices(30)] + [stocks(30)] + [tech_indicators(240)]\n",
    "    NO turbulence in state vector!\n",
    "    \n",
    "    Returns:\n",
    "        state, price, stocks, cash, turbulence, turbulence_bool, tech\n",
    "    \"\"\"\n",
    "    # Fetch latest data with technical indicators\n",
    "    price, tech, turbulence = alpaca_processor.fetch_latest_data(\n",
    "        ticker_list=TICKERS,\n",
    "        time_interval='1Min',\n",
    "        tech_indicator_list=TECH_INDICATORS\n",
    "    )\n",
    "    \n",
    "    # Determine turbulence threshold\n",
    "    turbulence_bool = 1 if turbulence >= CONFIG['TURBULENCE_THRESHOLD'] else 0\n",
    "    \n",
    "    # Scale tech indicators\n",
    "    tech_scaled = tech * 2 ** -7\n",
    "    \n",
    "    # Get current positions from Alpaca\n",
    "    alpaca = tradeapi.REST(\n",
    "        CONFIG['API_KEY'],\n",
    "        CONFIG['API_SECRET'],\n",
    "        CONFIG['API_BASE_URL'],\n",
    "        'v2'\n",
    "    )\n",
    "    \n",
    "    positions = alpaca.list_positions()\n",
    "    stocks = np.zeros(CONFIG['STOCK_DIM'])\n",
    "    for position in positions:\n",
    "        if position.symbol in TICKERS:\n",
    "            ind = TICKERS.index(position.symbol)\n",
    "            stocks[ind] = abs(int(float(position.qty)))\n",
    "    \n",
    "    # Get current cash\n",
    "    cash = float(alpaca.get_account().cash)\n",
    "    \n",
    "    # Build state vector (NO turbulence!)\n",
    "    # Model expects: 1 (cash) + 30 (prices) + 30 (stocks) + 240 (tech) = 301 features\n",
    "    amount = np.array(cash * (2 ** -12), dtype=np.float32)\n",
    "    scale = np.array(2 ** -6, dtype=np.float32)\n",
    "    \n",
    "    state = np.hstack((\n",
    "        amount,\n",
    "        price * scale,\n",
    "        stocks * scale,\n",
    "        tech_scaled,\n",
    "    )).astype(np.float32)\n",
    "    \n",
    "    # Handle NaN/Inf\n",
    "    state[np.isnan(state)] = 0.0\n",
    "    state[np.isinf(state)] = 0.0\n",
    "    \n",
    "    return state, price, stocks, cash, turbulence, turbulence_bool, tech\n",
    "\n",
    "\n",
    "print(\"‚úì Trading helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659ab94",
   "metadata": {},
   "source": [
    "# Part 5: Fine-Tuning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f383f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fine-tuning functions defined\n"
     ]
    }
   ],
   "source": [
    "def create_env(df, config):\n",
    "    \"\"\"Create StockTradingEnv for training/evaluation.\"\"\"\n",
    "    state_space = 1 + 2 * config['STOCK_DIM'] + len(TECH_INDICATORS) * config['STOCK_DIM']\n",
    "    \n",
    "    df_indexed = df.copy()\n",
    "    df_indexed = df_indexed.sort_values(['day', 'tic'])\n",
    "    df_indexed = df_indexed.set_index('day')\n",
    "    \n",
    "    env = StockTradingEnv(\n",
    "        df=df_indexed,\n",
    "        stock_dim=config['STOCK_DIM'],\n",
    "        hmax=config['HMAX'],\n",
    "        initial_amount=config['INITIAL_CASH'],\n",
    "        num_stock_shares=[0] * config['STOCK_DIM'],\n",
    "        buy_cost_pct=[config['TRANSACTION_COST_PCT']] * config['STOCK_DIM'],\n",
    "        sell_cost_pct=[config['TRANSACTION_COST_PCT']] * config['STOCK_DIM'],\n",
    "        reward_scaling=config['REWARD_SCALING'],\n",
    "        state_space=state_space,\n",
    "        action_space=config['STOCK_DIM'],\n",
    "        tech_indicator_list=TECH_INDICATORS,\n",
    "        print_verbosity=100000,\n",
    "    )\n",
    "    \n",
    "    return DummyVecEnv([lambda: env])\n",
    "\n",
    "\n",
    "def evaluate_model_on_df(model, df, config):\n",
    "    \"\"\"Evaluate model performance on DataFrame.\"\"\"\n",
    "    env = create_env(df, config)\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward[0]\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def finetune_model_with_validation(model, csv_path, config):\n",
    "    \"\"\"\n",
    "    Fine-tune model using recent CSV data with validation.\n",
    "    Returns: (model, result_dict)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINE-TUNING MODEL\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load data\n",
    "    df = load_recent_data_from_csv(csv_path, hours=config['FINETUNE_LOOKBACK_HOURS'])\n",
    "    \n",
    "    if df is None or len(df) < 100:\n",
    "        print(\"‚úó Insufficient data for fine-tuning\")\n",
    "        return model, None\n",
    "    \n",
    "    # Get unique dates\n",
    "    unique_dates = sorted(df['date'].unique())\n",
    "    \n",
    "    # Need at least 10 timestamps for meaningful split\n",
    "    if len(unique_dates) < 10:\n",
    "        print(f\"‚úó Need at least 10 timestamps (have {len(unique_dates)})\")\n",
    "        return model, None\n",
    "    \n",
    "    # Split train/validation\n",
    "    split_idx = max(len(unique_dates) - 2, int(len(unique_dates) * (1 - config['VALIDATION_SPLIT'])))\n",
    "    \n",
    "    train_df = df[df['date'].isin(unique_dates[:split_idx])].copy()\n",
    "    val_df = df[df['date'].isin(unique_dates[split_idx:])].copy()\n",
    "    \n",
    "    # Reset day indices\n",
    "    train_dates = sorted(train_df['date'].unique())\n",
    "    val_dates = sorted(val_df['date'].unique())\n",
    "    \n",
    "    train_date_to_day = {date: idx for idx, date in enumerate(train_dates)}\n",
    "    val_date_to_day = {date: idx for idx, date in enumerate(val_dates)}\n",
    "    \n",
    "    train_df['day'] = train_df['date'].map(train_date_to_day)\n",
    "    val_df['day'] = val_df['date'].map(val_date_to_day)\n",
    "    \n",
    "    print(f\"  Train: {len(train_df):,} rows ({len(train_dates)} timestamps)\")\n",
    "    print(f\"  Val: {len(val_df):,} rows ({len(val_dates)} timestamps)\")\n",
    "    \n",
    "    # Evaluate original\n",
    "    print(f\"  Evaluating original model...\")\n",
    "    original_score = evaluate_model_on_df(model, val_df, config)\n",
    "    print(f\"  Original score: {original_score:.2f}\")\n",
    "    \n",
    "    # Clone model\n",
    "    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp:\n",
    "        tmp_path = tmp.name\n",
    "        model.save(tmp_path)\n",
    "        model_ft = PPO.load(tmp_path)\n",
    "    os.remove(tmp_path)\n",
    "    \n",
    "    # Fine-tune\n",
    "    print(f\"  Fine-tuning ({config['FINETUNE_STEPS']} steps, lr={config['FINETUNE_LR']})...\")\n",
    "    model_ft.learning_rate = config['FINETUNE_LR']\n",
    "    ft_env = create_env(train_df, config)\n",
    "    model_ft.set_env(ft_env)\n",
    "    model_ft.learn(\n",
    "        total_timesteps=config['FINETUNE_STEPS'],\n",
    "        reset_num_timesteps=False,\n",
    "        progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate fine-tuned\n",
    "    print(f\"  Evaluating fine-tuned model...\")\n",
    "    finetuned_score = evaluate_model_on_df(model_ft, val_df, config)\n",
    "    print(f\"  Fine-tuned score: {finetuned_score:.2f}\")\n",
    "    \n",
    "    # Decision\n",
    "    threshold = original_score * config['ROLLBACK_THRESHOLD']\n",
    "    accepted = finetuned_score >= threshold\n",
    "    improvement = ((finetuned_score - original_score) / original_score * 100) if original_score != 0 else 0\n",
    "    \n",
    "    result = {\n",
    "        'timestamp': datetime.utcnow(),\n",
    "        'original_score': original_score,\n",
    "        'finetuned_score': finetuned_score,\n",
    "        'threshold': threshold,\n",
    "        'accepted': accepted,\n",
    "        'improvement_pct': improvement,\n",
    "        'train_records': len(train_df),\n",
    "        'val_records': len(val_df),\n",
    "    }\n",
    "    \n",
    "    if accepted:\n",
    "        print(f\"‚úÖ ACCEPTED (+{improvement:.2f}%)\")\n",
    "        return model_ft, result\n",
    "    else:\n",
    "        print(f\"‚ùå REJECTED ({improvement:.2f}%)\")\n",
    "        return model, result\n",
    "\n",
    "\n",
    "print(\"‚úì Fine-tuning functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363eae3",
   "metadata": {},
   "source": [
    "# Part 6: Main Trading Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66934f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ProductionPaperTrading class defined\n"
     ]
    }
   ],
   "source": [
    "class ProductionPaperTrading:\n",
    "    \"\"\"\n",
    "    Production paper trading with explainability and fine-tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, model_path):\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize Alpaca\n",
    "        self.alpaca_processor = AlpacaProcessor(\n",
    "            API_KEY=config['API_KEY'],\n",
    "            API_SECRET=config['API_SECRET'],\n",
    "            API_BASE_URL=config['API_BASE_URL']\n",
    "        )\n",
    "        \n",
    "        self.alpaca = tradeapi.REST(\n",
    "            config['API_KEY'],\n",
    "            config['API_SECRET'],\n",
    "            config['API_BASE_URL'],\n",
    "            'v2'\n",
    "        )\n",
    "        \n",
    "        # Load PPO model\n",
    "        print(f\"ü§ñ Loading PPO model: {model_path}\")\n",
    "        ppo_model = PPO.load(model_path)\n",
    "        print(\"‚úì PPO model loaded\")\n",
    "        \n",
    "        # Wrap with ExplainableAgent if enabled\n",
    "        if config['ENABLE_EXPLANATIONS']:\n",
    "            self.model = ExplainableAgent(\n",
    "                ppo_model, \n",
    "                stock_dim=config['STOCK_DIM'],\n",
    "                hmax=config['MAX_STOCK']\n",
    "            )\n",
    "            print(\"‚úì Wrapped with ExplainableAgent\")\n",
    "        else:\n",
    "            self.model = ppo_model\n",
    "            print(\"‚úì Using PPO without explanations\")\n",
    "        \n",
    "        # Initialize state\n",
    "        self.tickers = config['TICKERS']\n",
    "        self.stocks_cd = np.zeros(config['STOCK_DIM'])\n",
    "        \n",
    "        # Fine-tuning tracking\n",
    "        self.last_finetune = datetime.utcnow() - timedelta(hours=config['FINETUNE_INTERVAL_HOURS'])\n",
    "        self.finetune_history = []\n",
    "        self.trading_history = []\n",
    "        self.cycle = 0\n",
    "        self.model_version = 'original'\n",
    "        self.finetune_count = 0\n",
    "        \n",
    "        # Initialize data CSV\n",
    "        print(f\"\\nüìä Initializing data collection...\")\n",
    "        init_data_csv(config['DATA_CSV'])\n",
    "        \n",
    "        # Fetch historical data\n",
    "        fetch_historical_data_to_csv(\n",
    "            self.alpaca,\n",
    "            config['DATA_CSV'],\n",
    "            required_days=2\n",
    "        )\n",
    "        \n",
    "        # Train explainers if enabled\n",
    "        if config['ENABLE_EXPLANATIONS']:\n",
    "            self._train_explainers()\n",
    "        \n",
    "        print(\"‚úì ProductionPaperTrading initialized\")\n",
    "    \n",
    "    def _train_explainers(self):\n",
    "        \"\"\"Train SHAP and LIME explainers on historical data.\"\"\"\n",
    "        print(f\"\\nüîç Training SHAP + LIME explainers...\")\n",
    "        \n",
    "        try:\n",
    "            # Load historical data for explainer training\n",
    "            df_hist = load_recent_data_from_csv(\n",
    "                self.config['DATA_CSV'],\n",
    "                hours=self.config['FINETUNE_LOOKBACK_HOURS']\n",
    "            )\n",
    "            \n",
    "            if df_hist is None or len(df_hist) < 100:\n",
    "                print(\"‚ö†Ô∏è  Insufficient data for explainer training\")\n",
    "                print(\"   Will train explainers after collecting more data\")\n",
    "                return\n",
    "            \n",
    "            # Save to temp file for training\n",
    "            temp_path = Path(self.config['OUTPUT_DIR']) / 'temp_explainer_data.csv'\n",
    "            temp_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            df_hist.to_csv(temp_path, index=False)\n",
    "            \n",
    "            # Train explainers\n",
    "            self.model.train_explainers(\n",
    "                train_data_path=str(temp_path),\n",
    "                n_samples=self.config['SHAP_BACKGROUND_SAMPLES']\n",
    "            )\n",
    "            \n",
    "            # Clean up\n",
    "            temp_path.unlink()\n",
    "            \n",
    "            print(\"‚úì SHAP + LIME explainers trained\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Failed to train explainers: {e}\")\n",
    "            print(\"   Will continue without explainability\")\n",
    "    \n",
    "    def execute_trade(self):\n",
    "        \"\"\"\n",
    "        Execute trading decision with optional explanations.\n",
    "        \"\"\"\n",
    "        # Get current state (301 features)\n",
    "        state, price, stocks, cash, turbulence, turbulence_bool, tech = get_state_from_alpaca(\n",
    "            self.alpaca_processor\n",
    "        )\n",
    "        \n",
    "        # Get prediction\n",
    "        if self.config['ENABLE_EXPLANATIONS'] and hasattr(self.model, 'predict_with_explanation'):\n",
    "            # Check if explainers are trained\n",
    "            if self.model.shap_explainer is not None and self.model.lime_explainer is not None:\n",
    "                result = self.model.predict_with_explanation(\n",
    "                    state.reshape(1, -1),\n",
    "                    explain_method='all'\n",
    "                )\n",
    "                action = result['action']  # Already scaled\n",
    "                explanations = result['methods']\n",
    "            else:\n",
    "                # Fallback to regular predict\n",
    "                action, _ = self.model.predict(state.reshape(1, -1), deterministic=True)\n",
    "                action = action[0]\n",
    "                action = (action * self.config['MAX_STOCK']).astype(int)\n",
    "                explanations = None\n",
    "        else:\n",
    "            action, _ = self.model.predict(state.reshape(1, -1), deterministic=True)\n",
    "            action = action[0]\n",
    "            action = (action * self.config['MAX_STOCK']).astype(int)\n",
    "            explanations = None\n",
    "        \n",
    "        # Update cooldown\n",
    "        self.stocks_cd += 1\n",
    "        \n",
    "        # Execute trades\n",
    "        decisions = []\n",
    "        \n",
    "        if turbulence_bool == 0:\n",
    "            # Normal trading\n",
    "            min_action = self.config['MIN_ACTION_THRESHOLD']\n",
    "            threads = []\n",
    "            \n",
    "            # SELL orders\n",
    "            sell_indices = np.where(action < -min_action)[0]\n",
    "            for index in sell_indices:\n",
    "                sell_num_shares = min(stocks[index], -action[index])\n",
    "                qty = abs(int(sell_num_shares))\n",
    "                respSO = []\n",
    "                \n",
    "                t = threading.Thread(\n",
    "                    target=lambda q=qty, s=self.tickers[index]: submit_order(\n",
    "                        self.alpaca, q, s, 'sell', respSO\n",
    "                    )\n",
    "                )\n",
    "                t.start()\n",
    "                threads.append(t)\n",
    "                self.stocks_cd[index] = 0\n",
    "                \n",
    "                if qty > 0:\n",
    "                    decisions.append({\n",
    "                        'ticker': self.tickers[index],\n",
    "                        'action': 'SELL',\n",
    "                        'qty': qty,\n",
    "                        'price': price[index],\n",
    "                    })\n",
    "            \n",
    "            # Wait for sells\n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            \n",
    "            # Update cash\n",
    "            cash = float(self.alpaca.get_account().cash)\n",
    "            \n",
    "            # BUY orders\n",
    "            threads = []\n",
    "            buy_indices = np.where(action > min_action)[0]\n",
    "            for index in buy_indices:\n",
    "                tmp_cash = max(0, cash)\n",
    "                buy_num_shares = min(tmp_cash // price[index], abs(int(action[index])))\n",
    "                qty = abs(int(buy_num_shares)) if not np.isnan(buy_num_shares) else 0\n",
    "                respSO = []\n",
    "                \n",
    "                t = threading.Thread(\n",
    "                    target=lambda q=qty, s=self.tickers[index]: submit_order(\n",
    "                        self.alpaca, q, s, 'buy', respSO\n",
    "                    )\n",
    "                )\n",
    "                t.start()\n",
    "                threads.append(t)\n",
    "                self.stocks_cd[index] = 0\n",
    "                \n",
    "                if qty > 0:\n",
    "                    decisions.append({\n",
    "                        'ticker': self.tickers[index],\n",
    "                        'action': 'BUY',\n",
    "                        'qty': qty,\n",
    "                        'price': price[index],\n",
    "                    })\n",
    "            \n",
    "            # Wait for buys\n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            \n",
    "            # HOLD\n",
    "            hold_indices = np.where((action >= -min_action) & (action <= min_action))[0]\n",
    "            for index in hold_indices:\n",
    "                decisions.append({\n",
    "                    'ticker': self.tickers[index],\n",
    "                    'action': 'HOLD',\n",
    "                    'qty': 0,\n",
    "                    'price': price[index],\n",
    "                })\n",
    "        \n",
    "        else:\n",
    "            # High turbulence - liquidate all\n",
    "            print(\"  ‚ö†Ô∏è  HIGH TURBULENCE - Liquidating all positions\")\n",
    "            threads = []\n",
    "            positions = self.alpaca.list_positions()\n",
    "            \n",
    "            for position in positions:\n",
    "                side = 'sell' if position.side == 'long' else 'buy'\n",
    "                qty = abs(int(float(position.qty)))\n",
    "                respSO = []\n",
    "                \n",
    "                t = threading.Thread(\n",
    "                    target=lambda q=qty, sym=position.symbol, s=side: submit_order(\n",
    "                        self.alpaca, q, sym, s, respSO\n",
    "                    )\n",
    "                )\n",
    "                t.start()\n",
    "                threads.append(t)\n",
    "                \n",
    "                decisions.append({\n",
    "                    'ticker': position.symbol,\n",
    "                    'action': 'SELL_TURBULENCE',\n",
    "                    'qty': qty,\n",
    "                    'price': 0,\n",
    "                })\n",
    "            \n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            \n",
    "            self.stocks_cd[:] = 0\n",
    "        \n",
    "        # Get final values\n",
    "        cash = float(self.alpaca.get_account().cash)\n",
    "        portfolio_value = float(self.alpaca.get_account().last_equity)\n",
    "        \n",
    "        # Append new data to CSV\n",
    "        append_latest_data_to_csv(self.alpaca_processor, self.config['DATA_CSV'])\n",
    "        \n",
    "        return {\n",
    "            'decisions': decisions,\n",
    "            'portfolio_value': portfolio_value,\n",
    "            'cash': cash,\n",
    "            'turbulence': turbulence,\n",
    "            'turbulence_bool': turbulence_bool,\n",
    "            'explanations': explanations,\n",
    "        }\n",
    "    \n",
    "    def check_and_finetune(self):\n",
    "        \"\"\"Check if it's time to fine-tune and execute if needed.\"\"\"\n",
    "        current_time = datetime.utcnow()\n",
    "        time_since_finetune = (current_time - self.last_finetune).total_seconds() / 3600\n",
    "        \n",
    "        if time_since_finetune >= self.config['FINETUNE_INTERVAL_HOURS']:\n",
    "            print(f\"\\n‚è∞ Time to fine-tune (last: {time_since_finetune:.1f}h ago)\")\n",
    "            \n",
    "            self.model, ft_result = finetune_model_with_validation(\n",
    "                self.model,\n",
    "                self.config['DATA_CSV'],\n",
    "                self.config\n",
    "            )\n",
    "            \n",
    "            if ft_result:\n",
    "                self.finetune_history.append(ft_result)\n",
    "                self.last_finetune = current_time\n",
    "                \n",
    "                # Save results\n",
    "                output_dir = Path(self.config['OUTPUT_DIR'])\n",
    "                output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                pd.DataFrame(self.finetune_history).to_csv(\n",
    "                    output_dir / 'finetune_history.csv',\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                if ft_result['accepted']:\n",
    "                    self.finetune_count += 1\n",
    "                    self.model_version = f'finetuned_v{self.finetune_count}'\n",
    "                    \n",
    "                    # Save fine-tuned model\n",
    "                    model_path = output_dir / f'model_cycle_{self.cycle}.zip'\n",
    "                    self.model.save(str(model_path))\n",
    "                    print(f\"üíæ Saved fine-tuned model: {model_path}\")\n",
    "                    \n",
    "                    # Retrain explainers on updated model\n",
    "                    if self.config['ENABLE_EXPLANATIONS']:\n",
    "                        print(\"  Re-training explainers on fine-tuned model...\")\n",
    "                        self._train_explainers()\n",
    "    \n",
    "    def square_off_all_positions(self):\n",
    "        \"\"\"Liquidate all positions before market close.\"\"\"\n",
    "        print(\"\\nüîö Squaring off all positions...\")\n",
    "        positions = self.alpaca.list_positions()\n",
    "        \n",
    "        if len(positions) == 0:\n",
    "            print(\"   No positions to square off\")\n",
    "            return\n",
    "        \n",
    "        threads = []\n",
    "        for position in positions:\n",
    "            side = 'sell' if position.side == 'long' else 'buy'\n",
    "            qty = abs(int(float(position.qty)))\n",
    "            respSO = []\n",
    "            \n",
    "            t = threading.Thread(\n",
    "                target=lambda q=qty, sym=position.symbol, s=side: submit_order(\n",
    "                    self.alpaca, q, sym, s, respSO\n",
    "                )\n",
    "            )\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "        \n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        print(\"‚úì All positions squared off\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main trading loop.\"\"\"\n",
    "        # Wait for market to open\n",
    "        clock = self.alpaca.get_clock()\n",
    "        if not clock.is_open:\n",
    "            time_to_open = (clock.next_open.replace(tzinfo=timezone.utc) - \n",
    "                          clock.timestamp.replace(tzinfo=timezone.utc)).total_seconds()\n",
    "            print(f\"‚è∞ Market closed - waiting {int(time_to_open/60)} minutes...\")\n",
    "            time.sleep(time_to_open)\n",
    "        \n",
    "        # Wait initial delay after market open\n",
    "        print(f\"‚úÖ Market opened - waiting {self.config['INITIAL_TRADE_DELAY_MIN']} minutes...\")\n",
    "        time.sleep(self.config['INITIAL_TRADE_DELAY_MIN'] * 60)\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting paper trading (Model: {self.model_version})\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        output_dir = Path(self.config['OUTPUT_DIR'])\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                self.cycle += 1\n",
    "                \n",
    "                # Check market status\n",
    "                clock = self.alpaca.get_clock()\n",
    "                closing_time = clock.next_close.replace(tzinfo=timezone.utc).timestamp()\n",
    "                curr_time = clock.timestamp.replace(tzinfo=timezone.utc).timestamp()\n",
    "                time_to_close = closing_time - curr_time\n",
    "                \n",
    "                # Square off 15 min before close\n",
    "                if time_to_close < (15 * 60):\n",
    "                    self.square_off_all_positions()\n",
    "                    print(\"üîö Market closing soon - stopping trading\")\n",
    "                    break\n",
    "                \n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"CYCLE {self.cycle} - {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                print(f\"Model: {self.model_version} | Time to close: {int(time_to_close/60)} mins\")\n",
    "                print(f\"{'='*80}\")\n",
    "                \n",
    "                # Execute trade\n",
    "                trade_result = self.execute_trade()\n",
    "                \n",
    "                # Log trade\n",
    "                trade_log = {\n",
    "                    'timestamp': datetime.utcnow(),\n",
    "                    'cycle': self.cycle,\n",
    "                    'portfolio_value': trade_result['portfolio_value'],\n",
    "                    'cash': trade_result['cash'],\n",
    "                    'turbulence': trade_result['turbulence'],\n",
    "                    'num_trades': len([d for d in trade_result['decisions'] if d['action'] != 'HOLD']),\n",
    "                }\n",
    "                self.trading_history.append(trade_log)\n",
    "                \n",
    "                # Check and fine-tune\n",
    "                self.check_and_finetune()\n",
    "                \n",
    "                # Save trading history\n",
    "                pd.DataFrame(self.trading_history).to_csv(\n",
    "                    output_dir / 'trading_history.csv',\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                # Wait for next interval\n",
    "                time.sleep(self.config['TIME_INTERVAL_MIN'] * 60)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚ö†Ô∏è  Interrupted by user\")\n",
    "        \n",
    "        # Final summary\n",
    "        self._print_summary()\n",
    "    \n",
    "    def _print_summary(self):\n",
    "        \"\"\"Print final trading summary.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TRADING SESSION SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Total cycles: {self.cycle}\")\n",
    "        print(f\"Trading decisions: {len(self.trading_history)}\")\n",
    "        print(f\"Fine-tuning sessions: {len(self.finetune_history)}\")\n",
    "        \n",
    "        if self.trading_history:\n",
    "            final_value = self.trading_history[-1]['portfolio_value']\n",
    "            initial_value = self.trading_history[0]['portfolio_value']\n",
    "            total_return = (final_value - initial_value) / initial_value * 100\n",
    "            \n",
    "            print(f\"\\nüí∞ Portfolio Performance:\")\n",
    "            print(f\"   Initial: ${initial_value:,.2f}\")\n",
    "            print(f\"   Final: ${final_value:,.2f}\")\n",
    "            print(f\"   Return: {total_return:+.2f}%\")\n",
    "        \n",
    "        if self.finetune_history:\n",
    "            accepted = sum(1 for r in self.finetune_history if r['accepted'])\n",
    "            avg_improvement = np.mean([r['improvement_pct'] for r in self.finetune_history])\n",
    "            \n",
    "            print(f\"\\nüîÑ Fine-tuning:\")\n",
    "            print(f\"   Accepted: {accepted}/{len(self.finetune_history)}\")\n",
    "            print(f\"   Avg improvement: {avg_improvement:.2f}%\")\n",
    "        \n",
    "        print(f\"\\n‚úì Results saved to: {self.config['OUTPUT_DIR']}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "print(\"‚úì ProductionPaperTrading class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5ca02",
   "metadata": {},
   "source": [
    "# Part 7: Run Paper Trading\n",
    "\n",
    "Execute the paper trading loop with explainability and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce7627f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading PPO model: trained_models/agent_ppo.zip\n",
      "‚úì PPO model loaded\n",
      "‚úì ExplainableAgent initialized (SHAP + LIME only)\n",
      "‚úì Wrapped with ExplainableAgent\n",
      "\n",
      "üìä Initializing data collection...\n",
      "‚úì CSV exists: production_paper_trading_data.csv (22,620 records)\n",
      "\n",
      "üì• Fetching historical data for 2 trading days...\n",
      "  Download range: 2026-01-08 to 2026-01-10\n",
      "  Downloading...\n",
      "empty\n",
      "  ‚úì Downloaded: 22,584 records\n",
      "Data cleaning started\n",
      "align start and end dates\n",
      "produce full timestamp index\n",
      "Start processing tickers\n",
      "The price of the first row for ticker AAPL is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker AMGN is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker AXP is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker BA is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker CAT is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker CRM is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker CSCO is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker CVX is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker DIS is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker DOW is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker GS is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker HD is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker HON is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker IBM is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker INTC is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker JNJ is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker JPM is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker KO is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker MCD is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker MMM is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker MRK is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker MSFT is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker NKE is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker PG is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker TRV is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker UNH is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker V is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker VZ is NaN. It will be filled with the first valid price.\n",
      "The price of the first row for ticker WMT is NaN. It will be filled with the first valid price.\n",
      "ticker list complete\n",
      "Start concat and rename\n",
      "Data clean finished!\n",
      "Started adding Indicators\n",
      "Running Loop\n",
      "Restore Timestamps\n",
      "Finished adding Indicators\n",
      "  ‚úì Processed: 22,620 records\n",
      "‚úÖ Saved 22,620 records to production_paper_trading_data.csv\n",
      "   Date range: 2026-01-08 14:30:00 to 2026-01-09 20:59:00\n",
      "\n",
      "üîç Training SHAP + LIME explainers...\n",
      "‚ö†Ô∏è  No complete timestamps found\n",
      "‚ö†Ô∏è  Insufficient data for explainer training\n",
      "   Will train explainers after collecting more data\n",
      "‚úì ProductionPaperTrading initialized\n",
      "\n",
      "üöÄ Starting production paper trading\n",
      "   Trade interval: 1 minute(s)\n",
      "   Fine-tune interval: 2 hour(s)\n",
      "   Explainability: True\n",
      "   State dimension: 301 (Production format)\n",
      "‚è∞ Market closed - waiting 1246 minutes...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Explainability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mENABLE_EXPLANATIONS\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   State dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mstate_dim\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Production format)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mtrader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 342\u001b[39m, in \u001b[36mProductionPaperTrading.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    339\u001b[39m     time_to_open = (clock.next_open.replace(tzinfo=timezone.utc) - \n\u001b[32m    340\u001b[39m                   clock.timestamp.replace(tzinfo=timezone.utc)).total_seconds()\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚è∞ Market closed - waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time_to_open/\u001b[32m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_to_open\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# Wait initial delay after market open\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Market opened - waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mINITIAL_TRADE_DELAY_MIN\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize paper trading system\n",
    "trader = ProductionPaperTrading(\n",
    "    config=CONFIG,\n",
    "    model_path=CONFIG['TRAINED_MODEL']\n",
    ")\n",
    "\n",
    "# Start trading loop\n",
    "print(\"\\nüöÄ Starting production paper trading\")\n",
    "print(f\"   Trade interval: {CONFIG['TIME_INTERVAL_MIN']} minute(s)\")\n",
    "print(f\"   Fine-tune interval: {CONFIG['FINETUNE_INTERVAL_HOURS']} hour(s)\")\n",
    "print(f\"   Explainability: {CONFIG['ENABLE_EXPLANATIONS']}\")\n",
    "print(f\"   State dimension: {CONFIG['state_dim']} (Production format)\")\n",
    "\n",
    "trader.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7c8b0",
   "metadata": {},
   "source": [
    "# Part 8: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917fcc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Load results\n",
    "output_dir = Path(CONFIG['OUTPUT_DIR'])\n",
    "trading_history_path = output_dir / 'trading_history.csv'\n",
    "finetune_history_path = output_dir / 'finetune_history.csv'\n",
    "\n",
    "if trading_history_path.exists():\n",
    "    df_trading = pd.read_csv(trading_history_path)\n",
    "    df_trading['timestamp'] = pd.to_datetime(df_trading['timestamp'])\n",
    "    \n",
    "    # Plot portfolio value\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Portfolio value over time\n",
    "    axes[0].plot(df_trading['timestamp'], df_trading['portfolio_value'], \n",
    "                 linewidth=2, color='blue', marker='o', markersize=4)\n",
    "    axes[0].set_title('Portfolio Value Over Time', fontsize=16, fontweight='bold')\n",
    "    axes[0].set_xlabel('Time', fontsize=12)\n",
    "    axes[0].set_ylabel('Portfolio Value ($)', fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))\n",
    "    \n",
    "    # Cash over time\n",
    "    axes[1].plot(df_trading['timestamp'], df_trading['cash'], \n",
    "                 linewidth=2, color='green', marker='o', markersize=4)\n",
    "    axes[1].set_title('Cash Balance Over Time', fontsize=16, fontweight='bold')\n",
    "    axes[1].set_xlabel('Time', fontsize=12)\n",
    "    axes[1].set_ylabel('Cash ($)', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'trading_performance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance metrics\n",
    "    initial_value = df_trading['portfolio_value'].iloc[0]\n",
    "    final_value = df_trading['portfolio_value'].iloc[-1]\n",
    "    total_return = (final_value - initial_value) / initial_value * 100\n",
    "    max_value = df_trading['portfolio_value'].max()\n",
    "    min_value = df_trading['portfolio_value'].min()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Initial Portfolio Value: ${initial_value:,.2f}\")\n",
    "    print(f\"Final Portfolio Value:   ${final_value:,.2f}\")\n",
    "    print(f\"Total Return:            {total_return:+.2f}%\")\n",
    "    print(f\"Max Value:               ${max_value:,.2f}\")\n",
    "    print(f\"Min Value:               ${min_value:,.2f}\")\n",
    "    print(f\"Total Cycles:            {len(df_trading)}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No trading history found\")\n",
    "\n",
    "# Fine-tuning analysis\n",
    "if finetune_history_path.exists():\n",
    "    df_finetune = pd.read_csv(finetune_history_path)\n",
    "    df_finetune['timestamp'] = pd.to_datetime(df_finetune['timestamp'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Performance improvement bar chart\n",
    "    colors = ['green' if x else 'red' for x in df_finetune['accepted']]\n",
    "    bars = ax.bar(range(len(df_finetune)), df_finetune['improvement_pct'], \n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax.set_title('Fine-Tuning Performance Improvement', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Fine-Tuning Session', fontsize=12)\n",
    "    ax.set_ylabel('Improvement (%)', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add labels\n",
    "    for i, (bar, acc) in enumerate(zip(bars, df_finetune['accepted'])):\n",
    "        height = bar.get_height()\n",
    "        label = '‚úì' if acc else '‚úó'\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                label, ha='center', va='bottom' if height > 0 else 'top',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'finetune_performance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Fine-tuning summary\n",
    "    accepted_count = df_finetune['accepted'].sum()\n",
    "    total_count = len(df_finetune)\n",
    "    avg_improvement = df_finetune['improvement_pct'].mean()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINE-TUNING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Sessions:       {total_count}\")\n",
    "    print(f\"Accepted:             {accepted_count} ({accepted_count/total_count*100:.1f}%)\")\n",
    "    print(f\"Rejected:             {total_count - accepted_count}\")\n",
    "    print(f\"Avg Improvement:      {avg_improvement:+.2f}%\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No fine-tuning history found\")\n",
    "\n",
    "print(f\"\\n‚úì All results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93fdb1e",
   "metadata": {},
   "source": [
    "# Part 9: Explainability Analysis\n",
    "\n",
    "Analyze SHAP and LIME explanations if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98604fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have explanation data in trading history\n",
    "if trading_history_path.exists():\n",
    "    df_trading = pd.read_csv(trading_history_path)\n",
    "    \n",
    "    print(\"\\nüìä EXPLAINABILITY SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'explanations' in df_trading.columns:\n",
    "        print(\"‚úì Explanation data available\")\n",
    "        print(f\"  Total trading cycles with explanations: {len(df_trading)}\")\n",
    "        print(\"\\nNote: Detailed SHAP/LIME feature importance is logged during trading.\")\n",
    "        print(\"      Check console output for decision justifications.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No explanation data found in trading history\")\n",
    "        print(\"   This is normal - explanations are logged separately during trading\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìà TRADING STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'num_trades' in df_trading.columns:\n",
    "        total_trades = df_trading['num_trades'].sum()\n",
    "        avg_trades_per_cycle = df_trading['num_trades'].mean()\n",
    "        max_trades = df_trading['num_trades'].max()\n",
    "        \n",
    "        print(f\"Total trades executed: {total_trades}\")\n",
    "        print(f\"Avg trades per cycle: {avg_trades_per_cycle:.1f}\")\n",
    "        print(f\"Max trades in one cycle: {max_trades}\")\n",
    "    \n",
    "    if 'turbulence' in df_trading.columns:\n",
    "        high_turbulence_cycles = (df_trading['turbulence'] >= CONFIG['TURBULENCE_THRESHOLD']).sum()\n",
    "        print(f\"\\nHigh turbulence events: {high_turbulence_cycles}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No trading history available for analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
