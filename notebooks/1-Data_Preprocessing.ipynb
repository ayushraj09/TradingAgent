{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get the directory of this notebook\n",
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "\n",
    "# Load .env file\n",
    "env_path = os.path.join(notebook_dir, '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Verify loaded\n",
    "print(f\"✓ Environment variables loaded from: {env_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc584e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get relative paths\n",
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "# Install FinRL package\n",
    "%pip install -e {project_root} -q\n",
    "\n",
    "# Add to path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import finrl\n",
    "print(f'Using finrl from: {os.path.dirname(finrl.__file__)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.config import INDICATORS\n",
    "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import yfinance as yf\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl import config_tickers\n",
    "from finrl.config import INDICATORS\n",
    "from finrl.config import *\n",
    "import itertools\n",
    "\n",
    "from finrl.meta.data_processor import DataProcessor\n",
    "from finrl.meta.data_processors.processor_alpaca import AlpacaProcessor\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API credentials from environment\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "API_SECRET = os.getenv('API_SECRET')\n",
    "API_BASE_URL = os.getenv('API_BASE_URL')\n",
    "data_url = os.getenv('DATA_URL')\n",
    "\n",
    "# Configuration from environment\n",
    "API_RETRY_WAIT = int(os.getenv('API_RETRY_WAIT', '120'))\n",
    "API_REQUEST_SLEEP = int(os.getenv('API_REQUEST_SLEEP', '50'))\n",
    "\n",
    "# File paths (relative to notebook directory)\n",
    "BUFFER_CSV_PATH = os.path.join(notebook_dir, os.getenv('BUFFER_CSV_PATH', 'buffer.csv'))\n",
    "OUTPUT_CSV_PATH = os.path.join(notebook_dir, os.getenv('OUTPUT_CSV_PATH', 'ou.csv'))\n",
    "AFTER_CLEAN_DATA_CSV = os.path.join(notebook_dir, os.getenv('AFTER_CLEAN_DATA_CSV', 'after_clean_data.csv'))\n",
    "TRAIN_DATA_CSV = os.path.join(notebook_dir, os.getenv('TRAIN_DATA_CSV', 'train_data.csv'))\n",
    "TRADE_DATA_CSV = os.path.join(notebook_dir, os.getenv('TRADE_DATA_CSV', 'trade_data.csv'))\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  API Base URL: {API_BASE_URL}\")\n",
    "print(f\"  Data output path: {OUTPUT_CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Data Processor\n",
    "DP = DataProcessor(\n",
    "    data_source='alpaca',\n",
    "    API_KEY=API_KEY, \n",
    "    API_SECRET=API_SECRET, \n",
    "    API_BASE_URL=API_BASE_URL\n",
    ")\n",
    "\n",
    "print(\"✓ DataProcessor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c57a24",
   "metadata": {},
   "source": [
    "## 1. Data Download\n",
    "\n",
    "Download minute-level data from Alpaca API for Dow 30 stocks (excluding WBA and TRV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ticker list (Dow 30, excluding problematic tickers)\n",
    "ticker_list = [\n",
    "    \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CSCO\", \"CVX\", \"GS\", \"HD\", \"HON\",\n",
    "    \"IBM\", \"INTC\", \"JNJ\", \"KO\", \"JPM\", \"MCD\", \"MMM\", \"MRK\", \"MSFT\", \"NKE\",\n",
    "    \"PG\", \"UNH\", \"V\", \"VZ\", \"WMT\", \"DIS\", \"DOW\", \"CRM\"\n",
    "]\n",
    "\n",
    "# Months to download (adjust as needed)\n",
    "MONTHS_NEEDING_DOWNLOAD = [\n",
    "    ('2024-07-01', '2024-08-01'),\n",
    "    ('2024-08-02', '2024-09-02'),\n",
    "    ('2024-09-03', '2024-10-03'),\n",
    "    ('2024-10-04', '2024-11-04'),\n",
    "    ('2024-11-05', '2024-12-05'),\n",
    "    ('2024-12-06', '2025-01-06'),\n",
    "    ('2025-01-07', '2025-02-07'),\n",
    "    ('2025-02-08', '2025-03-08'),\n",
    "    ('2025-03-09', '2025-04-09'),\n",
    "    ('2025-04-10', '2025-05-10'),\n",
    "    ('2025-05-11', '2025-06-11'),\n",
    "    ('2025-06-12', '2025-07-12'),\n",
    "    ('2025-07-13', '2025-08-13'),\n",
    "    ('2025-08-14', '2025-09-14'),\n",
    "    ('2025-09-15', '2025-10-15'),\n",
    "    ('2025-10-16', '2025-11-09'),\n",
    "]\n",
    "\n",
    "print(f\"✓ Configuration ready\")\n",
    "print(f\"  Tickers: {len(ticker_list)}\")\n",
    "print(f\"  Months to download: {len(MONTHS_NEEDING_DOWNLOAD)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data if available\n",
    "try:\n",
    "    existing_df = pd.read_csv(BUFFER_CSV_PATH, index_col=0)\n",
    "    existing_df['timestamp'] = pd.to_datetime(existing_df['timestamp'], utc=True)\n",
    "    existing_df['timestamp'] = existing_df['timestamp'].dt.tz_localize(None)\n",
    "    print(f\"✓ Loaded existing data: {len(existing_df):,} records\")\n",
    "    print(f\"  Date range: {existing_df['timestamp'].min()} to {existing_df['timestamp'].max()}\")\n",
    "except FileNotFoundError:\n",
    "    existing_df = None\n",
    "    print(\"⚠ No existing data file found. Will create new one.\")\n",
    "\n",
    "# Download data for each month\n",
    "all_data_frames = [existing_df] if existing_df is not None else []\n",
    "downloaded_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOWNLOADING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for start_date, end_date in MONTHS_NEEDING_DOWNLOAD:\n",
    "    month_str = start_date[:7]\n",
    "    print(f\"\\n[{month_str}] Downloading data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    retry_count = 0\n",
    "    max_retries = 3\n",
    "    success = False\n",
    "    \n",
    "    while retry_count < max_retries and not success:\n",
    "        try:\n",
    "            df_temp = DP.download_data(\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                ticker_list=ticker_list,\n",
    "                time_interval='1Min'\n",
    "            )\n",
    "            \n",
    "            if df_temp is not None and len(df_temp) > 0:\n",
    "                all_data_frames.append(df_temp)\n",
    "                downloaded_count += 1\n",
    "                print(f\"✓ {month_str}: Downloaded {len(df_temp):,} records\")\n",
    "                success = True\n",
    "            else:\n",
    "                print(f\"✗ {month_str}: No data returned\")\n",
    "                failed_count += 1\n",
    "                success = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            if retry_count < max_retries:\n",
    "                print(f\"✗ {month_str}: API Error - {str(e)}\")\n",
    "                print(f\"  Retrying in {API_RETRY_WAIT} seconds... (Attempt {retry_count}/{max_retries})\")\n",
    "                time.sleep(API_RETRY_WAIT)\n",
    "            else:\n",
    "                print(f\"✗ {month_str}: Failed after {max_retries} attempts\")\n",
    "                failed_count += 1\n",
    "                success = True\n",
    "    \n",
    "    # Sleep between requests\n",
    "    if start_date != MONTHS_NEEDING_DOWNLOAD[-1][0]:\n",
    "        time.sleep(API_REQUEST_SLEEP)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Download Summary:\")\n",
    "print(f\"  Successfully downloaded: {downloaded_count} months\")\n",
    "print(f\"  Failed: {failed_count} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc37668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMBINING AND CLEANING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(all_data_frames) > 0:\n",
    "    df_raw = pd.concat(all_data_frames, ignore_index=True)\n",
    "    print(f\"✓ Combined dataframe: {len(df_raw):,} total records\")\n",
    "\n",
    "    # Rename if needed\n",
    "    if 'date' in df_raw.columns:\n",
    "        df_raw.rename(columns={'date': 'timestamp'}, inplace=True)\n",
    "\n",
    "    # Convert timestamps\n",
    "    df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'], utc=True)\n",
    "    df_raw['timestamp'] = df_raw['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "    # Remove duplicates (keep latest)\n",
    "    df_raw = df_raw.sort_values('timestamp').drop_duplicates(subset=['timestamp', 'tic'], keep='last')\n",
    "    print(f\"✓ After removing duplicates: {len(df_raw):,} records\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_raw.to_csv(BUFFER_CSV_PATH)\n",
    "    print(f\"✓ Saved raw data to: {os.path.basename(BUFFER_CSV_PATH)}\")\n",
    "    \n",
    "    print(f\"\\n  Date range: {df_raw['timestamp'].min()} to {df_raw['timestamp'].max()}\")\n",
    "    print(f\"  Unique tickers: {df_raw['tic'].nunique()}\")\n",
    "else:\n",
    "    print(\"⚠ No data downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70baf079",
   "metadata": {},
   "source": [
    "## 2. Data Verification and Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify and analyze the downloaded data\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA VERIFICATION AND QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check basic statistics\n",
    "print(f\"\\nTotal Records: {len(df_raw):,}\")\n",
    "print(f\"Date Range: {df_raw['timestamp'].min()} to {df_raw['timestamp'].max()}\")\n",
    "print(f\"Unique Tickers: {df_raw['tic'].nunique()}\")\n",
    "print(f\"Total Days: {(df_raw['timestamp'].max() - df_raw['timestamp'].min()).days}\")\n",
    "\n",
    "# Analyze by month\n",
    "df_raw['year_month'] = pd.to_datetime(df_raw['timestamp']).dt.to_period('M')\n",
    "records_per_month = df_raw.groupby(['year_month', 'tic']).size().reset_index(name='records')\n",
    "pivot_table = records_per_month.pivot(index='year_month', columns='tic', values='records')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECORDS PER MONTH\")\n",
    "print(\"=\" * 80)\n",
    "print(pivot_table.fillna(0).astype(int))\n",
    "\n",
    "# Calculate completeness by month\n",
    "expected_records = 390 * 21  # ~8,190 per ticker per month\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPLETENESS BY MONTH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for month in pivot_table.index:\n",
    "    month_data = pivot_table.loc[month]\n",
    "    total_expected = expected_records * len(ticker_list)\n",
    "    total_actual = month_data.sum()\n",
    "    completeness_pct = (total_actual / total_expected) * 100\n",
    "    \n",
    "    status = \"✓ COMPLETE\" if completeness_pct >= 95 else \"⚠ INCOMPLETE\"\n",
    "    print(f\"{month}: {completeness_pct:6.1f}% ({int(total_actual):,} / {total_expected:,}) {status}\")\n",
    "\n",
    "# Clean up temporary column\n",
    "df_raw = df_raw.drop('year_month', axis=1)\n",
    "\n",
    "print(\"\\n✓ Data verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590bdb4a",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning with AlpacaProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AlpacaProcessor for cleaning\n",
    "alpaca = AlpacaProcessor(\n",
    "    API_KEY=API_KEY,\n",
    "    API_SECRET=API_SECRET,\n",
    "    API_BASE_URL=API_BASE_URL\n",
    ")\n",
    "\n",
    "alpaca.start = \"2024-07-01\"\n",
    "alpaca.end = \"2025-11-09\"\n",
    "alpaca.time_interval = \"1Min\"\n",
    "\n",
    "print(\"✓ AlpacaProcessor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force UTC timezone\n",
    "df_raw[\"timestamp\"] = pd.to_datetime(df_raw[\"timestamp\"], utc=True)\n",
    "\n",
    "# Clean the data\n",
    "print(\"Cleaning data...\")\n",
    "processed_df = alpaca.clean_data(df_raw)\n",
    "\n",
    "print(f\"✓ Data cleaned\")\n",
    "print(f\"  Records: {len(processed_df):,}\")\n",
    "print(f\"  Columns: {list(processed_df.columns)}\")\n",
    "\n",
    "# Check for zeros\n",
    "zero_summary = (processed_df == 0).sum().sort_values(ascending=False)\n",
    "print(f\"\\nZeros per column:\\n{zero_summary[zero_summary > 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and check for NaN values\n",
    "processed_df = processed_df.sort_values(by=[\"timestamp\", \"tic\"]).reset_index(drop=True)\n",
    "print(f\"✓ Data sorted by timestamp and ticker\")\n",
    "print(f\"\\nNaN values per column:\")\n",
    "print(processed_df.isna().sum()[processed_df.isna().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98474c",
   "metadata": {},
   "source": [
    "## 4. Add Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add VIX\n",
    "print(\"Adding VIX data...\")\n",
    "processed_df = alpaca.add_vix(processed_df)\n",
    "print(\"✓ VIX data added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc337328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add technical indicators\n",
    "from finrl.config import INDICATORS\n",
    "\n",
    "print(f\"Adding technical indicators: {INDICATORS}\")\n",
    "processed_df[\"timestamp\"] = pd.to_datetime(processed_df[\"timestamp\"], utc=True)\n",
    "processed_df_tech = alpaca.add_technical_indicator(processed_df, INDICATORS)\n",
    "\n",
    "print(f\"✓ Technical indicators added\")\n",
    "print(f\"  Total columns: {len(processed_df_tech.columns)}\")\n",
    "print(f\"  Sample columns: {list(processed_df_tech.columns[:10])}\")\n",
    "\n",
    "# Check for zeros\n",
    "zero_summary = (processed_df_tech == 0).sum().sort_values(ascending=False)\n",
    "print(f\"\\nZeros in technical indicators:\\n{zero_summary[zero_summary > 0].head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename timestamp to date (for compatibility with FinRL)\n",
    "processed_df_tech.rename(columns={'timestamp': 'date'}, inplace=True)\n",
    "print(\"✓ Renamed 'timestamp' column to 'date'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1b59b",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and trading periods\n",
    "# Training: July 2024 - July 2025\n",
    "# Trading: Aug 2025 - Nov 2025\n",
    "train = data_split(processed_df_tech, '2024-07-01', '2025-07-31')\n",
    "trade = data_split(processed_df_tech, '2025-08-01', '2025-11-09')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training data size: {len(train):,}\")\n",
    "print(f\"Trading data size: {len(trade):,}\")\n",
    "\n",
    "print(\"\\nTraining date range:\")\n",
    "print(f\"  Start: {train['date'].min()}\")\n",
    "print(f\"  End: {train['date'].max()}\")\n",
    "\n",
    "print(\"\\nTrading date range:\")\n",
    "print(f\"  Start: {trade['date'].min()}\")\n",
    "print(f\"  End: {trade['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in technical indicators\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HANDLING NaN VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Before NaN handling:\")\n",
    "nan_counts = train.isna().sum()\n",
    "print(f\"  Total NaN values: {nan_counts.sum()}\")\n",
    "if nan_counts.sum() > 0:\n",
    "    print(nan_counts[nan_counts > 0])\n",
    "\n",
    "# Forward fill first, then backward fill\n",
    "train = train.ffill().bfill()\n",
    "trade = trade.ffill().bfill()\n",
    "\n",
    "print(\"\\nAfter NaN handling:\")\n",
    "print(f\"  Training NaN values: {train.isna().sum().sum()}\")\n",
    "print(f\"  Trading NaN values: {trade.isna().sum().sum()}\")\n",
    "\n",
    "# Verify no NaN values\n",
    "assert not train.isna().any().any(), \"Training data should not have NaN values\"\n",
    "assert not trade.isna().any().any(), \"Trading data should not have NaN values\"\n",
    "print(\"\\n✓ All NaN values handled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea5df0",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f362ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "train.to_csv(TRAIN_DATA_CSV)\n",
    "trade.to_csv(TRADE_DATA_CSV)\n",
    "\n",
    "# Also save the full processed data\n",
    "processed_df_tech.to_csv(AFTER_CLEAN_DATA_CSV)\n",
    "print(f\"✓ Full processed data saved to: {os.path.basename(AFTER_CLEAN_DATA_CSV)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
