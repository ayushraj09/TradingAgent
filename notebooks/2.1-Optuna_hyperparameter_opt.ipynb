{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e3ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using finrl from: /Users/ayushraj/Documents/Python/FinRL/FinRL/finrl\n"
     ]
    }
   ],
   "source": [
    "# Setup and environment configuration\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get relative paths\n",
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "# Load environment variables\n",
    "env_path = os.path.join(notebook_dir, '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Install FinRL package\n",
    "%pip install -e {project_root} -q\n",
    "\n",
    "# Add to path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import finrl\n",
    "print(f'Using finrl from: {os.path.dirname(finrl.__file__)}')\n",
    "print(f'Project root: {project_root}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d7a6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already available\n",
    "%pip install optuna -q\n",
    "%pip install stable-baselines3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de60bd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Optuna Hyperparameter Tuning for FinRL\n",
    "# This notebook demonstrates hyperparameter optimization for PPO, SAC, and TD3 algorithms\n",
    "# Using real stock trading environments based on NeurIPS_test.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.logger import configure\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the parent directory to path to import finrl modules\n",
    "sys.path.append('../')\n",
    "sys.path.append('../finrl')\n",
    "\n",
    "# Import FinRL modules (same as NeurIPS_test.ipynb)\n",
    "from finrl.agents.stablebaselines3.tune_sb3 import TuneSB3Optuna, LoggingCallback\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl import config\n",
    "from finrl.main import check_and_make_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e56cf621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFYING IMPORTS:\n",
      "‚úÖ Optuna version: 4.6.0\n",
      "‚úÖ TuneSB3Optuna imported successfully\n",
      "‚úÖ Stable-Baselines3 algorithms imported successfully\n",
      "üéØ All required imports verified!\n"
     ]
    }
   ],
   "source": [
    "# Verify that all required imports are working\n",
    "print(\"üîç VERIFYING IMPORTS:\")\n",
    "try:\n",
    "    import optuna\n",
    "    print(f\"‚úÖ Optuna version: {optuna.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Optuna import failed: {e}\")\n",
    "    \n",
    "try:\n",
    "    from finrl.agents.stablebaselines3.tune_sb3 import TuneSB3Optuna\n",
    "    print(\"‚úÖ TuneSB3Optuna imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå TuneSB3Optuna import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    from stable_baselines3 import PPO, SAC, TD3\n",
    "    print(\"‚úÖ Stable-Baselines3 algorithms imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Stable-Baselines3 import failed: {e}\")\n",
    "\n",
    "print(\"üéØ All required imports verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97545fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing hyperparameter samplers...\n",
      "‚úÖ TD3 sampler works!\n",
      "   TD3 params keys: ['gamma', 'learning_rate', 'batch_size', 'buffer_size', 'train_freq', 'gradient_steps', 'policy_kwargs', 'tau', 'action_noise', 'noise_std']\n",
      "   action_noise: ornstein_uhlenbeck (type: <class 'str'>)\n",
      "   noise_std: 0.5\n",
      "‚úÖ SAC sampler works!\n",
      "   SAC params keys: ['gamma', 'learning_rate', 'batch_size', 'buffer_size', 'learning_starts', 'train_freq', 'gradient_steps', 'ent_coef', 'tau', 'target_entropy', 'policy_kwargs']\n",
      "üéØ Hyperparameter sampler tests completed!\n"
     ]
    }
   ],
   "source": [
    "# Test hyperparameter samplers with dummy trial to verify fixes\n",
    "import optuna\n",
    "from finrl.agents.stablebaselines3 import hyperparams_opt as hpt\n",
    "\n",
    "# Create dummy trial-like object with required attributes\n",
    "class TestTrial:\n",
    "    def __init__(self):\n",
    "        self.n_actions = 30\n",
    "        self.using_her_replay_buffer = False\n",
    "        self.her_kwargs = {}\n",
    "    \n",
    "    def suggest_categorical(self, name, choices):\n",
    "        return choices[0]\n",
    "    \n",
    "    def suggest_loguniform(self, name, a, b):\n",
    "        return (a + b) / 2\n",
    "    \n",
    "    def suggest_uniform(self, name, a, b):\n",
    "        return (a + b) / 2\n",
    "    \n",
    "    def suggest_int(self, name, a, b):\n",
    "        return a\n",
    "\n",
    "print(\"üß™ Testing hyperparameter samplers...\")\n",
    "\n",
    "# Test TD3 sampler\n",
    "trial = TestTrial()\n",
    "try:\n",
    "    td3_params = hpt.sample_td3_params(trial)\n",
    "    print(\"‚úÖ TD3 sampler works!\")\n",
    "    print(f\"   TD3 params keys: {list(td3_params.keys())}\")\n",
    "    if 'action_noise' in td3_params:\n",
    "        print(f\"   action_noise: {td3_params['action_noise']} (type: {type(td3_params['action_noise'])})\")\n",
    "        if 'noise_std' in td3_params:\n",
    "            print(f\"   noise_std: {td3_params['noise_std']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TD3 sampler failed: {e}\")\n",
    "\n",
    "# Test SAC sampler\n",
    "trial = TestTrial()\n",
    "try:\n",
    "    sac_params = hpt.sample_sac_params(trial)\n",
    "    print(\"‚úÖ SAC sampler works!\")\n",
    "    print(f\"   SAC params keys: {list(sac_params.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SAC sampler failed: {e}\")\n",
    "\n",
    "print(\"üéØ Hyperparameter sampler tests completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8932bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.main import check_and_make_directories\n",
    "from finrl import config\n",
    "\n",
    "# Create necessary directories\n",
    "check_and_make_directories([\n",
    "    config.DATA_SAVE_DIR,\n",
    "    config.TRAINED_MODEL_DIR,\n",
    "    config.TENSORBOARD_LOG_DIR,\n",
    "    config.RESULTS_DIR,\n",
    "])\n",
    "\n",
    "ALGORITHMS = [\"ppo\", \"sac\", \"td3\"]  # Algorithms to optimize\n",
    "\n",
    "TOTAL_TIMESTEPS = 150000  # Balanced approach for 1-minute 6-month data\n",
    "\n",
    "N_TRIALS = 15  # Number of optimization trials per algorithm\n",
    "\n",
    "# Early stopping configuration\n",
    "THRESHOLD = 0.01  # Sharpe ratio improvement threshold\n",
    "TRIAL_NUMBER = 3  # Minimum trials before early stopping\n",
    "PATIENCE = 2      # Patience for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5123b379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model persistence system initialized.\n",
      "üìÅ Best models directory: trained_models/best_optuna_models\n",
      "üìÑ Metadata file: trained_models/best_optuna_models/best_models_metadata.json\n",
      "üìä Logs directory: results/optuna_logs\n"
     ]
    }
   ],
   "source": [
    "# Model persistence and logging configuration\n",
    "BEST_MODELS_DIR = os.path.join(config.TRAINED_MODEL_DIR, \"best_optuna_models\")\n",
    "BEST_MODELS_METADATA_FILE = os.path.join(BEST_MODELS_DIR, \"best_models_metadata.json\")\n",
    "LOGS_DIR = os.path.join(config.RESULTS_DIR, \"optuna_logs\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(BEST_MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "def load_best_models_metadata():\n",
    "    \"\"\"Load metadata about previously saved best models.\"\"\"\n",
    "    if os.path.exists(BEST_MODELS_METADATA_FILE):\n",
    "        with open(BEST_MODELS_METADATA_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_best_models_metadata(metadata):\n",
    "    \"\"\"Save metadata about best models.\"\"\"\n",
    "    with open(BEST_MODELS_METADATA_FILE, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "def get_best_model_path(algo):\n",
    "    \"\"\"Get the path for the best model of a given algorithm.\"\"\"\n",
    "    return os.path.join(BEST_MODELS_DIR, f\"best_{algo}_model.zip\")\n",
    "\n",
    "def get_model_log_path(algo):\n",
    "    \"\"\"Get the log directory path for a given algorithm.\"\"\"\n",
    "    return os.path.join(LOGS_DIR, f\"{algo}_logs\")\n",
    "\n",
    "def backup_previous_best_model(algo):\n",
    "    \"\"\"Backup the previous best model before replacing it.\"\"\"\n",
    "    model_path = get_best_model_path(algo)\n",
    "    if os.path.exists(model_path):\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        backup_path = os.path.join(BEST_MODELS_DIR, f\"backup_{algo}_model_{timestamp}.zip\")\n",
    "        shutil.copy2(model_path, backup_path)\n",
    "        print(f\"  üì¶ Previous best {algo.upper()} model backed up to: {backup_path}\")\n",
    "        return backup_path\n",
    "    return None\n",
    "\n",
    "def setup_model_logger(algo, trial_number=None):\n",
    "    \"\"\"Set up logger for model training with proper directory structure.\"\"\"\n",
    "    log_suffix = f\"_trial_{trial_number}\" if trial_number is not None else \"\"\n",
    "    log_path = os.path.join(LOGS_DIR, f\"{algo}{log_suffix}\")\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    \n",
    "    logger = configure(log_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    return logger, log_path\n",
    "\n",
    "print(\"‚úÖ Model persistence system initialized.\")\n",
    "print(f\"üìÅ Best models directory: {BEST_MODELS_DIR}\")\n",
    "print(f\"üìÑ Metadata file: {BEST_MODELS_METADATA_FILE}\")\n",
    "print(f\"üìä Logs directory: {LOGS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finrl_environments():\n",
    "    \"\"\"\n",
    "    Create real FinRL trading environments using processed stock data.\n",
    "    This follows the same pattern as NeurIPS_test.ipynb\n",
    "    Filters training data to 2025-02-01 to 2025-07-30\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from finrl.config import INDICATORS\n",
    "    from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "    \n",
    "    # Use relative paths\n",
    "    train_data_path = os.path.join(notebook_dir, 'train_data.csv')\n",
    "    trade_data_path = os.path.join(notebook_dir, 'trade_data.csv')\n",
    "    \n",
    "    # Load training and trading data\n",
    "    train = pd.read_csv(train_data_path, index_col=0)\n",
    "    trade = pd.read_csv(trade_data_path, index_col=0)\n",
    "    \n",
    "    # Convert date column to datetime and filter train data to 2025-02-01 to 2025-08-01\n",
    "    train['date'] = pd.to_datetime(train['date'])\n",
    "    train = train[(train['date'] >= '2025-02-01') & (train['date'] <= '2025-08-01')]\n",
    "    \n",
    "    trade['date'] = pd.to_datetime(trade['date'])\n",
    "    trade = trade[(trade['date'] >= '2025-08-01') & (trade['date'] <= '2025-11-01')]\n",
    "    \n",
    "    # Get unique dates and create day mapping\n",
    "    train_dates = sorted(train['date'].unique())\n",
    "    trade_dates = sorted(trade['date'].unique())\n",
    "    \n",
    "    # Create day index for train and trade data\n",
    "    train['day'] = train['date'].map({date: i for i, date in enumerate(train_dates)})\n",
    "    trade['day'] = trade['date'].map({date: i for i, date in enumerate(trade_dates)})\n",
    "    \n",
    "    # Set the day as index - this is what StockTradingEnv expects\n",
    "    train = train.set_index('day')\n",
    "    trade = trade.set_index('day')\n",
    "    \n",
    "    # Handle NaN values\n",
    "    train = train.ffill()\n",
    "    train = train.bfill()\n",
    "    trade = trade.ffill()\n",
    "    trade = trade.bfill()\n",
    "\n",
    "    # Verify no NaN values\n",
    "    assert not train.isna().any().any(), \"NaN values found in training data\"\n",
    "    assert not trade.isna().any().any(), \"NaN values found in trading data\"\n",
    "    \n",
    "    # Calculate environment parameters\n",
    "    stock_dimension = len(train.tic.unique())\n",
    "    state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "    \n",
    "    # Environment parameters\n",
    "    buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "    num_stock_shares = [0] * stock_dimension\n",
    "    \n",
    "    env_kwargs = {\n",
    "        \"hmax\": 100,\n",
    "        \"initial_amount\": 1000000,\n",
    "        \"num_stock_shares\": num_stock_shares,\n",
    "        \"buy_cost_pct\": buy_cost_list,\n",
    "        \"sell_cost_pct\": sell_cost_list,\n",
    "        \"state_space\": state_space,\n",
    "        \"stock_dim\": stock_dimension,\n",
    "        \"tech_indicator_list\": INDICATORS,\n",
    "        \"action_space\": stock_dimension,\n",
    "        \"reward_scaling\": 1e-4\n",
    "    }\n",
    "    \n",
    "    # Create training environment (vectorized for SB3)\n",
    "    e_train_gym = StockTradingEnv(df=train, **env_kwargs)\n",
    "    env_train, _ = e_train_gym.get_sb_env()\n",
    "    \n",
    "    # Create trading environment (raw for backtesting)\n",
    "    env_trade = StockTradingEnv(df=trade, turbulence_threshold=70, risk_indicator_col='vixy', **env_kwargs)\n",
    "    \n",
    "    return env_train, env_trade\n",
    "\n",
    "env_train, env_trade = create_finrl_environments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be68cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîç VERIFYING ENVIRONMENT DIMENSIONS\n",
      "======================================================================\n",
      "\n",
      "üìä COMPUTED DIMENSIONS:\n",
      "  Stock dimension: 30\n",
      "  Number of indicators: 8\n",
      "  Expected state space: 301\n",
      "    Formula: 1 + 2√ó30 + 8√ó30 = 301\n",
      "\n",
      "üîé VERIFYING AGAINST ENVIRONMENT:\n",
      "  Training env type: VecEnv (vectorized)\n",
      "  Observation space shape: (301,)\n",
      "  Action space: Box(-1.0, 1.0, (30,), float32)\n",
      "  Action dimension: 30\n",
      "\n",
      "‚úÖ VERIFICATION RESULTS:\n",
      "  ‚úÖ Observation space matches: 301 == 301\n",
      "  ‚úÖ Action space matches: 30 == 30\n",
      "\n",
      "üìã TECHNICAL INDICATORS (8 total):\n",
      "  1. macd\n",
      "  2. boll_ub\n",
      "  3. boll_lb\n",
      "  4. rsi_30\n",
      "  5. cci_30\n",
      "  6. dx_30\n",
      "  7. close_30_sma\n",
      "  8. close_60_sma\n",
      "\n",
      "üß™ TESTING ENVIRONMENT RESET:\n",
      "  Sample observation shape: (1, 301)\n",
      "  Sample observation type: <class 'numpy.ndarray'>\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DIMENSION VERIFICATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üìä COMPUTED DIMENSIONS:\n",
      "  Stock dimension: 30\n",
      "  Number of indicators: 8\n",
      "  Expected state space: 301\n",
      "    Formula: 1 + 2√ó30 + 8√ó30 = 301\n",
      "\n",
      "üîé VERIFYING AGAINST ENVIRONMENT:\n",
      "  Training env type: VecEnv (vectorized)\n",
      "  Observation space shape: (301,)\n",
      "  Action space: Box(-1.0, 1.0, (30,), float32)\n",
      "  Action dimension: 30\n",
      "\n",
      "‚úÖ VERIFICATION RESULTS:\n",
      "  ‚úÖ Observation space matches: 301 == 301\n",
      "  ‚úÖ Action space matches: 30 == 30\n",
      "\n",
      "üìã TECHNICAL INDICATORS (8 total):\n",
      "  1. macd\n",
      "  2. boll_ub\n",
      "  3. boll_lb\n",
      "  4. rsi_30\n",
      "  5. cci_30\n",
      "  6. dx_30\n",
      "  7. close_30_sma\n",
      "  8. close_60_sma\n",
      "\n",
      "üß™ TESTING ENVIRONMENT RESET:\n",
      "  Sample observation shape: (1, 301)\n",
      "  Sample observation type: <class 'numpy.ndarray'>\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DIMENSION VERIFICATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# DIMENSION VERIFICATION - Run this before training to ensure compatibility\n",
    "print(\"=\"*70)\n",
    "print(\"üîç VERIFYING ENVIRONMENT DIMENSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the same data that create_finrl_environments uses\n",
    "import pandas as pd\n",
    "from finrl.config import INDICATORS\n",
    "\n",
    "# Use relative path\n",
    "train_data_path = os.path.join(notebook_dir, 'train_data.csv')\n",
    "train = pd.read_csv(train_data_path, index_col=0)\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train_filtered = train[(train['date'] >= '2025-02-01') & (train['date'] <= '2025-08-01')]\n",
    "\n",
    "# Compute dimensions\n",
    "stock_dimension = len(train_filtered['tic'].unique())\n",
    "n_indicators = len(INDICATORS)\n",
    "expected_state_space = 1 + 2*stock_dimension + n_indicators*stock_dimension\n",
    "\n",
    "print(f\"\\nüìä COMPUTED DIMENSIONS:\")\n",
    "print(f\"  Stock dimension: {stock_dimension}\")\n",
    "print(f\"  Number of indicators: {n_indicators}\")\n",
    "print(f\"  Expected state space: {expected_state_space}\")\n",
    "print(f\"    Formula: 1 + 2√ó{stock_dimension} + {n_indicators}√ó{stock_dimension} = {expected_state_space}\")\n",
    "\n",
    "# Verify against actual environment\n",
    "print(f\"\\nüîé VERIFYING AGAINST ENVIRONMENT:\")\n",
    "try:\n",
    "    # Check vectorized training environment\n",
    "    if hasattr(env_train, 'envs') and len(env_train.envs) > 0:\n",
    "        # VecEnv wrapper\n",
    "        inner_env = env_train.envs[0]\n",
    "        obs_space = inner_env.observation_space\n",
    "        act_space = inner_env.action_space\n",
    "        print(f\"  Training env type: VecEnv (vectorized)\")\n",
    "    else:\n",
    "        # Direct env\n",
    "        obs_space = env_train.observation_space\n",
    "        act_space = env_train.action_space\n",
    "        print(f\"  Training env type: Direct environment\")\n",
    "    \n",
    "    # Get observation space shape\n",
    "    obs_shape = obs_space.shape[0] if hasattr(obs_space, 'shape') else None\n",
    "    print(f\"  Observation space shape: {obs_space.shape if hasattr(obs_space, 'shape') else 'N/A'}\")\n",
    "    \n",
    "    # Get action space dimension\n",
    "    if hasattr(act_space, 'shape') and act_space.shape:\n",
    "        act_dim = act_space.shape[0]\n",
    "    elif hasattr(act_space, 'n'):\n",
    "        act_dim = act_space.n\n",
    "    else:\n",
    "        act_dim = None\n",
    "    print(f\"  Action space: {act_space}\")\n",
    "    print(f\"  Action dimension: {act_dim}\")\n",
    "    \n",
    "    # Verification checks\n",
    "    print(f\"\\n‚úÖ VERIFICATION RESULTS:\")\n",
    "    \n",
    "    if obs_shape is not None:\n",
    "        if obs_shape == expected_state_space:\n",
    "            print(f\"  ‚úÖ Observation space matches: {obs_shape} == {expected_state_space}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  MISMATCH: Observation space {obs_shape} != expected {expected_state_space}\")\n",
    "            print(f\"      Difference: {obs_shape - expected_state_space}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Could not determine observation space shape\")\n",
    "    \n",
    "    if act_dim is not None:\n",
    "        if act_dim == stock_dimension:\n",
    "            print(f\"  ‚úÖ Action space matches: {act_dim} == {stock_dimension}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  MISMATCH: Action space {act_dim} != expected {stock_dimension}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Could not determine action space dimension\")\n",
    "    \n",
    "    # Display indicator list for reference\n",
    "    print(f\"\\nüìã TECHNICAL INDICATORS ({n_indicators} total):\")\n",
    "    for i, ind in enumerate(INDICATORS, 1):\n",
    "        print(f\"  {i}. {ind}\")\n",
    "    \n",
    "    # Test a sample observation reset\n",
    "    print(f\"\\nüß™ TESTING ENVIRONMENT RESET:\")\n",
    "    if hasattr(env_train, 'reset'):\n",
    "        sample_obs = env_train.reset()\n",
    "        if isinstance(sample_obs, tuple):\n",
    "            sample_obs = sample_obs[0]  # Handle gym/gymnasium API differences\n",
    "        if hasattr(sample_obs, 'shape'):\n",
    "            print(f\"  Sample observation shape: {sample_obs.shape}\")\n",
    "            print(f\"  Sample observation type: {type(sample_obs)}\")\n",
    "        else:\n",
    "            print(f\"  Sample observation: {sample_obs[:10] if len(sample_obs) > 10 else sample_obs}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Verification error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ DIMENSION VERIFICATION COMPLETE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08189787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environments ready for optimization.\n",
      "‚ÑπÔ∏è  No previous best models found. Starting fresh optimization.\n"
     ]
    }
   ],
   "source": [
    "def run_optuna_optimization(env_train, env_trade, algorithms=None):\n",
    "    \"\"\"\n",
    "    Run Optuna hyperparameter optimization for specified algorithms.\n",
    "    Now includes comprehensive model saving, logging, and best model persistence.\n",
    "    \n",
    "    Args:\n",
    "        env_train: Training environment\n",
    "        env_trade: Trading/testing environment  \n",
    "        algorithms: List of algorithms to optimize (default: [\"ppo\", \"sac\", \"td3\"])\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results for each algorithm\n",
    "    \"\"\"\n",
    "    if algorithms is None:\n",
    "        algorithms = ALGORITHMS\n",
    "    \n",
    "    if env_train is None or env_trade is None:\n",
    "        return {}\n",
    "    \n",
    "    # Load existing best models metadata\n",
    "    best_models_metadata = load_best_models_metadata()\n",
    "    results = {}\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üöÄ Starting {algo.upper()} optimization...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Check if we have a previous best model\n",
    "        previous_best_sharpe = None\n",
    "        if algo in best_models_metadata:\n",
    "            previous_best_sharpe = best_models_metadata[algo].get('best_sharpe', None)\n",
    "            if previous_best_sharpe is not None:\n",
    "                print(f\"üìä Previous best {algo.upper()} Sharpe ratio: {previous_best_sharpe:.4f}\")\n",
    "                print(f\"üìÖ Saved on: {best_models_metadata[algo].get('timestamp', 'Unknown')}\")\n",
    "        \n",
    "        try:\n",
    "            # Setup logging for this optimization run\n",
    "            run_logger, log_path = setup_model_logger(algo, \"optuna_run\")\n",
    "            print(f\"üìù Logs will be saved to: {log_path}\")\n",
    "            \n",
    "            logging_cb = LoggingCallback(\n",
    "                threshold=THRESHOLD,\n",
    "                trial_number=TRIAL_NUMBER,\n",
    "                patience=PATIENCE\n",
    "            )\n",
    "            \n",
    "            # Create custom TuneSB3Optuna with enhanced logging and model saving\n",
    "            class EnhancedTuneSB3Optuna(TuneSB3Optuna):\n",
    "                def __init__(self, *args, **kwargs):\n",
    "                    super().__init__(*args, **kwargs)\n",
    "                    self.algo_name = kwargs.get('model_name', 'unknown')\n",
    "                    self.run_logger = run_logger\n",
    "                    self.trial_models = {}  # Store trial models for later reference\n",
    "                \n",
    "                def objective(self, trial: optuna.Trial):\n",
    "                    \"\"\"Override objective to add enhanced logging and model saving for each trial.\"\"\"\n",
    "                    print(f\"  üîÑ Trial {trial.number}: Starting {self.algo_name.upper()} optimization...\")\n",
    "\n",
    "                    # Ensure trial has environment-dependent attributes that some samplers expect\n",
    "                    try:\n",
    "                        n_actions = None\n",
    "                        if hasattr(self.env_train, \"action_space\") and getattr(self.env_train.action_space, \"shape\", None) is not None:\n",
    "                            n_actions = int(self.env_train.action_space.shape[-1])\n",
    "                        else:\n",
    "                            if hasattr(self.env_train, \"envs\") and len(self.env_train.envs) > 0:\n",
    "                                n_actions = int(self.env_train.envs[0].action_space.shape[-1])\n",
    "                    except Exception:\n",
    "                        n_actions = None\n",
    "\n",
    "                    try:\n",
    "                        setattr(trial, \"n_actions\", n_actions)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                    # Default to not using HER unless explicitly set elsewhere\n",
    "                    try:\n",
    "                        if not hasattr(trial, \"using_her_replay_buffer\"):\n",
    "                            setattr(trial, \"using_her_replay_buffer\", False)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        if not hasattr(trial, \"her_kwargs\"):\n",
    "                            setattr(trial, \"her_kwargs\", {})\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                    # Sample hyperparameters (same as original)\n",
    "                    hyperparameters = self.default_sample_hyperparameters(trial)\n",
    "                    policy_kwargs = hyperparameters.get(\"policy_kwargs\", {})\n",
    "                    if \"policy_kwargs\" in hyperparameters:\n",
    "                        del hyperparameters[\"policy_kwargs\"]\n",
    "                    \n",
    "                    print(f\"  üìä Trial {trial.number} hyperparameters: {hyperparameters}\")\n",
    "                    \n",
    "                    # Create model with trial hyperparameters\n",
    "                    model = self.agent.get_model(\n",
    "                        self.model_name, policy_kwargs=policy_kwargs, model_kwargs=hyperparameters\n",
    "                    )\n",
    "                    \n",
    "                    # Setup trial-specific logger\n",
    "                    trial_logger, trial_log_path = setup_model_logger(self.algo_name, trial.number)\n",
    "                    model.set_logger(trial_logger)\n",
    "                    \n",
    "                    # Train the model\n",
    "                    print(f\"  üéØ Trial {trial.number}: Training for {self.total_timesteps:,} timesteps...\")\n",
    "                    trained_model = self.agent.train_model(\n",
    "                        model=model,\n",
    "                        tb_log_name=f\"{self.model_name}_trial_{trial.number}\",\n",
    "                        total_timesteps=self.total_timesteps,\n",
    "                    )\n",
    "                    \n",
    "                    # === SAVE TRIAL MODEL BEFORE ANY DELETION ===\n",
    "                    # Save trial model (use same format as original tune_sb3.py - .pth files)\n",
    "                    # This ensures compatibility with the original backtest method\n",
    "                    trial_model_path = f\"./{config.TRAINED_MODEL_DIR}/{self.model_name}_{trial.number}.pth\"\n",
    "                    print(f\"  üíæ Trial {trial.number}: Saving model to {trial_model_path}...\")\n",
    "                    trained_model.save(trial_model_path)\n",
    "                    \n",
    "                    # Verify primary model file was saved successfully\n",
    "                    if not os.path.exists(trial_model_path):\n",
    "                        raise RuntimeError(f\"Failed to save trial model to {trial_model_path}\")\n",
    "                    \n",
    "                    print(f\"  ‚úÖ Trial {trial.number} model saved to: {trial_model_path}\")\n",
    "                    \n",
    "                    # Also save a backup copy in our enhanced format for better organization\n",
    "                    backup_trial_path = os.path.join(BEST_MODELS_DIR, f\"trial_{self.algo_name}_{trial.number}.zip\")\n",
    "                    print(f\"  üíæ Trial {trial.number}: Saving backup to {backup_trial_path}...\")\n",
    "                    trained_model.save(backup_trial_path)\n",
    "                    \n",
    "                    # Verify backup was saved successfully\n",
    "                    if not os.path.exists(backup_trial_path):\n",
    "                        print(f\"  ‚ö†Ô∏è  Warning: Backup save to {backup_trial_path} may have failed\")\n",
    "                    else:\n",
    "                        print(f\"  ‚úÖ Backup saved successfully\")\n",
    "                    \n",
    "                    # Store model reference for potential later use\n",
    "                    self.trial_models[trial.number] = {\n",
    "                        'model_path': trial_model_path,  # Use original format path for compatibility\n",
    "                        'backup_path': backup_trial_path,  # Our enhanced backup\n",
    "                        'hyperparameters': hyperparameters,\n",
    "                        'policy_kwargs': policy_kwargs\n",
    "                    }\n",
    "                    \n",
    "                    # Backtest this trial's model on trade environment\n",
    "                    print(f\"  üìà Trial {trial.number}: Backtesting on trade data...\")\n",
    "                    df_account_value, _ = DRLAgent.DRL_prediction(\n",
    "                        model=trained_model, environment=self.env_trade\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate Sharpe ratio for this trial\n",
    "                    sharpe = self.calculate_sharpe(df_account_value)\n",
    "                    print(f\"  üìä Trial {trial.number}: Sharpe ratio = {sharpe:.4f}\")\n",
    "                    \n",
    "                    # === MEMORY CLEANUP AFTER SAVING AND BACKTESTING ===\n",
    "                    print(f\"  üßπ Trial {trial.number}: Cleaning up memory...\")\n",
    "                    \n",
    "                    # Delete model objects to free memory (models already saved to disk)\n",
    "                    del trained_model\n",
    "                    del model\n",
    "                    if 'df_account_value' in locals():\n",
    "                        del df_account_value\n",
    "                    \n",
    "                    # Force Python garbage collection\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    # Clear CUDA cache if available\n",
    "                    try:\n",
    "                        import torch\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                            torch.cuda.synchronize()\n",
    "                            print(f\"  ‚úÖ CUDA memory cleared\")\n",
    "                    except Exception as e:\n",
    "                        pass  # No CUDA or torch not available\n",
    "                    \n",
    "                    # Additional aggressive cleanup for PyTorch\n",
    "                    try:\n",
    "                        import torch\n",
    "                        # Clear all cached memory\n",
    "                        if hasattr(torch.cuda, 'ipc_collect'):\n",
    "                            torch.cuda.ipc_collect()\n",
    "                        # Reset peak memory stats\n",
    "                        if hasattr(torch.cuda, 'reset_peak_memory_stats'):\n",
    "                            torch.cuda.reset_peak_memory_stats()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    \n",
    "                    print(f\"  ‚úÖ Memory cleanup complete for trial {trial.number}\")\n",
    "                    \n",
    "                    return sharpe\n",
    "                \n",
    "                def get_best_trial_model(self, study):\n",
    "                    \"\"\"Load and return the best trial model using the same method as original backtest.\"\"\"\n",
    "                    best_trial_num = study.best_trial.number\n",
    "                    # Use the same model loading approach as the original backtest method\n",
    "                    best_model_path = f\"./{config.TRAINED_MODEL_DIR}/{self.model_name}_{best_trial_num}.pth\"\n",
    "                    \n",
    "                    # Verify the best model file exists before loading\n",
    "                    if not os.path.exists(best_model_path):\n",
    "                        raise FileNotFoundError(f\"Best trial model not found at {best_model_path}\")\n",
    "                    \n",
    "                    # Load using the same method as original TuneSB3Optuna.backtest()\n",
    "                    best_model = self.MODELS[self.model_name].load(\n",
    "                        best_model_path,\n",
    "                        env=self.env_train\n",
    "                    )\n",
    "                    return best_model\n",
    "            \n",
    "            tuner = EnhancedTuneSB3Optuna(\n",
    "                env_train=env_train,\n",
    "                model_name=algo,\n",
    "                env_trade=env_trade,\n",
    "                logging_callback=logging_cb,\n",
    "                total_timesteps=TOTAL_TIMESTEPS,\n",
    "                n_trials=N_TRIALS,\n",
    "            )\n",
    "            \n",
    "            # Run optimization (this will call our enhanced objective method for each trial)\n",
    "            print(f\"üîç Running {N_TRIALS} trials for {algo.upper()}...\")\n",
    "            study = tuner.run_optuna()\n",
    "            \n",
    "            print(f\"‚úÖ {algo.upper()} optimization completed!\")\n",
    "            print(f\"üìä Best trial: {study.best_trial.number} with Sharpe: {study.best_value:.4f}\")\n",
    "            \n",
    "            # Get the actual best model (not re-backtest, just load it)\n",
    "            best_model = tuner.get_best_trial_model(study)\n",
    "            \n",
    "            # Perform final backtesting on best model (this is what the original backtest method does)\n",
    "            print(f\"üìà Final backtesting of best {algo.upper()} model...\")\n",
    "            df_account_value, df_actions, perf_stats = tuner.backtest(study)\n",
    "            \n",
    "            current_best_sharpe = study.best_value\n",
    "            \n",
    "            # Determine if this is a new best model\n",
    "            is_new_best = True\n",
    "            improvement = 0\n",
    "            if previous_best_sharpe is not None:\n",
    "                is_new_best = current_best_sharpe > previous_best_sharpe\n",
    "                improvement = current_best_sharpe - previous_best_sharpe\n",
    "            else:\n",
    "                improvement = current_best_sharpe\n",
    "            \n",
    "            if is_new_best:\n",
    "                print(f\"üéâ NEW BEST {algo.upper()} MODEL FOUND!\")\n",
    "                print(f\"   Current Sharpe: {current_best_sharpe:.4f}\")\n",
    "                if previous_best_sharpe is not None:\n",
    "                    print(f\"   Previous best: {previous_best_sharpe:.4f}\")\n",
    "                    print(f\"   Improvement: +{improvement:.4f}\")\n",
    "                \n",
    "                # Backup previous model if it exists\n",
    "                backup_path = backup_previous_best_model(algo)\n",
    "                \n",
    "                # Save the actual best model (from best trial)\n",
    "                best_model_path = get_best_model_path(algo)\n",
    "                best_model.save(best_model_path)\n",
    "                print(f\"üíæ New best {algo.upper()} model saved to: {best_model_path}\")\n",
    "                \n",
    "                # Save Optuna study\n",
    "                study_path = os.path.join(BEST_MODELS_DIR, f\"best_{algo}_study.pkl\")\n",
    "                with open(study_path, 'wb') as f:\n",
    "                    pickle.dump(study, f)\n",
    "                print(f\"üìä Optuna study saved to: {study_path}\")\n",
    "                \n",
    "                # Update metadata\n",
    "                best_models_metadata[algo] = {\n",
    "                    'best_sharpe': current_best_sharpe,\n",
    "                    'best_params': study.best_params,\n",
    "                    'best_trial': study.best_trial.number,\n",
    "                    'model_path': best_model_path,\n",
    "                    'study_path': study_path,\n",
    "                    'backup_path': backup_path,\n",
    "                    'log_path': log_path,\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'total_trials': len(study.trials),\n",
    "                    'improvement': improvement,\n",
    "                    'training_timesteps': TOTAL_TIMESTEPS,\n",
    "                    'performance_stats': perf_stats\n",
    "                }\n",
    "                \n",
    "                status = \"NEW_BEST\"\n",
    "                print(f\"‚úÖ {algo.upper()} optimization completed successfully!\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"üìâ No improvement for {algo.upper()}\")\n",
    "                print(f\"   Current Sharpe: {current_best_sharpe:.4f}\")\n",
    "                print(f\"   Previous best: {previous_best_sharpe:.4f}\")\n",
    "                print(f\"   Difference: {improvement:.4f}\")\n",
    "                print(f\"üîÑ Keeping previous best model\")\n",
    "                \n",
    "                status = \"NO_IMPROVEMENT\"\n",
    "            \n",
    "            # Store results\n",
    "            results[algo] = {\n",
    "                'study': study,\n",
    "                'best_params': study.best_params,\n",
    "                'best_value': current_best_sharpe,\n",
    "                'best_trial': study.best_trial,\n",
    "                'account_value': df_account_value,\n",
    "                'actions': df_actions,\n",
    "                'performance_stats': perf_stats,\n",
    "                'is_new_best': is_new_best,\n",
    "                'previous_best_sharpe': previous_best_sharpe,\n",
    "                'improvement': improvement,\n",
    "                'status': status,\n",
    "                'model_saved': is_new_best,\n",
    "                'log_path': log_path,\n",
    "                'trial_models': tuner.trial_models  # Include trial model info\n",
    "            }\n",
    "            \n",
    "            print(f\"üìã {algo.upper()} Summary: Sharpe {current_best_sharpe:.4f} ({status})\")\n",
    "            \n",
    "            # Clean up after algorithm optimization completes\n",
    "            print(f\"üßπ Cleaning up memory after {algo.upper()} optimization...\")\n",
    "            del best_model\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            try:\n",
    "                import torch\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.synchronize()\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {algo.upper()} optimization failed: {str(e)}\")\n",
    "            print(f\"üìã Error details saved to logs\")\n",
    "            \n",
    "            # Save error information\n",
    "            error_info = {\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'algorithm': algo,\n",
    "                'total_timesteps': TOTAL_TIMESTEPS,\n",
    "                'n_trials': N_TRIALS\n",
    "            }\n",
    "            \n",
    "            error_log_path = os.path.join(LOGS_DIR, f\"{algo}_error_log.json\")\n",
    "            with open(error_log_path, 'w') as f:\n",
    "                json.dump(error_info, f, indent=2)\n",
    "            \n",
    "            results[algo] = {\n",
    "                'error': str(e),\n",
    "                'error_log_path': error_log_path,\n",
    "                'status': 'FAILED'\n",
    "            }\n",
    "            continue\n",
    "    \n",
    "    # Save updated metadata\n",
    "    save_best_models_metadata(best_models_metadata)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üíæ Best models metadata updated\")\n",
    "    print(f\"üìÑ Metadata file: {BEST_MODELS_METADATA_FILE}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if env_train is not None and env_trade is not None:\n",
    "    print(\"‚úÖ Environments ready for optimization.\")\n",
    "    \n",
    "    # Display current best models info\n",
    "    current_metadata = load_best_models_metadata()\n",
    "    if current_metadata:\n",
    "        print(f\"\\nüìä CURRENT BEST MODELS:\")\n",
    "        print(f\"{'Algorithm':<10} {'Sharpe Ratio':<12} {'Date Saved':<12} {'Trials':<8}\")\n",
    "        print(\"-\" * 50)\n",
    "        for algo, info in current_metadata.items():\n",
    "            date_saved = info.get('timestamp', '')[:10] if info.get('timestamp') else 'Unknown'\n",
    "            trials = info.get('total_trials', 'N/A')\n",
    "            sharpe = info.get('best_sharpe', 0)\n",
    "            print(f\"{algo.upper():<10} {sharpe:<12.4f} {date_saved:<12} {trials:<8}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No previous best models found. Starting fresh optimization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3b154ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ Starting PPO optimization...\n",
      "============================================================\n",
      "Logging to results/optuna_logs/ppo_trial_optuna_run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-23 23:48:39,260] A new study created in memory with name: ppo_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Logs will be saved to: results/optuna_logs/ppo_trial_optuna_run\n",
      "üîç Running 15 trials for PPO...\n",
      "  üîÑ Trial 0: Starting PPO optimization...\n",
      "  üìä Trial 0 hyperparameters: {'n_steps': 2048, 'batch_size': 256, 'gamma': 0.999, 'learning_rate': 0.00011526449540315612, 'ent_coef': 1.8740223688836284e-07, 'clip_range': 0.3, 'n_epochs': 5, 'gae_lambda': 0.92, 'max_grad_norm': 0.8, 'vf_coef': 0.4401524937396013}\n",
      "{'n_steps': 2048, 'batch_size': 256, 'gamma': 0.999, 'learning_rate': 0.00011526449540315612, 'ent_coef': 1.8740223688836284e-07, 'clip_range': 0.3, 'n_epochs': 5, 'gae_lambda': 0.92, 'max_grad_norm': 0.8, 'vf_coef': 0.4401524937396013}\n",
      "Using mps device\n",
      "Logging to results/optuna_logs/ppo_trial_0\n",
      "  üéØ Trial 0: Training for 150,000 timesteps...\n",
      "  üéØ Trial 0: Training for 150,000 timesteps...\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 18            |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 111           |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    reward          | -0.0255853    |\n",
      "|    reward_max      | 0.790667      |\n",
      "|    reward_mean     | 0.00023476194 |\n",
      "|    reward_min      | -0.94921935   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 18            |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 111           |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    reward          | -0.0255853    |\n",
      "|    reward_max      | 0.790667      |\n",
      "|    reward_mean     | 0.00023476194 |\n",
      "|    reward_min      | -0.94921935   |\n",
      "--------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 223          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 80461.53     |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -163         |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 5            |\n",
      "|    policy_gradient_loss | 0.192        |\n",
      "|    reward               | -0.00495668  |\n",
      "|    reward_max           | 0.36374676   |\n",
      "|    reward_mean          | 0.0014874589 |\n",
      "|    reward_min           | -1.1410937   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.6e+03      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 223          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 80461.53     |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -163         |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 5            |\n",
      "|    policy_gradient_loss | 0.192        |\n",
      "|    reward               | -0.00495668  |\n",
      "|    reward_max           | 0.36374676   |\n",
      "|    reward_mean          | 0.0014874589 |\n",
      "|    reward_min           | -1.1410937   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.6e+03      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 332           |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3055.314      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.519        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 1.31          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | 0.255         |\n",
      "|    reward               | -0.01893918   |\n",
      "|    reward_max           | 0.24539708    |\n",
      "|    reward_mean          | -0.0013483997 |\n",
      "|    reward_min           | -0.3765133    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.76          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 332           |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3055.314      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.519        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 1.31          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | 0.255         |\n",
      "|    reward               | -0.01893918   |\n",
      "|    reward_max           | 0.24539708    |\n",
      "|    reward_mean          | -0.0013483997 |\n",
      "|    reward_min           | -0.3765133    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.76          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 442           |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4749.126      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -1.2          |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.705         |\n",
      "|    n_updates            | 15            |\n",
      "|    policy_gradient_loss | 0.266         |\n",
      "|    reward               | 0.10223464    |\n",
      "|    reward_max           | 0.2836311     |\n",
      "|    reward_mean          | -0.0020883076 |\n",
      "|    reward_min           | -0.7910636    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.16          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 442           |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4749.126      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -1.2          |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.705         |\n",
      "|    n_updates            | 15            |\n",
      "|    policy_gradient_loss | 0.266         |\n",
      "|    reward               | 0.10223464    |\n",
      "|    reward_max           | 0.2836311     |\n",
      "|    reward_mean          | -0.0020883076 |\n",
      "|    reward_min           | -0.7910636    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.16          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 554           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3744.3813     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.076        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.819         |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | 0.253         |\n",
      "|    reward               | -0.00320188   |\n",
      "|    reward_max           | 1.079967      |\n",
      "|    reward_mean          | -0.0013724861 |\n",
      "|    reward_min           | -1.1521995    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.89          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 554           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3744.3813     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.076        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.819         |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | 0.253         |\n",
      "|    reward               | -0.00320188   |\n",
      "|    reward_max           | 1.079967      |\n",
      "|    reward_mean          | -0.0013724861 |\n",
      "|    reward_min           | -1.1521995    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.89          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 664           |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5462.3125     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.65         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.5           |\n",
      "|    n_updates            | 25            |\n",
      "|    policy_gradient_loss | 0.261         |\n",
      "|    reward               | -0.00976656   |\n",
      "|    reward_max           | 0.4447005     |\n",
      "|    reward_mean          | 0.00087115774 |\n",
      "|    reward_min           | -0.5539785    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.98          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 664           |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5462.3125     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.65         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.5           |\n",
      "|    n_updates            | 25            |\n",
      "|    policy_gradient_loss | 0.261         |\n",
      "|    reward               | -0.00976656   |\n",
      "|    reward_max           | 0.4447005     |\n",
      "|    reward_mean          | 0.00087115774 |\n",
      "|    reward_min           | -0.5539785    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.98          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 775           |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7344.57       |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.616        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.963         |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | 0.273         |\n",
      "|    reward               | 0.01863948    |\n",
      "|    reward_max           | 0.8191924     |\n",
      "|    reward_mean          | 0.00037187693 |\n",
      "|    reward_min           | -0.66015196   |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.41          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 775           |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7344.57       |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.616        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.963         |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | 0.273         |\n",
      "|    reward               | 0.01863948    |\n",
      "|    reward_max           | 0.8191924     |\n",
      "|    reward_mean          | 0.00037187693 |\n",
      "|    reward_min           | -0.66015196   |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.41          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 885           |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7132.313      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.0958       |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.937         |\n",
      "|    n_updates            | 35            |\n",
      "|    policy_gradient_loss | 0.269         |\n",
      "|    reward               | 0.05094203    |\n",
      "|    reward_max           | 0.219067      |\n",
      "|    reward_mean          | -0.0019360029 |\n",
      "|    reward_min           | -3.6206598    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.65          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 885           |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7132.313      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.0958       |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.937         |\n",
      "|    n_updates            | 35            |\n",
      "|    policy_gradient_loss | 0.269         |\n",
      "|    reward               | 0.05094203    |\n",
      "|    reward_max           | 0.219067      |\n",
      "|    reward_mean          | -0.0019360029 |\n",
      "|    reward_min           | -3.6206598    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.65          |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 18             |\n",
      "|    iterations           | 9              |\n",
      "|    time_elapsed         | 996            |\n",
      "|    total_timesteps      | 18432          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 8113.6367      |\n",
      "|    clip_fraction        | 0.975          |\n",
      "|    clip_range           | 0.3            |\n",
      "|    entropy_loss         | -42.6          |\n",
      "|    explained_variance   | -0.278         |\n",
      "|    learning_rate        | 0.000115       |\n",
      "|    loss                 | 0.576          |\n",
      "|    n_updates            | 40             |\n",
      "|    policy_gradient_loss | 0.268          |\n",
      "|    reward               | -0.08467525    |\n",
      "|    reward_max           | 2.727199       |\n",
      "|    reward_mean          | -0.00095190096 |\n",
      "|    reward_min           | -3.2109263     |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 1.91           |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 18             |\n",
      "|    iterations           | 9              |\n",
      "|    time_elapsed         | 996            |\n",
      "|    total_timesteps      | 18432          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 8113.6367      |\n",
      "|    clip_fraction        | 0.975          |\n",
      "|    clip_range           | 0.3            |\n",
      "|    entropy_loss         | -42.6          |\n",
      "|    explained_variance   | -0.278         |\n",
      "|    learning_rate        | 0.000115       |\n",
      "|    loss                 | 0.576          |\n",
      "|    n_updates            | 40             |\n",
      "|    policy_gradient_loss | 0.268          |\n",
      "|    reward               | -0.08467525    |\n",
      "|    reward_max           | 2.727199       |\n",
      "|    reward_mean          | -0.00095190096 |\n",
      "|    reward_min           | -3.2109263     |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 1.91           |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 1106         |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7947.9346    |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -0.295       |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.625        |\n",
      "|    n_updates            | 45           |\n",
      "|    policy_gradient_loss | 0.264        |\n",
      "|    reward               | 0.10193314   |\n",
      "|    reward_max           | 3.54222      |\n",
      "|    reward_mean          | 0.0008224886 |\n",
      "|    reward_min           | -0.95800245  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.79         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 1106         |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7947.9346    |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -0.295       |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.625        |\n",
      "|    n_updates            | 45           |\n",
      "|    policy_gradient_loss | 0.264        |\n",
      "|    reward               | 0.10193314   |\n",
      "|    reward_max           | 3.54222      |\n",
      "|    reward_mean          | 0.0008224886 |\n",
      "|    reward_min           | -0.95800245  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.79         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 1216         |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6237.423     |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.558        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | 0.269        |\n",
      "|    reward               | 0.02078922   |\n",
      "|    reward_max           | 3.3601336    |\n",
      "|    reward_mean          | 0.0025285534 |\n",
      "|    reward_min           | -1.2170243   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.909        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 1216         |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6237.423     |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.558        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | 0.269        |\n",
      "|    reward               | 0.02078922   |\n",
      "|    reward_max           | 3.3601336    |\n",
      "|    reward_mean          | 0.0025285534 |\n",
      "|    reward_min           | -1.2170243   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.909        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 1327          |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7946.4043     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | 0.495         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.418         |\n",
      "|    n_updates            | 55            |\n",
      "|    policy_gradient_loss | 0.258         |\n",
      "|    reward               | -0.08012526   |\n",
      "|    reward_max           | 0.6421583     |\n",
      "|    reward_mean          | 4.0612118e-05 |\n",
      "|    reward_min           | -1.4371465    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.5           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 1327          |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7946.4043     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | 0.495         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.418         |\n",
      "|    n_updates            | 55            |\n",
      "|    policy_gradient_loss | 0.258         |\n",
      "|    reward               | -0.08012526   |\n",
      "|    reward_max           | 0.6421583     |\n",
      "|    reward_mean          | 4.0612118e-05 |\n",
      "|    reward_min           | -1.4371465    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.5           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 1437         |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8215.101     |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -1.06        |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 1.66         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | 0.244        |\n",
      "|    reward               | -0.1021955   |\n",
      "|    reward_max           | 4.2357316    |\n",
      "|    reward_mean          | 0.0017627286 |\n",
      "|    reward_min           | -0.74101126  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.29         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 1437         |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8215.101     |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -1.06        |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 1.66         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | 0.244        |\n",
      "|    reward               | -0.1021955   |\n",
      "|    reward_max           | 4.2357316    |\n",
      "|    reward_mean          | 0.0017627286 |\n",
      "|    reward_min           | -0.74101126  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.29         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 1548         |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4452.8027    |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -1.2         |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.966        |\n",
      "|    n_updates            | 65           |\n",
      "|    policy_gradient_loss | 0.241        |\n",
      "|    reward               | 0.0025388    |\n",
      "|    reward_max           | 0.27549937   |\n",
      "|    reward_mean          | 0.0006128476 |\n",
      "|    reward_min           | -1.6540191   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.67         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 1548         |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4452.8027    |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -1.2         |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.966        |\n",
      "|    n_updates            | 65           |\n",
      "|    policy_gradient_loss | 0.241        |\n",
      "|    reward               | 0.0025388    |\n",
      "|    reward_max           | 0.27549937   |\n",
      "|    reward_mean          | 0.0006128476 |\n",
      "|    reward_min           | -1.6540191   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.67         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 1657          |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6690.7393     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.085        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.378         |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | 0.258         |\n",
      "|    reward               | 0.04435138    |\n",
      "|    reward_max           | 1.3888795     |\n",
      "|    reward_mean          | -0.0011563399 |\n",
      "|    reward_min           | -2.404467     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.47          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 1657          |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6690.7393     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.085        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.378         |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | 0.258         |\n",
      "|    reward               | 0.04435138    |\n",
      "|    reward_max           | 1.3888795     |\n",
      "|    reward_mean          | -0.0011563399 |\n",
      "|    reward_min           | -2.404467     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.47          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 1767         |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 10088.246    |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -0.783       |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.348        |\n",
      "|    n_updates            | 75           |\n",
      "|    policy_gradient_loss | 0.257        |\n",
      "|    reward               | 0.04447775   |\n",
      "|    reward_max           | 1.3955905    |\n",
      "|    reward_mean          | 0.0007989771 |\n",
      "|    reward_min           | -0.4657657   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.727        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 1767         |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 10088.246    |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -0.783       |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.348        |\n",
      "|    n_updates            | 75           |\n",
      "|    policy_gradient_loss | 0.257        |\n",
      "|    reward               | 0.04447775   |\n",
      "|    reward_max           | 1.3955905    |\n",
      "|    reward_mean          | 0.0007989771 |\n",
      "|    reward_min           | -0.4657657   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.727        |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 18             |\n",
      "|    iterations           | 17             |\n",
      "|    time_elapsed         | 1877           |\n",
      "|    total_timesteps      | 34816          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 10907.432      |\n",
      "|    clip_fraction        | 0.975          |\n",
      "|    clip_range           | 0.3            |\n",
      "|    entropy_loss         | -42.6          |\n",
      "|    explained_variance   | -1.63          |\n",
      "|    learning_rate        | 0.000115       |\n",
      "|    loss                 | 0.333          |\n",
      "|    n_updates            | 80             |\n",
      "|    policy_gradient_loss | 0.239          |\n",
      "|    reward               | 0.03343745     |\n",
      "|    reward_max           | 1.6986605      |\n",
      "|    reward_mean          | -0.00019562276 |\n",
      "|    reward_min           | -0.5237258     |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 0.433          |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 18             |\n",
      "|    iterations           | 17             |\n",
      "|    time_elapsed         | 1877           |\n",
      "|    total_timesteps      | 34816          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 10907.432      |\n",
      "|    clip_fraction        | 0.975          |\n",
      "|    clip_range           | 0.3            |\n",
      "|    entropy_loss         | -42.6          |\n",
      "|    explained_variance   | -1.63          |\n",
      "|    learning_rate        | 0.000115       |\n",
      "|    loss                 | 0.333          |\n",
      "|    n_updates            | 80             |\n",
      "|    policy_gradient_loss | 0.239          |\n",
      "|    reward               | 0.03343745     |\n",
      "|    reward_max           | 1.6986605      |\n",
      "|    reward_mean          | -0.00019562276 |\n",
      "|    reward_min           | -0.5237258     |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 0.433          |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 1986          |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 11028.365     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.36         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.427         |\n",
      "|    n_updates            | 85            |\n",
      "|    policy_gradient_loss | 0.255         |\n",
      "|    reward               | -0.00384628   |\n",
      "|    reward_max           | 0.6798878     |\n",
      "|    reward_mean          | -0.0005474194 |\n",
      "|    reward_min           | -0.501379     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.864         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 1986          |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 11028.365     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.36         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.427         |\n",
      "|    n_updates            | 85            |\n",
      "|    policy_gradient_loss | 0.255         |\n",
      "|    reward               | -0.00384628   |\n",
      "|    reward_max           | 0.6798878     |\n",
      "|    reward_mean          | -0.0005474194 |\n",
      "|    reward_min           | -0.501379     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.864         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 2096         |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4590.749     |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -1.04        |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.348        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | 0.248        |\n",
      "|    reward               | 0.01539684   |\n",
      "|    reward_max           | 0.902553     |\n",
      "|    reward_mean          | 0.0014455214 |\n",
      "|    reward_min           | -0.2222853   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.27         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 2096         |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4590.749     |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -1.04        |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.348        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | 0.248        |\n",
      "|    reward               | 0.01539684   |\n",
      "|    reward_max           | 0.902553     |\n",
      "|    reward_mean          | 0.0014455214 |\n",
      "|    reward_min           | -0.2222853   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.27         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 2206        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 10995.75    |\n",
      "|    clip_fraction        | 0.975       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.482      |\n",
      "|    learning_rate        | 0.000115    |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 95          |\n",
      "|    policy_gradient_loss | 0.243       |\n",
      "|    reward               | 0.00681425  |\n",
      "|    reward_max           | 0.6143006   |\n",
      "|    reward_mean          | 0.002353172 |\n",
      "|    reward_min           | -0.44494966 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 2206        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 10995.75    |\n",
      "|    clip_fraction        | 0.975       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.482      |\n",
      "|    learning_rate        | 0.000115    |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 95          |\n",
      "|    policy_gradient_loss | 0.243       |\n",
      "|    reward               | 0.00681425  |\n",
      "|    reward_max           | 0.6143006   |\n",
      "|    reward_mean          | 0.002353172 |\n",
      "|    reward_min           | -0.44494966 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 2316          |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7505.5703     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -2.33         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.258         |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | 0.206         |\n",
      "|    reward               | 0.07286126    |\n",
      "|    reward_max           | 0.3780543     |\n",
      "|    reward_mean          | -0.0008976623 |\n",
      "|    reward_min           | -0.6771673    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.887         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 2316          |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7505.5703     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -2.33         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.258         |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | 0.206         |\n",
      "|    reward               | 0.07286126    |\n",
      "|    reward_max           | 0.3780543     |\n",
      "|    reward_mean          | -0.0008976623 |\n",
      "|    reward_min           | -0.6771673    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.887         |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 2426        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6603.086    |\n",
      "|    clip_fraction        | 0.975       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -1.29       |\n",
      "|    learning_rate        | 0.000115    |\n",
      "|    loss                 | 0.323       |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | 0.234       |\n",
      "|    reward               | 3.678e-05   |\n",
      "|    reward_max           | 0.72427446  |\n",
      "|    reward_mean          | 0.001196188 |\n",
      "|    reward_min           | -0.33690983 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 2426        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6603.086    |\n",
      "|    clip_fraction        | 0.975       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -1.29       |\n",
      "|    learning_rate        | 0.000115    |\n",
      "|    loss                 | 0.323       |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | 0.234       |\n",
      "|    reward               | 3.678e-05   |\n",
      "|    reward_max           | 0.72427446  |\n",
      "|    reward_mean          | 0.001196188 |\n",
      "|    reward_min           | -0.33690983 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 2536          |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8747.699      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -1.24         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.402         |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | 0.242         |\n",
      "|    reward               | -0.02667965   |\n",
      "|    reward_max           | 0.48279157    |\n",
      "|    reward_mean          | 0.00022126127 |\n",
      "|    reward_min           | -0.20634514   |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.765         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 2536          |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8747.699      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -1.24         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.402         |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | 0.242         |\n",
      "|    reward               | -0.02667965   |\n",
      "|    reward_max           | 0.48279157    |\n",
      "|    reward_mean          | 0.00022126127 |\n",
      "|    reward_min           | -0.20634514   |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.765         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 2645          |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 11058.396     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.698        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 4.45          |\n",
      "|    n_updates            | 115           |\n",
      "|    policy_gradient_loss | 0.252         |\n",
      "|    reward               | -0.05186      |\n",
      "|    reward_max           | 0.45296675    |\n",
      "|    reward_mean          | 0.00011078626 |\n",
      "|    reward_min           | -0.2977348    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.65          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 2645          |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 11058.396     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.698        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 4.45          |\n",
      "|    n_updates            | 115           |\n",
      "|    policy_gradient_loss | 0.252         |\n",
      "|    reward               | -0.05186      |\n",
      "|    reward_max           | 0.45296675    |\n",
      "|    reward_mean          | 0.00011078626 |\n",
      "|    reward_min           | -0.2977348    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.65          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 2757         |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 24053.861    |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -78.7        |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 2.16         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | 0.109        |\n",
      "|    reward               | -0.01052201  |\n",
      "|    reward_max           | 1.105068     |\n",
      "|    reward_mean          | 0.0007418977 |\n",
      "|    reward_min           | -1.160882    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.44e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 2757         |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 24053.861    |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -78.7        |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 2.16         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | 0.109        |\n",
      "|    reward               | -0.01052201  |\n",
      "|    reward_max           | 1.105068     |\n",
      "|    reward_mean          | 0.0007418977 |\n",
      "|    reward_min           | -1.160882    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.44e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 2867          |\n",
      "|    total_timesteps      | 53248         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9205.393      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.771        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.507         |\n",
      "|    n_updates            | 125           |\n",
      "|    policy_gradient_loss | 0.255         |\n",
      "|    reward               | -0.0124035    |\n",
      "|    reward_max           | 0.31499895    |\n",
      "|    reward_mean          | -0.0009436629 |\n",
      "|    reward_min           | -0.6076815    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.488         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 2867          |\n",
      "|    total_timesteps      | 53248         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9205.393      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.771        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.507         |\n",
      "|    n_updates            | 125           |\n",
      "|    policy_gradient_loss | 0.255         |\n",
      "|    reward               | -0.0124035    |\n",
      "|    reward_max           | 0.31499895    |\n",
      "|    reward_mean          | -0.0009436629 |\n",
      "|    reward_min           | -0.6076815    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.488         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 2978          |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6930.588      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -1.47         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.458         |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | 0.242         |\n",
      "|    reward               | 0.06793473    |\n",
      "|    reward_max           | 0.4210081     |\n",
      "|    reward_mean          | -0.0011575534 |\n",
      "|    reward_min           | -0.22983778   |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.05          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 2978          |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6930.588      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -1.47         |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.458         |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | 0.242         |\n",
      "|    reward               | 0.06793473    |\n",
      "|    reward_max           | 0.4210081     |\n",
      "|    reward_mean          | -0.0011575534 |\n",
      "|    reward_min           | -0.22983778   |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.05          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 3125          |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7482.7393     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.919        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.469         |\n",
      "|    n_updates            | 135           |\n",
      "|    policy_gradient_loss | 0.255         |\n",
      "|    reward               | 0.04869365    |\n",
      "|    reward_max           | 0.48660204    |\n",
      "|    reward_mean          | -0.0018774334 |\n",
      "|    reward_min           | -1.0499489    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.19          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 3125          |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7482.7393     |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.919        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.469         |\n",
      "|    n_updates            | 135           |\n",
      "|    policy_gradient_loss | 0.255         |\n",
      "|    reward               | 0.04869365    |\n",
      "|    reward_max           | 0.48660204    |\n",
      "|    reward_mean          | -0.0018774334 |\n",
      "|    reward_min           | -1.0499489    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.19          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 3253          |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9436.812      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | 0.0339        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.318         |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | 0.253         |\n",
      "|    reward               | -0.0189835    |\n",
      "|    reward_max           | 0.561112      |\n",
      "|    reward_mean          | -0.0017177227 |\n",
      "|    reward_min           | -0.43898353   |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.541         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 18            |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 3253          |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9436.812      |\n",
      "|    clip_fraction        | 0.975         |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | 0.0339        |\n",
      "|    learning_rate        | 0.000115      |\n",
      "|    loss                 | 0.318         |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | 0.253         |\n",
      "|    reward               | -0.0189835    |\n",
      "|    reward_max           | 0.561112      |\n",
      "|    reward_mean          | -0.0017177227 |\n",
      "|    reward_min           | -0.43898353   |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.541         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 3362         |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7049.3174    |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -0.759       |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.422        |\n",
      "|    n_updates            | 145          |\n",
      "|    policy_gradient_loss | 0.246        |\n",
      "|    reward               | -0.01230964  |\n",
      "|    reward_max           | 0.7261062    |\n",
      "|    reward_mean          | 0.0014285331 |\n",
      "|    reward_min           | -0.69625694  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.238        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 3362         |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7049.3174    |\n",
      "|    clip_fraction        | 0.975        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -0.759       |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.422        |\n",
      "|    n_updates            | 145          |\n",
      "|    policy_gradient_loss | 0.246        |\n",
      "|    reward               | -0.01230964  |\n",
      "|    reward_max           | 0.7261062    |\n",
      "|    reward_mean          | 0.0014285331 |\n",
      "|    reward_min           | -0.69625694  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.238        |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 18             |\n",
      "|    iterations           | 31             |\n",
      "|    time_elapsed         | 3476           |\n",
      "|    total_timesteps      | 63488          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 9223.027       |\n",
      "|    clip_fraction        | 0.975          |\n",
      "|    clip_range           | 0.3            |\n",
      "|    entropy_loss         | -42.6          |\n",
      "|    explained_variance   | -0.46          |\n",
      "|    learning_rate        | 0.000115       |\n",
      "|    loss                 | 0.363          |\n",
      "|    n_updates            | 150            |\n",
      "|    policy_gradient_loss | 0.259          |\n",
      "|    reward               | -0.0259375     |\n",
      "|    reward_max           | 0.18798023     |\n",
      "|    reward_mean          | -0.00026977708 |\n",
      "|    reward_min           | -0.7006945     |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 0.736          |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 18             |\n",
      "|    iterations           | 31             |\n",
      "|    time_elapsed         | 3476           |\n",
      "|    total_timesteps      | 63488          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 9223.027       |\n",
      "|    clip_fraction        | 0.975          |\n",
      "|    clip_range           | 0.3            |\n",
      "|    entropy_loss         | -42.6          |\n",
      "|    explained_variance   | -0.46          |\n",
      "|    learning_rate        | 0.000115       |\n",
      "|    loss                 | 0.363          |\n",
      "|    n_updates            | 150            |\n",
      "|    policy_gradient_loss | 0.259          |\n",
      "|    reward               | -0.0259375     |\n",
      "|    reward_max           | 0.18798023     |\n",
      "|    reward_mean          | -0.00026977708 |\n",
      "|    reward_min           | -0.7006945     |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 0.736          |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-11-24 00:46:58,527] Trial 0 failed with parameters: {'batch_size': 256, 'n_steps': 2048, 'gamma': 0.999, 'learning_rate': 0.00011526449540315612, 'ent_coef': 1.8740223688836284e-07, 'clip_range': 0.3, 'n_epochs': 5, 'gae_lambda': 0.92, 'max_grad_norm': 0.8, 'vf_coef': 0.4401524937396013, 'net_arch': 'medium', 'activation_fn': 'relu'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/qr/80j1cy4s5q542dmdc5lwzzqc0000gn/T/ipykernel_76885/3147036171.py\", line 108, in objective\n",
      "    trained_model = self.agent.train_model(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/FinRL/finrl/agents/stablebaselines3/models.py\", line 147, in train_model\n",
      "    model = model.learn(\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py\", line 311, in learn\n",
      "    return super().learn(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py\", line 324, in learn\n",
      "    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py\", line 218, in collect_rollouts\n",
      "    new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\", line 222, in step\n",
      "    return self.step_wait()\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\", line 59, in step_wait\n",
      "    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n",
      "                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/FinRL/finrl/meta/env_stock_trading/env_stocktrading.py\", line 352, in step\n",
      "    self.state = self._update_state()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/FinRL/finrl/meta/env_stock_trading/env_stocktrading.py\", line 469, in _update_state\n",
      "    if len(self.df.tic.unique()) > 1:\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/pandas/core/series.py\", line 2407, in unique\n",
      "    return super().unique()\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/pandas/core/base.py\", line 1025, in unique\n",
      "    result = algorithms.unique1d(values)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/pandas/core/algorithms.py\", line 401, in unique\n",
      "    return unique_with_mask(values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ayushraj/Documents/Python/FinRL/venv/lib/python3.12/site-packages/pandas/core/algorithms.py\", line 440, in unique_with_mask\n",
      "    uniques = table.unique(values)\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-11-24 00:46:58,533] Trial 0 failed with value None.\n",
      "[W 2025-11-24 00:46:58,533] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m optimization_results = \u001b[43mrun_optuna_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_trade\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mALGORITHMS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOptimization Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m algo, result \u001b[38;5;129;01min\u001b[39;00m optimization_results.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 168\u001b[39m, in \u001b[36mrun_optuna_optimization\u001b[39m\u001b[34m(env_train, env_trade, algorithms)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Run optimization (this will call our enhanced objective method for each trial)\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîç Running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_TRIALS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m trials for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgo.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m study = \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgo.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m optimization completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    171\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Best trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_trial.number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with Sharpe: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/FinRL/finrl/agents/stablebaselines3/tune_sb3.py:201\u001b[39m, in \u001b[36mTuneSB3Optuna.run_optuna\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m sampler = optuna.samplers.TPESampler(seed=\u001b[32m42\u001b[39m)\n\u001b[32m    194\u001b[39m study = optuna.create_study(\n\u001b[32m    195\u001b[39m     study_name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_study\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    196\u001b[39m     direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    197\u001b[39m     sampler=sampler,\n\u001b[32m    198\u001b[39m     pruner=optuna.pruners.HyperbandPruner(),\n\u001b[32m    199\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogging_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m joblib.dump(study, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_study.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m study\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/optuna/study/_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/optuna/study/_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/optuna/study/_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/optuna/study/_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mrun_optuna_optimization.<locals>.EnhancedTuneSB3Optuna.objective\u001b[39m\u001b[34m(self, trial)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  üéØ Trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.total_timesteps\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m timesteps...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m trained_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_trial_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Save trial model (use same format as original tune_sb3.py - .pth files)\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# This ensures compatibility with the original backtest method\u001b[39;00m\n\u001b[32m    116\u001b[39m trial_model_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.TRAINED_MODEL_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/FinRL/finrl/agents/stablebaselines3/models.py:147\u001b[39m, in \u001b[36mDRLAgent.train_model\u001b[39m\u001b[34m(model, tb_log_name, total_timesteps, callbacks)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model\u001b[39m(\n\u001b[32m    142\u001b[39m     model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    145\u001b[39m     callbacks: Type[BaseCallback] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    146\u001b[39m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCallbackList\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    155\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[32m    216\u001b[39m         clipped_actions = np.clip(actions, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/FinRL/finrl/meta/env_stock_trading/env_stocktrading.py:352\u001b[39m, in \u001b[36mStockTradingEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.df.tic.unique()) > \u001b[32m1\u001b[39m:\n\u001b[32m    351\u001b[39m         \u001b[38;5;28mself\u001b[39m.turbulence = \u001b[38;5;28mself\u001b[39m.data[\u001b[38;5;28mself\u001b[39m.risk_indicator_col].values[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m \u001b[38;5;28mself\u001b[39m.state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m end_total_asset = \u001b[38;5;28mself\u001b[39m.state[\u001b[32m0\u001b[39m] + \u001b[38;5;28msum\u001b[39m(\n\u001b[32m    355\u001b[39m     np.array(\u001b[38;5;28mself\u001b[39m.state[\u001b[32m1\u001b[39m : (\u001b[38;5;28mself\u001b[39m.stock_dim + \u001b[32m1\u001b[39m)])\n\u001b[32m    356\u001b[39m     * np.array(\u001b[38;5;28mself\u001b[39m.state[(\u001b[38;5;28mself\u001b[39m.stock_dim + \u001b[32m1\u001b[39m) : (\u001b[38;5;28mself\u001b[39m.stock_dim * \u001b[32m2\u001b[39m + \u001b[32m1\u001b[39m)])\n\u001b[32m    357\u001b[39m )\n\u001b[32m    358\u001b[39m \u001b[38;5;28mself\u001b[39m.asset_memory.append(end_total_asset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/FinRL/finrl/meta/env_stock_trading/env_stocktrading.py:469\u001b[39m, in \u001b[36mStockTradingEnv._update_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtic\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) > \u001b[32m1\u001b[39m:\n\u001b[32m    470\u001b[39m         \u001b[38;5;66;03m# for multiple stock\u001b[39;00m\n\u001b[32m    471\u001b[39m         state = (\n\u001b[32m    472\u001b[39m             [\u001b[38;5;28mself\u001b[39m.state[\u001b[32m0\u001b[39m]]\n\u001b[32m    473\u001b[39m             + \u001b[38;5;28mself\u001b[39m.data.close.values.tolist()\n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m             )\n\u001b[32m    482\u001b[39m         )\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    485\u001b[39m         \u001b[38;5;66;03m# for single stock\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/pandas/core/series.py:2407\u001b[39m, in \u001b[36mSeries.unique\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[32m   2345\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2346\u001b[39m \u001b[33;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[32m   2347\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2405\u001b[39m \u001b[33;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[32m   2406\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/pandas/core/base.py:1025\u001b[39m, in \u001b[36mIndexOpsMixin.unique\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m     result = values.unique()\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     result = \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:401\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique\u001b[39m(values):\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[32m    310\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m \u001b[33;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/FinRL/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:440\u001b[39m, in \u001b[36munique_with_mask\u001b[39m\u001b[34m(values, mask)\u001b[39m\n\u001b[32m    438\u001b[39m table = hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     uniques = \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     uniques = _reconstruct_data(uniques, original.dtype, original)\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimization_results = run_optuna_optimization(env_train, env_trade, ALGORITHMS)\n",
    "\n",
    "print(\"\\nOptimization Summary:\")\n",
    "for algo, result in optimization_results.items():\n",
    "    if 'error' in result:\n",
    "        print(f\"{algo.upper()}: Failed - {result['error']}\")\n",
    "    else:\n",
    "        print(f\"{algo.upper()}: Best Sharpe = {result['best_value']:.4f} (Trial {result['best_trial'].number})\")\n",
    "\n",
    "print(f\"\\nResults saved in:\")\n",
    "print(f\"  Trained models: {config.TRAINED_MODEL_DIR}\")\n",
    "print(f\"  Study files: Current directory (*_study.pkl)\")\n",
    "print(f\"  Performance stats: {config.RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c125fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Analysis and Visualization\n",
    "# Analyze the optimization results and create visualizations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_optimization_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and visualize optimization results.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ùå No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä Detailed Results Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_data = []\n",
    "    \n",
    "    for algo, result in results.items():\n",
    "        if 'error' not in result:\n",
    "            comparison_data.append({\n",
    "                'Algorithm': algo.upper(),\n",
    "                'Best_Sharpe': result['best_value'],\n",
    "                'Best_Trial': result['best_trial'].number,\n",
    "                'Total_Trials': len(result['study'].trials)\n",
    "            })\n",
    "            \n",
    "            print(f\"\\\\nüéØ {algo.upper()} Results:\")\n",
    "            print(f\"   - Best Sharpe Ratio: {result['best_value']:.4f}\")\n",
    "            print(f\"   - Best Trial Number: {result['best_trial'].number}\")\n",
    "            print(f\"   - Total Trials: {len(result['study'].trials)}\")\n",
    "            print(f\"   - Best Parameters:\")\n",
    "            for param, value in result['best_params'].items():\n",
    "                print(f\"     ‚Ä¢ {param}: {value}\")\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot 1: Best Sharpe Ratio Comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.barplot(data=comparison_df, x='Algorithm', y='Best_Sharpe')\n",
    "        plt.title('Best Sharpe Ratio by Algorithm')\n",
    "        plt.ylabel('Sharpe Ratio')\n",
    "        \n",
    "        # Plot 2: Trial Convergence (if available)\n",
    "        plt.subplot(2, 2, 2)\n",
    "        for algo, result in results.items():\n",
    "            if 'error' not in result:\n",
    "                trial_values = [trial.value for trial in result['study'].trials if trial.value is not None]\n",
    "                plt.plot(trial_values, label=algo.upper(), marker='o', alpha=0.7)\n",
    "        plt.title('Optimization Progress')\n",
    "        plt.xlabel('Trial')\n",
    "        plt.ylabel('Sharpe Ratio')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot 3: Best Trial Numbers\n",
    "        plt.subplot(2, 2, 3)\n",
    "        sns.barplot(data=comparison_df, x='Algorithm', y='Best_Trial')\n",
    "        plt.title('Best Trial Number by Algorithm')\n",
    "        plt.ylabel('Trial Number')\n",
    "        \n",
    "        # Plot 4: Total Trials\n",
    "        plt.subplot(2, 2, 4)\n",
    "        sns.barplot(data=comparison_df, x='Algorithm', y='Total_Trials')\n",
    "        plt.title('Total Trials by Algorithm')\n",
    "        plt.ylabel('Number of Trials')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display comparison table\n",
    "        print(f\"\\\\nüìã Summary Table:\")\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Find best overall algorithm\n",
    "        best_algo = comparison_df.loc[comparison_df['Best_Sharpe'].idxmax()]\n",
    "        print(f\"\\\\nüèÜ Best Overall Algorithm: {best_algo['Algorithm']}\")\n",
    "        print(f\"   - Sharpe Ratio: {best_algo['Best_Sharpe']:.4f}\")\n",
    "\n",
    "# Analyze results if optimization was run\n",
    "if 'optimization_results' in locals() and optimization_results:\n",
    "    analyze_optimization_results(optimization_results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No optimization results found. Run the optimization first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d36cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Management and Utilities\n",
    "def load_best_model(algo):\n",
    "    \"\"\"Load the best saved model for a given algorithm.\"\"\"\n",
    "    from stable_baselines3 import PPO, SAC, TD3\n",
    "    \n",
    "    model_classes = {\n",
    "        'ppo': PPO,\n",
    "        'sac': SAC, \n",
    "        'td3': TD3\n",
    "    }\n",
    "    \n",
    "    if algo.lower() not in model_classes:\n",
    "        raise ValueError(f\"Unsupported algorithm: {algo}\")\n",
    "    \n",
    "    model_path = get_best_model_path(algo.lower())\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"No best model found for {algo.upper()}\")\n",
    "    \n",
    "    ModelClass = model_classes[algo.lower()]\n",
    "    model = ModelClass.load(model_path)\n",
    "    print(f\"‚úÖ Loaded best {algo.upper()} model from: {model_path}\")\n",
    "    return model\n",
    "\n",
    "def get_model_performance_summary():\n",
    "    \"\"\"Get a comprehensive performance summary of all best models.\"\"\"\n",
    "    metadata = load_best_models_metadata()\n",
    "    \n",
    "    if not metadata:\n",
    "        print(\"‚ÑπÔ∏è  No best models found.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìä COMPREHENSIVE MODEL PERFORMANCE SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for algo, info in metadata.items():\n",
    "        summary_data.append({\n",
    "            'Algorithm': algo.upper(),\n",
    "            'Best_Sharpe': info.get('best_sharpe', 0),\n",
    "            'Improvement': info.get('improvement', 0),\n",
    "            'Best_Trial': info.get('best_trial', 'N/A'),\n",
    "            'Total_Trials': info.get('total_trials', 'N/A'),\n",
    "            'Training_Steps': info.get('training_timesteps', 'N/A'),\n",
    "            'Date_Saved': info.get('timestamp', '')[:19] if info.get('timestamp') else 'Unknown'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nüéØ {algo.upper()} DETAILS:\")\n",
    "        print(f\"   Best Sharpe Ratio: {info.get('best_sharpe', 0):.4f}\")\n",
    "        print(f\"   Improvement: +{info.get('improvement', 0):.4f}\")\n",
    "        print(f\"   Best Trial: {info.get('best_trial', 'N/A')}\")\n",
    "        print(f\"   Total Trials: {info.get('total_trials', 'N/A')}\")\n",
    "        print(f\"   Training Steps: {info.get('training_timesteps', 'N/A'):,}\")\n",
    "        print(f\"   Date Saved: {info.get('timestamp', 'Unknown')[:19]}\")\n",
    "        print(f\"   Model Path: {info.get('model_path', 'N/A')}\")\n",
    "        print(f\"   Log Path: {info.get('log_path', 'N/A')}\")\n",
    "        \n",
    "        # Show best hyperparameters\n",
    "        best_params = info.get('best_params', {})\n",
    "        if best_params:\n",
    "            print(f\"   Best Parameters:\")\n",
    "            for param, value in best_params.items():\n",
    "                print(f\"     ‚Ä¢ {param}: {value}\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    if len(summary_df) > 0:\n",
    "        print(f\"\\nüìã SUMMARY TABLE:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "        \n",
    "        # Find best overall algorithm\n",
    "        best_idx = summary_df['Best_Sharpe'].idxmax()\n",
    "        best_algo_info = summary_df.loc[best_idx]\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST OVERALL ALGORITHM: {best_algo_info['Algorithm']}\")\n",
    "        print(f\"   Sharpe Ratio: {best_algo_info['Best_Sharpe']:.4f}\")\n",
    "        print(f\"   Date Achieved: {best_algo_info['Date_Saved']}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def clean_old_trial_models(keep_best_n=5):\n",
    "    \"\"\"Clean up old trial models, keeping only the best N trials for each algorithm.\"\"\"\n",
    "    print(f\"üßπ Cleaning old trial models (keeping top {keep_best_n} for each algorithm)...\")\n",
    "    \n",
    "    trial_files = [f for f in os.listdir(BEST_MODELS_DIR) if f.startswith('trial_') and f.endswith('.zip')]\n",
    "    \n",
    "    # Group by algorithm\n",
    "    algo_trials = {}\n",
    "    for file in trial_files:\n",
    "        parts = file.replace('trial_', '').replace('.zip', '').split('_')\n",
    "        if len(parts) >= 2:\n",
    "            algo = parts[0]\n",
    "            trial_num = parts[1]\n",
    "            if algo not in algo_trials:\n",
    "                algo_trials[algo] = []\n",
    "            algo_trials[algo].append((file, trial_num))\n",
    "    \n",
    "    cleaned_count = 0\n",
    "    for algo, trials in algo_trials.items():\n",
    "        if len(trials) > keep_best_n:\n",
    "            # Sort by trial number (assuming higher trial numbers are more recent)\n",
    "            trials.sort(key=lambda x: int(x[1]) if x[1].isdigit() else 0, reverse=True)\n",
    "            \n",
    "            # Remove older trials\n",
    "            for file, trial_num in trials[keep_best_n:]:\n",
    "                file_path = os.path.join(BEST_MODELS_DIR, file)\n",
    "                if os.path.exists(file_path):\n",
    "                    os.remove(file_path)\n",
    "                    cleaned_count += 1\n",
    "                    print(f\"  üóëÔ∏è  Removed {file}\")\n",
    "    \n",
    "    print(f\"‚úÖ Cleanup complete. Removed {cleaned_count} old trial models.\")\n",
    "\n",
    "# Display current status\n",
    "print(\"üîß Model management utilities loaded:\")\n",
    "print(\"  ‚Ä¢ load_best_model(algo) - Load best saved model\")\n",
    "print(\"  ‚Ä¢ get_model_performance_summary() - Show comprehensive performance summary\")  \n",
    "print(\"  ‚Ä¢ clean_old_trial_models(keep_best_n=5) - Clean up old trial models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9912e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage and Quick Start Guide\n",
    "\n",
    "print(\"üöÄ FINRL OPTUNA ENHANCED - QUICK START GUIDE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ RUN OPTIMIZATION:\")\n",
    "print(\"   optimization_results = run_optuna_optimization(env_train, env_trade, ALGORITHMS)\")\n",
    "print(\"   # This will automatically save best models and logs\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ ANALYZE RESULTS:\")\n",
    "print(\"   analyze_optimization_results(optimization_results)\")\n",
    "print(\"   # Shows detailed analysis with visualizations\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ GET PERFORMANCE SUMMARY:\")\n",
    "print(\"   summary_df = get_model_performance_summary()\")\n",
    "print(\"   # Shows comprehensive model performance across all runs\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ LOAD BEST MODEL:\")\n",
    "print(\"   best_ppo = load_best_model('ppo')\")\n",
    "print(\"   # Loads the best saved PPO model\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ CLEAN OLD FILES:\")\n",
    "print(\"   clean_old_trial_models(keep_best_n=3)\")\n",
    "print(\"   # Keeps only top 3 trial models per algorithm\")\n",
    "\n",
    "print(\"\\nüìÅ KEY DIRECTORIES:\")\n",
    "print(f\"   Best Models: {BEST_MODELS_DIR}\")\n",
    "print(f\"   Logs: {LOGS_DIR}\")\n",
    "print(f\"   Metadata: {BEST_MODELS_METADATA_FILE}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to run! Execute the cells above to start optimization.\")\n",
    "\n",
    "# Show current system status\n",
    "if len(load_best_models_metadata()) > 0:\n",
    "    print(f\"\\nüìä Current best models summary:\")\n",
    "    get_model_performance_summary()\n",
    "else:\n",
    "    print(f\"\\n‚ÑπÔ∏è  No previous models found. Ready for first optimization run!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîç ANALYZING TRAINED PPO MODEL DIMENSIONS\n",
      "======================================================================\n",
      "‚úÖ Model file found: /Users/ayushraj/Documents/Python/FinRL/FinRL/examples/trained_models/sac_best_model.zip\n",
      "   File size: 0.90 MB\n",
      "\n",
      "üì¶ Loading model...\n",
      "\n",
      "üìä MODEL DIMENSIONS:\n",
      "  Observation space: Box(-inf, inf, (211,), float32)\n",
      "  Observation dimension: 211\n",
      "  Action space: Box(-1.0, 1.0, (30,), float32)\n",
      "  Action dimension: 30\n",
      "\n",
      "üèóÔ∏è  POLICY NETWORK ARCHITECTURE:\n",
      "\n",
      "‚öôÔ∏è  MODEL HYPERPARAMETERS:\n",
      "  Learning rate: 0.002800682007731022\n",
      "  N steps: 1\n",
      "  Batch size: 128\n",
      "  Gamma (discount): 0.995\n",
      "\n",
      "üßÆ REVERSE ENGINEERING ENVIRONMENT PARAMETERS:\n",
      "  Observation dimension: 211\n",
      "  Number of indicators: 8\n",
      "  Computed stock dimension: 21.00\n",
      "  ‚úÖ Stock dimension (exact): 21\n",
      "  Formula check: 1 + 2√ó21 + 8√ó21 = 211\n",
      "\n",
      "üîÑ COMPARING WITH CURRENT ENVIRONMENT:\n",
      "  Current env observation dim: 301\n",
      "  Current env action dim: 30\n",
      "  ‚ö†Ô∏è  MISMATCH: Model has 211, current env has 301\n",
      "      Difference: 90\n",
      "  ‚úÖ Model action space matches current environment!\n",
      "\n",
      "======================================================================\n",
      "‚úÖ MODEL ANALYSIS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# INSPECT TRAINED MODEL DIMENSIONS\n",
    "# Analyze the dimensions of a saved model\n",
    "\n",
    "from stable_baselines3 import PPO, SAC\n",
    "import os\n",
    "\n",
    "# Use relative path - update this to point to your model file\n",
    "model_filename = 'sac_best_model.zip'  # Change this to your model filename\n",
    "model_path = os.path.join(project_root, 'trained_models', model_filename)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç ANALYZING TRAINED MODEL DIMENSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if model file exists\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"‚ùå Model file not found: {model_path}\")\n",
    "    print(f\"   Please update the model_filename variable to point to your trained model\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model file found: {os.path.basename(model_path)}\")\n",
    "    file_size = os.path.getsize(model_path) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"   File size: {file_size:.2f} MB\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model without environment (to inspect saved parameters)\n",
    "        print(\"\\nüì¶ Loading model...\")\n",
    "        model = SAC.load(model_path)  # Change to PPO or TD3 if needed\n",
    "        \n",
    "        # Get observation and action space from model\n",
    "        print(\"\\nüìä MODEL DIMENSIONS:\")\n",
    "        \n",
    "        # Observation space\n",
    "        if hasattr(model, 'observation_space'):\n",
    "            obs_space = model.observation_space\n",
    "            print(f\"  Observation space: {obs_space}\")\n",
    "            if hasattr(obs_space, 'shape'):\n",
    "                print(f\"  Observation dimension: {obs_space.shape[0]}\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Could not determine observation space\")\n",
    "        \n",
    "        # Action space\n",
    "        if hasattr(model, 'action_space'):\n",
    "            act_space = model.action_space\n",
    "            print(f\"  Action space: {act_space}\")\n",
    "            if hasattr(act_space, 'shape') and act_space.shape:\n",
    "                print(f\"  Action dimension: {act_space.shape[0]}\")\n",
    "            elif hasattr(act_space, 'n'):\n",
    "                print(f\"  Action dimension: {act_space.n}\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Could not determine action space\")\n",
    "        \n",
    "        # Policy network architecture\n",
    "        print(\"\\nüèóÔ∏è  POLICY NETWORK ARCHITECTURE:\")\n",
    "        if hasattr(model, 'policy'):\n",
    "            policy = model.policy\n",
    "            \n",
    "            # Check for actor network (policy network)\n",
    "            if hasattr(policy, 'mlp_extractor'):\n",
    "                mlp = policy.mlp_extractor\n",
    "                print(f\"  Policy type: {type(policy).__name__}\")\n",
    "                \n",
    "                # Get network layers\n",
    "                if hasattr(mlp, 'policy_net'):\n",
    "                    print(f\"  Policy network: {mlp.policy_net}\")\n",
    "                if hasattr(mlp, 'value_net'):\n",
    "                    print(f\"  Value network: {mlp.value_net}\")\n",
    "            \n",
    "            # Get network features\n",
    "            if hasattr(policy, 'features_dim'):\n",
    "                print(f\"  Features dimension: {policy.features_dim}\")\n",
    "        \n",
    "        # Model hyperparameters\n",
    "        print(\"\\n‚öôÔ∏è  MODEL HYPERPARAMETERS:\")\n",
    "        if hasattr(model, 'learning_rate'):\n",
    "            lr = model.learning_rate\n",
    "            if callable(lr):\n",
    "                print(f\"  Learning rate: {lr(1.0)} (schedule)\")\n",
    "            else:\n",
    "                print(f\"  Learning rate: {lr}\")\n",
    "        \n",
    "        if hasattr(model, 'n_steps'):\n",
    "            print(f\"  N steps: {model.n_steps}\")\n",
    "        if hasattr(model, 'batch_size'):\n",
    "            print(f\"  Batch size: {model.batch_size}\")\n",
    "        if hasattr(model, 'n_epochs'):\n",
    "            print(f\"  N epochs: {model.n_epochs}\")\n",
    "        if hasattr(model, 'gamma'):\n",
    "            print(f\"  Gamma (discount): {model.gamma}\")\n",
    "        if hasattr(model, 'gae_lambda'):\n",
    "            print(f\"  GAE lambda: {model.gae_lambda}\")\n",
    "        if hasattr(model, 'clip_range'):\n",
    "            clip = model.clip_range\n",
    "            if callable(clip):\n",
    "                print(f\"  Clip range: {clip(1.0)} (schedule)\")\n",
    "            else:\n",
    "                print(f\"  Clip range: {clip}\")\n",
    "        \n",
    "        # Compute expected state space from observation dimension\n",
    "        if hasattr(model, 'observation_space') and hasattr(model.observation_space, 'shape'):\n",
    "            obs_dim = model.observation_space.shape[0]\n",
    "            \n",
    "            print(f\"\\nüßÆ REVERSE ENGINEERING ENVIRONMENT PARAMETERS:\")\n",
    "            print(f\"  Observation dimension: {obs_dim}\")\n",
    "            \n",
    "            # Try common indicator counts (from FinRL)\n",
    "            from finrl.config import INDICATORS\n",
    "            n_indicators = len(INDICATORS)\n",
    "            \n",
    "            # Formula: state_space = 1 + 2*stock_dim + n_indicators*stock_dim\n",
    "            # Solving for stock_dim: obs_dim = 1 + 2*stock_dim + n_indicators*stock_dim\n",
    "            #                        obs_dim - 1 = stock_dim * (2 + n_indicators)\n",
    "            #                        stock_dim = (obs_dim - 1) / (2 + n_indicators)\n",
    "            \n",
    "            if n_indicators > 0:\n",
    "                stock_dim_computed = (obs_dim - 1) / (2 + n_indicators)\n",
    "                print(f\"  Number of indicators: {n_indicators}\")\n",
    "                print(f\"  Computed stock dimension: {stock_dim_computed:.2f}\")\n",
    "                \n",
    "                if stock_dim_computed == int(stock_dim_computed):\n",
    "                    stock_dim = int(stock_dim_computed)\n",
    "                    print(f\"  ‚úÖ Stock dimension (exact): {stock_dim}\")\n",
    "                    print(f\"  Formula check: 1 + 2√ó{stock_dim} + {n_indicators}√ó{stock_dim} = {1 + 2*stock_dim + n_indicators*stock_dim}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è  Non-integer stock dimension - formula may not match\")\n",
    "            \n",
    "            # Compare with current environment if available\n",
    "            if 'env_train' in globals():\n",
    "                print(f\"\\nüîÑ COMPARING WITH CURRENT ENVIRONMENT:\")\n",
    "                try:\n",
    "                    if hasattr(env_train, 'envs') and len(env_train.envs) > 0:\n",
    "                        current_obs = env_train.envs[0].observation_space.shape[0]\n",
    "                        current_act = env_train.envs[0].action_space.shape[0]\n",
    "                    else:\n",
    "                        current_obs = env_train.observation_space.shape[0]\n",
    "                        current_act = env_train.action_space.shape[0]\n",
    "                    \n",
    "                    print(f\"  Current env observation dim: {current_obs}\")\n",
    "                    print(f\"  Current env action dim: {current_act}\")\n",
    "                    \n",
    "                    if current_obs == obs_dim:\n",
    "                        print(f\"  ‚úÖ Model observation matches current environment!\")\n",
    "                    else:\n",
    "                        print(f\"  ‚ö†Ô∏è  MISMATCH: Model has {obs_dim}, current env has {current_obs}\")\n",
    "                        print(f\"      Difference: {abs(obs_dim - current_obs)}\")\n",
    "                    \n",
    "                    model_act_dim = model.action_space.shape[0] if hasattr(model.action_space, 'shape') else None\n",
    "                    if model_act_dim and model_act_dim == current_act:\n",
    "                        print(f\"  ‚úÖ Model action space matches current environment!\")\n",
    "                    elif model_act_dim:\n",
    "                        print(f\"  ‚ö†Ô∏è  MISMATCH: Model has {model_act_dim}, current env has {current_act}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  Could not compare with current environment: {e}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ MODEL ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca71303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
