{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c052cdb7",
   "metadata": {},
   "source": [
    "# Paper Trading on Alpaca with Real-Time Fine-Tuning\n",
    "\n",
    "This notebook implements a complete paper trading system that:\n",
    "- **Fetches real-time 1-min data** from Alpaca API\n",
    "- **Stores data** in CSV for persistence\n",
    "- **Executes paper trades** on Alpaca account\n",
    "- **Fine-tunes the model** every 2 hours with latest data\n",
    "- **Validates and accepts/rejects** fine-tuned models\n",
    "- **Tracks performance** with detailed logging\n",
    "\n",
    "## Workflow\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Market Open                                        â”‚\n",
    "â”‚  â†“                                                  â”‚\n",
    "â”‚  Fetch 1-min data from Alpaca API                 â”‚\n",
    "â”‚  â†“                                                  â”‚\n",
    "â”‚  Append to CSV (paper_trading_data_1min.csv)       â”‚\n",
    "â”‚  â†“                                                  â”‚\n",
    "â”‚  Get current state (positions, cash, prices)       â”‚\n",
    "â”‚  â†“                                                  â”‚\n",
    "â”‚  Predict action with current model                 â”‚\n",
    "â”‚  â†“                                                  â”‚\n",
    "â”‚  Execute trades on Alpaca paper account            â”‚\n",
    "â”‚  â†“                                                  â”‚\n",
    "â”‚  Wait 1 minute â†’ Loop                              â”‚\n",
    "â”‚                                                      â”‚\n",
    "â”‚  Every 2 hours:                                     â”‚\n",
    "â”‚  â”œâ”€ Load last 48h from CSV                         â”‚\n",
    "â”‚  â”œâ”€ Split train/validation (80/20)                 â”‚\n",
    "â”‚  â”œâ”€ Fine-tune model on train set                   â”‚\n",
    "â”‚  â”œâ”€ Evaluate on validation set                     â”‚\n",
    "â”‚  â”œâ”€ Accept if score >= 95% of original             â”‚\n",
    "â”‚  â””â”€ Save model if accepted                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Disclaimer**: This is for educational purposes. Not financial advice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5cbc15",
   "metadata": {},
   "source": [
    "# Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c98d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get relative paths dynamically\n",
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"âœ“ Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import threading\n",
    "import tempfile\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FinRL imports\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.config import INDICATORS\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.meta.data_processors.processor_alpaca import AlpacaProcessor\n",
    "\n",
    "# DRL imports\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Alpaca imports\n",
    "import alpaca_trade_api as tradeapi\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# PyTorch (for ElegantRL agent)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb964cee",
   "metadata": {},
   "source": [
    "# Part 2: Configuration and API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API credentials from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from notebook directory\n",
    "env_path = os.path.join(notebook_dir, '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "API_KEY = os.getenv('ALPACA_API_KEY')\n",
    "API_SECRET = os.getenv('ALPACA_API_SECRET')\n",
    "API_BASE_URL = os.getenv('ALPACA_API_BASE_URL')\n",
    "\n",
    "# Technical indicators\n",
    "TECH_INDICATORS = INDICATORS\n",
    "\n",
    "# Tickers (DOW 30)\n",
    "TICKERS = DOW_30_TICKER\n",
    "\n",
    "print(f\"âœ“ API credentials loaded from .env\")\n",
    "print(f\"âœ“ Using {len(TICKERS)} tickers\")\n",
    "print(f\"âœ“ Technical indicators: {len(TECH_INDICATORS)} indicators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main configuration\n",
    "CONFIG = {\n",
    "    # Alpaca API\n",
    "    'API_KEY': API_KEY,\n",
    "    'API_SECRET': API_SECRET,\n",
    "    'API_BASE_URL': API_BASE_URL,\n",
    "    \n",
    "    # Tickers and dimensions\n",
    "    'TICKERS': TICKERS,\n",
    "    'STOCK_DIM': len(TICKERS),\n",
    "    \n",
    "    # Model paths (Stable Baselines3 PPO)\n",
    "    'TRAINED_MODEL': 'trained_models/agent_ppo.zip',  # SB3 PPO model (relative to FinRL root)\n",
    "    'OUTPUT_DIR': 'paper_trading_finetune_results',\n",
    "    \n",
    "    # Trading parameters\n",
    "    'INITIAL_CASH': 1_000_000,\n",
    "    'HMAX': 100,\n",
    "    'TRANSACTION_COST_PCT': 0.001,\n",
    "    'REWARD_SCALING': 1e-4,\n",
    "    'TURBULENCE_THRESH': 30,\n",
    "    'MIN_ACTION_THRESHOLD': 5,\n",
    "    \n",
    "    # Fine-tuning parameters\n",
    "    'FINETUNE_INTERVAL_HOURS': 2,\n",
    "    'FINETUNE_LOOKBACK_HOURS': 48,\n",
    "    'FINETUNE_LR': 1e-5,\n",
    "    'FINETUNE_STEPS': 2000,\n",
    "    'VALIDATION_SPLIT': 0.2,\n",
    "    'VALIDATION_THRESHOLD': 0.95,\n",
    "    \n",
    "    # Data storage\n",
    "    'DATA_CSV': 'paper_trading_data_1min.csv',\n",
    "}\n",
    "\n",
    "# Calculate state and action dimensions (Production format: 301 features)\n",
    "# State: 1 (cash) + 30 (prices) + 30 (stocks) + 240 (tech indicators) = 301\n",
    "# NO turbulence in state vector!\n",
    "action_dim = CONFIG['STOCK_DIM']\n",
    "state_dim = 1 + 2 * action_dim + len(TECH_INDICATORS) * action_dim\n",
    "\n",
    "CONFIG['action_dim'] = action_dim\n",
    "CONFIG['state_dim'] = state_dim\n",
    "\n",
    "print(\"\\nðŸ“‹ CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {CONFIG['TRAINED_MODEL']} (Stable Baselines3 PPO)\")\n",
    "print(f\"Output: {CONFIG['OUTPUT_DIR']}\")\n",
    "print(f\"Data CSV: {CONFIG['DATA_CSV']}\")\n",
    "print(f\"Tickers: {CONFIG['STOCK_DIM']}\")\n",
    "print(f\"State dim: {state_dim}, Action dim: {action_dim} (Production format)\")\n",
    "print(f\"Initial cash: ${CONFIG['INITIAL_CASH']:,}\")\n",
    "print(f\"Fine-tune: Every {CONFIG['FINETUNE_INTERVAL_HOURS']}h using last {CONFIG['FINETUNE_LOOKBACK_HOURS']}h data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570fa611",
   "metadata": {},
   "source": [
    "# Part 3: Data Collection Functions\n",
    "\n",
    "These functions fetch 1-min OHLCV data from Alpaca and store in CSV for fine-tuning.\n",
    "\n",
    "## Key Features:\n",
    "- **Live OHLCV bars**: Gets open, high, low, close, volume for all 30 tickers\n",
    "- **Minute-level rounding**: Timestamps rounded to the minute for consistency\n",
    "- **Technical indicators**: Calculates all FinRL indicators per symbol\n",
    "- **Historical backfill**: Fetches 2 trading days of historical data on startup\n",
    "- **Duplicate prevention**: Checks timestamps before appending\n",
    "- **Robust calculation**: Uses expanding windows for indicators to avoid NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207703ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data_csv(csv_path):\n",
    "    \"\"\"Initialize CSV file for real-time data collection.\"\"\"\n",
    "    if not Path(csv_path).exists():\n",
    "        columns = ['timestamp', 'tic', 'open', 'high', 'low', 'close', 'volume'] + TECH_INDICATORS\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"âœ“ Initialized data CSV: {csv_path}\")\n",
    "    else:\n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "        print(f\"âœ“ Data CSV exists: {csv_path} ({len(existing_df):,} records)\")\n",
    "\n",
    "\n",
    "def calculate_indicators(df):\n",
    "    \"\"\"\n",
    "    Calculate FinRL indicators with proper handling for initial periods.\n",
    "    Uses expanding windows to avoid NaN values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        if len(df) < 2:\n",
    "            # Set defaults for insufficient data\n",
    "            df['macd'] = 0.0\n",
    "            df['boll_ub'] = df['close']\n",
    "            df['boll_lb'] = df['close']\n",
    "            df['rsi_30'] = 50.0\n",
    "            df['cci_30'] = 0.0\n",
    "            df['dx_30'] = 0.0\n",
    "            df['close_30_sma'] = df['close']\n",
    "            df['close_60_sma'] = df['close']\n",
    "            return df\n",
    "        \n",
    "        # MACD (12-26 EMA)\n",
    "        if len(df) >= 12:\n",
    "            ema12 = df['close'].ewm(span=12, adjust=False, min_periods=1).mean()\n",
    "            ema26 = df['close'].ewm(span=26, adjust=False, min_periods=1).mean()\n",
    "            df['macd'] = ema12 - ema26\n",
    "        else:\n",
    "            df['macd'] = 0.0\n",
    "        \n",
    "        # Bollinger Bands (20-period)\n",
    "        if len(df) >= 20:\n",
    "            sma20 = df['close'].rolling(20, min_periods=1).mean()\n",
    "            std20 = df['close'].rolling(20, min_periods=1).std()\n",
    "            df['boll_ub'] = sma20 + (2 * std20)\n",
    "            df['boll_lb'] = sma20 - (2 * std20)\n",
    "        else:\n",
    "            expanding_mean = df['close'].expanding(min_periods=1).mean()\n",
    "            expanding_std = df['close'].expanding(min_periods=1).std()\n",
    "            df['boll_ub'] = expanding_mean + (2 * expanding_std.fillna(0))\n",
    "            df['boll_lb'] = expanding_mean - (2 * expanding_std.fillna(0))\n",
    "        \n",
    "        # RSI-30\n",
    "        delta = df['close'].diff()\n",
    "        if len(df) >= 30:\n",
    "            gain = delta.where(delta > 0, 0).rolling(30, min_periods=1).mean()\n",
    "            loss = -delta.where(delta < 0, 0).rolling(30, min_periods=1).mean()\n",
    "        else:\n",
    "            gain = delta.where(delta > 0, 0).expanding(min_periods=1).mean()\n",
    "            loss = -delta.where(delta < 0, 0).expanding(min_periods=1).mean()\n",
    "        rs = gain / loss.replace(0, 1e-10)\n",
    "        df['rsi_30'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # CCI-30\n",
    "        tp = (df['high'] + df['low'] + df['close']) / 3\n",
    "        if len(df) >= 30:\n",
    "            sma_tp = tp.rolling(30, min_periods=1).mean()\n",
    "            mad = tp.rolling(30, min_periods=1).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "        else:\n",
    "            sma_tp = tp.expanding(min_periods=1).mean()\n",
    "            mad = tp.expanding(min_periods=1).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "        df['cci_30'] = (tp - sma_tp) / (0.015 * mad.replace(0, 1e-10))\n",
    "        \n",
    "        # DX-30 (Directional Movement Index)\n",
    "        high_low = df['high'] - df['low']\n",
    "        high_close = np.abs(df['high'] - df['close'].shift())\n",
    "        low_close = np.abs(df['low'] - df['close'].shift())\n",
    "        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "        \n",
    "        if len(df) >= 30:\n",
    "            atr = tr.rolling(30, min_periods=1).mean()\n",
    "            plus_dm = df['high'].diff()\n",
    "            minus_dm = -df['low'].diff()\n",
    "            plus_dm = plus_dm.where((plus_dm > minus_dm) & (plus_dm > 0), 0)\n",
    "            minus_dm = minus_dm.where((minus_dm > plus_dm) & (minus_dm > 0), 0)\n",
    "            plus_di = 100 * (plus_dm.rolling(30, min_periods=1).mean() / atr.replace(0, 1e-10))\n",
    "            minus_di = 100 * (minus_dm.rolling(30, min_periods=1).mean() / atr.replace(0, 1e-10))\n",
    "        else:\n",
    "            atr = tr.expanding(min_periods=1).mean()\n",
    "            plus_dm = df['high'].diff()\n",
    "            minus_dm = -df['low'].diff()\n",
    "            plus_dm = plus_dm.where((plus_dm > minus_dm) & (plus_dm > 0), 0)\n",
    "            minus_dm = minus_dm.where((minus_dm > plus_dm) & (minus_dm > 0), 0)\n",
    "            plus_di = 100 * (plus_dm.expanding(min_periods=1).mean() / atr.replace(0, 1e-10))\n",
    "            minus_di = 100 * (minus_dm.expanding(min_periods=1).mean() / atr.replace(0, 1e-10))\n",
    "        \n",
    "        df['dx_30'] = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di).replace(0, 1e-10)\n",
    "        \n",
    "        # SMAs\n",
    "        df['close_30_sma'] = df['close'].rolling(30, min_periods=1).mean()\n",
    "        df['close_60_sma'] = df['close'].rolling(60, min_periods=1).mean()\n",
    "        \n",
    "        # Fill any remaining NaN with safe defaults\n",
    "        df['macd'] = df['macd'].fillna(0)\n",
    "        df['boll_ub'] = df['boll_ub'].fillna(df['close'])\n",
    "        df['boll_lb'] = df['boll_lb'].fillna(df['close'])\n",
    "        df['rsi_30'] = df['rsi_30'].fillna(50)\n",
    "        df['cci_30'] = df['cci_30'].fillna(0)\n",
    "        df['dx_30'] = df['dx_30'].fillna(0)\n",
    "        df['close_30_sma'] = df['close_30_sma'].fillna(df['close'])\n",
    "        df['close_60_sma'] = df['close_60_sma'].fillna(df['close'])\n",
    "        \n",
    "        # Replace infinities\n",
    "        df = df.replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error calculating indicators: {e}\")\n",
    "        # Return with defaults if calculation fails\n",
    "        df['macd'] = 0.0\n",
    "        df['boll_ub'] = df.get('close', 0)\n",
    "        df['boll_lb'] = df.get('close', 0)\n",
    "        df['rsi_30'] = 50.0\n",
    "        df['cci_30'] = 0.0\n",
    "        df['dx_30'] = 0.0\n",
    "        df['close_30_sma'] = df.get('close', 0)\n",
    "        df['close_60_sma'] = df.get('close', 0)\n",
    "        return df\n",
    "\n",
    "\n",
    "def get_date_range_for_day(days_ago):\n",
    "    \"\"\"\n",
    "    Get start/end timestamps for a single trading day.\n",
    "    Market hours: 9:30 AM - 4:00 PM EST (14:30 - 21:00 UTC)\n",
    "    \"\"\"\n",
    "    now = datetime.utcnow()\n",
    "    target_date = now - timedelta(days=days_ago)\n",
    "    \n",
    "    # Market hours in UTC\n",
    "    start = target_date.replace(hour=14, minute=30, second=0, microsecond=0)\n",
    "    end = target_date.replace(hour=21, minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    start_str = start.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_str = end.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    date_str = target_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    return start_str, end_str, date_str\n",
    "\n",
    "\n",
    "def fetch_day_bars_alpaca(alpaca_api, symbol, start, end):\n",
    "    \"\"\"Fetch 1-minute bars for one symbol for one day using Alpaca REST API.\"\"\"\n",
    "    try:\n",
    "        # Use Alpaca's get_bars method\n",
    "        bars = alpaca_api.get_bars(\n",
    "            symbol,\n",
    "            '1Min',\n",
    "            start=start,\n",
    "            end=end,\n",
    "            limit=10000,\n",
    "            adjustment='split'\n",
    "        ).df\n",
    "        \n",
    "        if bars is None or len(bars) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Convert to records\n",
    "        bars = bars.reset_index()\n",
    "        bars.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'trade_count', 'vwap']\n",
    "        \n",
    "        records = []\n",
    "        for _, row in bars.iterrows():\n",
    "            records.append({\n",
    "                'timestamp': row['timestamp'],\n",
    "                'open': float(row['open']),\n",
    "                'high': float(row['high']),\n",
    "                'low': float(row['low']),\n",
    "                'close': float(row['close']),\n",
    "                'volume': int(row['volume'])\n",
    "            })\n",
    "        \n",
    "        return records\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Error fetching {symbol}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def fetch_historical_data_to_csv(alpaca_api, csv_path, required_trading_days=2, min_hours_per_day=4):\n",
    "    \"\"\"\n",
    "    Fetch historical data from Alpaca and populate CSV.\n",
    "    Ensures we have 2 complete trading days with at least 4 hours of data each.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ“¥ Fetching historical data for {required_trading_days} trading days...\")\n",
    "    print(f\"   Minimum: {min_hours_per_day} hours of data per day\")\n",
    "    \n",
    "    # Check if CSV already has sufficient data\n",
    "    if Path(csv_path).exists():\n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "        if len(existing_df) > 0:\n",
    "            existing_df['timestamp'] = pd.to_datetime(existing_df['timestamp'])\n",
    "            \n",
    "            # Check if we have enough trading days\n",
    "            existing_df['date'] = existing_df['timestamp'].dt.date\n",
    "            days_by_symbol = existing_df.groupby(['date', 'tic']).size().reset_index(name='bars')\n",
    "            \n",
    "            # Count days with at least min_hours worth of data\n",
    "            min_bars_needed = min_hours_per_day * 60\n",
    "            valid_days_per_symbol = days_by_symbol[days_by_symbol['bars'] >= min_bars_needed]\n",
    "            valid_days = valid_days_per_symbol['date'].value_counts()\n",
    "            \n",
    "            sufficient_days = (valid_days >= len(CONFIG['TICKERS']) * 0.8).sum()  # 80% of symbols\n",
    "            \n",
    "            if sufficient_days >= required_trading_days:\n",
    "                latest_timestamp = existing_df['timestamp'].max()\n",
    "                hours_old = (datetime.utcnow() - latest_timestamp).total_seconds() / 3600\n",
    "                \n",
    "                if hours_old < 24:  # Data is recent (less than 1 day old)\n",
    "                    print(f\"âœ“ CSV has {sufficient_days} trading days with sufficient data\")\n",
    "                    print(f\"   Latest data: {latest_timestamp} ({hours_old:.1f}h ago)\")\n",
    "                    print(f\"   Skipping historical fetch\")\n",
    "                    return\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nðŸ” Searching for {required_trading_days} trading days...\")\n",
    "        \n",
    "        trading_days_found = 0\n",
    "        days_searched = 0\n",
    "        max_search = 14  # Search up to 2 weeks back\n",
    "        all_historical_data = []\n",
    "        \n",
    "        while trading_days_found < required_trading_days and days_searched < max_search:\n",
    "            days_ago = days_searched + 1\n",
    "            start, end, date_str = get_date_range_for_day(days_ago)\n",
    "            \n",
    "            print(f\"\\n   ðŸ“… {date_str}: Checking...\")\n",
    "            \n",
    "            # Test first symbol to see if it's a trading day\n",
    "            test_bars = fetch_day_bars_alpaca(alpaca_api, CONFIG['TICKERS'][0], start, end)\n",
    "            \n",
    "            if not test_bars:\n",
    "                print(f\"      âŒ Non-trading day (weekend/holiday)\")\n",
    "                days_searched += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if we have minimum hours of data\n",
    "            min_bars_needed = min_hours_per_day * 60\n",
    "            if len(test_bars) < min_bars_needed:\n",
    "                print(f\"      âš ï¸  Partial day ({len(test_bars)} bars < {min_bars_needed})\")\n",
    "                # Still fetch but warn\n",
    "            \n",
    "            print(f\"      âœ… Trading day! Fetching {len(CONFIG['TICKERS'])} symbols...\")\n",
    "            \n",
    "            # Fetch all symbols for this day\n",
    "            day_data = []\n",
    "            for symbol in CONFIG['TICKERS']:\n",
    "                bars = fetch_day_bars_alpaca(alpaca_api, symbol, start, end)\n",
    "                if bars:\n",
    "                    for bar in bars:\n",
    "                        bar['tic'] = symbol\n",
    "                        day_data.append(bar)\n",
    "                    print(f\"         {symbol}: {len(bars)} bars\")\n",
    "            \n",
    "            if day_data:\n",
    "                all_historical_data.extend(day_data)\n",
    "                trading_days_found += 1\n",
    "                print(f\"      âœ“ Day {trading_days_found}/{required_trading_days} complete ({len(day_data)} total bars)\")\n",
    "            \n",
    "            days_searched += 1\n",
    "        \n",
    "        if not all_historical_data:\n",
    "            print(\"\\nâš ï¸  No historical data collected\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(all_historical_data)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values(['timestamp', 'tic']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Processing {len(df):,} bars for {len(df['tic'].unique())} symbols...\")\n",
    "        \n",
    "        # Calculate indicators per symbol\n",
    "        processed_data = []\n",
    "        for symbol in CONFIG['TICKERS']:\n",
    "            symbol_df = df[df['tic'] == symbol].copy()\n",
    "            if len(symbol_df) == 0:\n",
    "                continue\n",
    "            \n",
    "            symbol_df = calculate_indicators(symbol_df)\n",
    "            processed_data.append(symbol_df)\n",
    "            print(f\"   âœ“ {symbol}: {len(symbol_df)} bars with indicators\")\n",
    "        \n",
    "        if not processed_data:\n",
    "            print(\"âœ— No data processed\")\n",
    "            return\n",
    "        \n",
    "        # Combine all symbols\n",
    "        final_df = pd.concat(processed_data, ignore_index=True)\n",
    "        \n",
    "        # Select required columns\n",
    "        required_cols = ['timestamp', 'tic', 'open', 'high', 'low', 'close', 'volume'] + TECH_INDICATORS\n",
    "        final_df = final_df[required_cols]\n",
    "        \n",
    "        # Save to CSV\n",
    "        final_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"\\nâœ… SUCCESS: Saved {len(final_df):,} records to CSV\")\n",
    "        print(f\"   Date range: {final_df['timestamp'].min()} to {final_df['timestamp'].max()}\")\n",
    "        print(f\"   Trading days found: {trading_days_found}\")\n",
    "        \n",
    "        # Summary\n",
    "        unique_dates = final_df['timestamp'].dt.date.nunique()\n",
    "        records_per_ticker = len(final_df) // len(CONFIG['TICKERS'])\n",
    "        print(f\"   {unique_dates} unique dates, ~{records_per_ticker} bars per ticker\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— Failed to fetch historical data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def fetch_and_append_data(alpaca_api, csv_path):\n",
    "    \"\"\"\n",
    "    Fetch latest 1-min OHLCV data from Alpaca and append to CSV.\n",
    "    Uses live monitor logic with proper OHLCV bars, rounded to minute level.\n",
    "    Returns: (df_new, price, tech, turbulence)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get current time rounded to the minute\n",
    "        current_time = datetime.utcnow()\n",
    "        rounded_time = current_time.replace(second=0, microsecond=0)\n",
    "        \n",
    "        print(f\"ðŸ“¡ Fetching OHLCV bars for {len(CONFIG['TICKERS'])} tickers...\")\n",
    "        \n",
    "        # Fetch OHLCV bars for all tickers\n",
    "        all_bars = []\n",
    "        for ticker in CONFIG['TICKERS']:\n",
    "            try:\n",
    "                # Get latest bar from Alpaca\n",
    "                bar = alpaca_api.get_latest_bar(ticker)\n",
    "                \n",
    "                if bar:\n",
    "                    # Round bar timestamp to minute level\n",
    "                    bar_timestamp = bar.t\n",
    "                    if bar_timestamp.tzinfo is None:\n",
    "                        bar_timestamp = bar_timestamp.replace(tzinfo=timezone.utc)\n",
    "                    else:\n",
    "                        bar_timestamp = bar_timestamp.astimezone(timezone.utc)\n",
    "                    \n",
    "                    rounded_bar_time = bar_timestamp.replace(second=0, microsecond=0)\n",
    "                    \n",
    "                    all_bars.append({\n",
    "                        'timestamp': rounded_bar_time,\n",
    "                        'tic': ticker,\n",
    "                        'open': float(bar.o),\n",
    "                        'high': float(bar.h),\n",
    "                        'low': float(bar.l),\n",
    "                        'close': float(bar.c),\n",
    "                        'volume': int(bar.v)\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"   âš ï¸  No bar for {ticker}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âœ— Error fetching {ticker}: {e}\")\n",
    "        \n",
    "        if not all_bars:\n",
    "            print(\"âœ— No OHLCV data fetched\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df_new = pd.DataFrame(all_bars)\n",
    "        df_new['timestamp'] = pd.to_datetime(df_new['timestamp'])\n",
    "        \n",
    "        # Check if CSV exists and load existing data for indicator calculation\n",
    "        if Path(csv_path).exists():\n",
    "            existing_df = pd.read_csv(csv_path)\n",
    "            if len(existing_df) > 0:\n",
    "                existing_df['timestamp'] = pd.to_datetime(existing_df['timestamp'])\n",
    "                \n",
    "                # Check for duplicates (same timestamp and ticker)\n",
    "                last_timestamp = existing_df['timestamp'].max()\n",
    "                new_timestamp = df_new['timestamp'].iloc[0]\n",
    "                \n",
    "                if new_timestamp <= last_timestamp:\n",
    "                    print(f\"âš ï¸  Data already exists for {new_timestamp}, skipping append\")\n",
    "                    # Return existing values for trading\n",
    "                    price = df_new['close'].values\n",
    "                    return df_new, price, None, 0\n",
    "        \n",
    "        # Calculate technical indicators per symbol\n",
    "        print(f\"ðŸ“Š Calculating technical indicators...\")\n",
    "        processed_data = []\n",
    "        \n",
    "        # Load historical data for proper indicator calculation\n",
    "        if Path(csv_path).exists():\n",
    "            historical_df = pd.read_csv(csv_path)\n",
    "            historical_df['timestamp'] = pd.to_datetime(historical_df['timestamp'])\n",
    "        else:\n",
    "            historical_df = pd.DataFrame()\n",
    "        \n",
    "        for ticker in CONFIG['TICKERS']:\n",
    "            ticker_new = df_new[df_new['tic'] == ticker].copy()\n",
    "            if len(ticker_new) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Combine with historical data for this ticker\n",
    "            if len(historical_df) > 0:\n",
    "                ticker_historical = historical_df[historical_df['tic'] == ticker].copy()\n",
    "                ticker_combined = pd.concat([ticker_historical, ticker_new], ignore_index=True)\n",
    "                ticker_combined = ticker_combined.sort_values('timestamp').drop_duplicates(subset=['timestamp'], keep='last')\n",
    "                \n",
    "                # Keep last 500 bars for indicator calculation\n",
    "                ticker_combined = ticker_combined.tail(500)\n",
    "            else:\n",
    "                ticker_combined = ticker_new\n",
    "            \n",
    "            # Calculate indicators\n",
    "            ticker_combined = calculate_indicators(ticker_combined)\n",
    "            \n",
    "            # Get only the new row with indicators\n",
    "            new_row = ticker_combined[ticker_combined['timestamp'] == ticker_new['timestamp'].iloc[0]]\n",
    "            processed_data.append(new_row)\n",
    "        \n",
    "        if not processed_data:\n",
    "            print(\"âœ— No data processed\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # Combine all tickers\n",
    "        final_df = pd.concat(processed_data, ignore_index=True)\n",
    "        \n",
    "        # Select required columns in correct order\n",
    "        required_cols = ['timestamp', 'tic', 'open', 'high', 'low', 'close', 'volume'] + TECH_INDICATORS\n",
    "        final_df = final_df[required_cols]\n",
    "        \n",
    "        # Append to CSV\n",
    "        final_df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "        print(f\"âœ“ Appended {len(final_df)} records at {rounded_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
    "        \n",
    "        # Extract price and tech for trading (format compatible with existing code)\n",
    "        price = final_df['close'].values\n",
    "        \n",
    "        # Build tech array (all indicators for all tickers, flattened)\n",
    "        tech_cols = TECH_INDICATORS\n",
    "        tech_data = []\n",
    "        for ticker in CONFIG['TICKERS']:\n",
    "            ticker_row = final_df[final_df['tic'] == ticker]\n",
    "            if len(ticker_row) > 0:\n",
    "                for col in tech_cols:\n",
    "                    tech_data.append(ticker_row[col].values[0])\n",
    "            else:\n",
    "                tech_data.extend([0] * len(tech_cols))\n",
    "        \n",
    "        tech = np.array(tech_data)\n",
    "        turbulence = 0  # Placeholder, can calculate if needed\n",
    "        \n",
    "        return final_df, price, tech, turbulence\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Data fetch error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def load_recent_data(csv_path, hours=48):\n",
    "    \"\"\"Load last N hours of data from CSV for fine-tuning.\"\"\"\n",
    "    if not Path(csv_path).exists():\n",
    "        print(f\"âš ï¸  CSV not found: {csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"âš ï¸  CSV is empty\")\n",
    "        return None\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Get last N hours\n",
    "    cutoff = datetime.utcnow() - timedelta(hours=hours)\n",
    "    df_filtered = df[df['timestamp'] >= cutoff]\n",
    "    \n",
    "    df_filtered = df_filtered.sort_values(['timestamp', 'tic']).reset_index(drop=True)\n",
    "    \n",
    "    # Show data availability\n",
    "    if len(df_filtered) < 100:\n",
    "        hours_available = (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600\n",
    "        print(f\"âš ï¸  Only {len(df_filtered)} records in {hours}h window (total available: {hours_available:.1f}h)\")\n",
    "    else:\n",
    "        print(f\"âœ“ Loaded {len(df_filtered):,} records from CSV ({hours}h window)\")\n",
    "        unique_timestamps = df_filtered['timestamp'].nunique()\n",
    "        print(f\"   {unique_timestamps} unique timestamps\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "print(\"âœ“ Data collection functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be2826",
   "metadata": {},
   "source": [
    "# Part 4: Environment Creation for Fine-Tuning\n",
    "\n",
    "Create StockTradingEnv from DataFrame for model training/evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env_from_df(df, config):\n",
    "    \"\"\"\n",
    "    Create StockTradingEnv from DataFrame (Production format: 301 features).\n",
    "    State: 1 (cash) + 30 (prices) + 30 (stocks) + 240 (tech indicators) = 301\n",
    "    \"\"\"\n",
    "    # Create day index\n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values(['timestamp', 'tic']).reset_index(drop=True)\n",
    "    \n",
    "    unique_dates = sorted(df['timestamp'].unique())\n",
    "    date_to_day = {date: idx for idx, date in enumerate(unique_dates)}\n",
    "    df['day'] = df['timestamp'].map(date_to_day)\n",
    "    df = df.rename(columns={'timestamp': 'date'})\n",
    "    \n",
    "    # Set index for StockTradingEnv\n",
    "    df_indexed = df.sort_values(['day', 'tic']).set_index('day')\n",
    "    \n",
    "    state_space = 1 + 2 * config['STOCK_DIM'] + len(TECH_INDICATORS) * config['STOCK_DIM']\n",
    "    \n",
    "    env = StockTradingEnv(\n",
    "        df=df_indexed,\n",
    "        stock_dim=config['STOCK_DIM'],\n",
    "        hmax=config['HMAX'],\n",
    "        initial_amount=config['INITIAL_CASH'],\n",
    "        num_stock_shares=[0] * config['STOCK_DIM'],\n",
    "        buy_cost_pct=[config['TRANSACTION_COST_PCT']] * config['STOCK_DIM'],\n",
    "        sell_cost_pct=[config['TRANSACTION_COST_PCT']] * config['STOCK_DIM'],\n",
    "        reward_scaling=config['REWARD_SCALING'],\n",
    "        state_space=state_space,\n",
    "        action_space=config['STOCK_DIM'],\n",
    "        tech_indicator_list=TECH_INDICATORS,\n",
    "        print_verbosity=100000,\n",
    "    )\n",
    "    \n",
    "    return DummyVecEnv([lambda: env])\n",
    "\n",
    "\n",
    "def evaluate_model_on_df(model, df, config):\n",
    "    \"\"\"Evaluate SB3 PPO model performance on DataFrame.\"\"\"\n",
    "    env = create_env_from_df(df, config)\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward[0]\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "print(\"âœ“ Environment creation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13075f12",
   "metadata": {},
   "source": [
    "# Part 5: Fine-Tuning Logic with Validation\n",
    "\n",
    "Fine-tune SB3 PPO model with validation and rollback (from finetune_simulation.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model_with_csv_data(model, csv_path, config):\n",
    "    \"\"\"\n",
    "    Fine-tune SB3 PPO model using data from CSV (from finetune_simulation.py logic).\n",
    "    Returns: (finetuned_model, result_dict)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINE-TUNING MODEL WITH CSV DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load data\n",
    "    df = load_recent_data(csv_path, hours=config['FINETUNE_LOOKBACK_HOURS'])\n",
    "    \n",
    "    if df is None or len(df) < 100:\n",
    "        print(\"âœ— Insufficient data for fine-tuning\")\n",
    "        return model, None\n",
    "    \n",
    "    # Split train/validation\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    unique_dates = sorted(df['timestamp'].unique())\n",
    "    \n",
    "    if len(unique_dates) < 2:\n",
    "        print(\"âœ— Need at least 2 unique timestamps\")\n",
    "        return model, None\n",
    "    \n",
    "    split_idx = int(len(unique_dates) * (1 - config['VALIDATION_SPLIT']))\n",
    "    \n",
    "    train_df = df[df['timestamp'].isin(unique_dates[:split_idx])].copy()\n",
    "    val_df = df[df['timestamp'].isin(unique_dates[split_idx:])].copy()\n",
    "    \n",
    "    print(f\"ðŸ“Š Train: {len(train_df):,} records ({len(unique_dates[:split_idx])} timestamps)\")\n",
    "    print(f\"ðŸ“Š Val: {len(val_df):,} records ({len(unique_dates[split_idx:])} timestamps)\")\n",
    "    \n",
    "    # Evaluate original model\n",
    "    print(\"ðŸ§ª Evaluating original model...\")\n",
    "    original_score = evaluate_model_on_df(model, val_df, config)\n",
    "    print(f\"   Original score: {original_score:.2f}\")\n",
    "    \n",
    "    # Clone model (SB3 PPO)\n",
    "    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp:\n",
    "        tmp_path = tmp.name\n",
    "        model.save(tmp_path)\n",
    "        model_ft = PPO.load(tmp_path)\n",
    "    os.remove(tmp_path)\n",
    "    \n",
    "    # Fine-tune\n",
    "    print(f\"ðŸ”„ Fine-tuning ({config['FINETUNE_STEPS']} steps, lr={config['FINETUNE_LR']})...\")\n",
    "    model_ft.learning_rate = config['FINETUNE_LR']\n",
    "    ft_env = create_env_from_df(train_df, config)\n",
    "    model_ft.set_env(ft_env)\n",
    "    model_ft.learn(\n",
    "        total_timesteps=config['FINETUNE_STEPS'],\n",
    "        reset_num_timesteps=False,\n",
    "        progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate fine-tuned\n",
    "    print(\"ðŸ§ª Evaluating fine-tuned model...\")\n",
    "    finetuned_score = evaluate_model_on_df(model_ft, val_df, config)\n",
    "    print(f\"   Fine-tuned score: {finetuned_score:.2f}\")\n",
    "    \n",
    "    # Decision (from finetune_simulation.py)\n",
    "    threshold = original_score * config['VALIDATION_THRESHOLD']\n",
    "    accepted = finetuned_score >= threshold\n",
    "    \n",
    "    improvement = ((finetuned_score - original_score) / abs(original_score) * 100) if original_score != 0 else 0\n",
    "    \n",
    "    result = {\n",
    "        'timestamp': datetime.utcnow(),\n",
    "        'original_score': original_score,\n",
    "        'finetuned_score': finetuned_score,\n",
    "        'threshold': threshold,\n",
    "        'accepted': accepted,\n",
    "        'improvement': improvement,\n",
    "        'train_records': len(train_df),\n",
    "        'val_records': len(val_df),\n",
    "    }\n",
    "    \n",
    "    if accepted:\n",
    "        print(f\"âœ… ACCEPTED (+{improvement:.2f}%)\")\n",
    "        return model_ft, result\n",
    "    else:\n",
    "        print(f\"âŒ REJECTED ({improvement:.2f}%)\")\n",
    "        return model, result\n",
    "\n",
    "\n",
    "print(\"âœ“ Fine-tuning functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b04b8c",
   "metadata": {},
   "source": [
    "# Part 6: Paper Trading Class with Real-Time Fine-Tuning\n",
    "\n",
    "AlpacaPaperTrading class merged with fine-tuning logic (from finrl-papertrading-demo.ipynb + realtime_finetune.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5baaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlpacaPaperTradingWithFineTuning:\n",
    "    \"\"\"\n",
    "    Enhanced Alpaca Paper Trading class with real-time fine-tuning.\n",
    "    Combines logic from:\n",
    "    - finrl-papertrading-demo.ipynb (AlpacaPaperTrading class)\n",
    "    - realtime_finetune.py (data collection + fine-tuning)\n",
    "    - finetune_simulation.py (validation logic)\n",
    "    - historical_data.py (robust data fetching)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, model_path):\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize Alpaca\n",
    "        self.alpaca_processor = AlpacaProcessor(\n",
    "            API_KEY=config['API_KEY'],\n",
    "            API_SECRET=config['API_SECRET'],\n",
    "            API_BASE_URL=config['API_BASE_URL']\n",
    "        )\n",
    "        \n",
    "        self.alpaca = tradeapi.REST(\n",
    "            config['API_KEY'],\n",
    "            config['API_SECRET'],\n",
    "            config['API_BASE_URL'],\n",
    "            'v2'\n",
    "        )\n",
    "        \n",
    "        # Load SB3 PPO model\n",
    "        print(f\"ðŸ¤– Loading SB3 PPO model: {model_path}\")\n",
    "        self.model = PPO.load(model_path)\n",
    "        print(\"âœ“ Model loaded\")\n",
    "        \n",
    "        # Initialize state (from finrl-papertrading-demo.ipynb)\n",
    "        self.tickers = config['TICKERS']\n",
    "        self.stocks = np.zeros(config['STOCK_DIM'])\n",
    "        self.stocks_cd = np.zeros(config['STOCK_DIM'])\n",
    "        self.cash = None\n",
    "        self.price = np.zeros(config['STOCK_DIM'])\n",
    "        self.turbulence_bool = 0\n",
    "        \n",
    "        # Fine-tuning tracking\n",
    "        self.last_finetune = datetime.utcnow() - timedelta(hours=config['FINETUNE_INTERVAL_HOURS'])\n",
    "        self.finetune_history = []\n",
    "        self.trading_history = []\n",
    "        self.cycle = 0\n",
    "        self.model_version = 'original'  # Track if using original or fine-tuned model\n",
    "        self.finetune_count = 0  # Count of accepted fine-tunes\n",
    "        \n",
    "        # Initialize data CSV and fetch historical data\n",
    "        print(\"\\nðŸ“Š Initializing data collection system...\")\n",
    "        init_data_csv(config['DATA_CSV'])\n",
    "        \n",
    "        # Fetch 2 trading days of historical data for fine-tuning\n",
    "        # Uses robust logic from historical_data.py\n",
    "        fetch_historical_data_to_csv(\n",
    "            self.alpaca,  # Pass Alpaca REST API\n",
    "            config['DATA_CSV'],\n",
    "            required_trading_days=2,\n",
    "            min_hours_per_day=4\n",
    "        )\n",
    "        \n",
    "        print(\"âœ“ Paper trading instance initialized\")\n",
    "    \n",
    "    def sigmoid_sign(self, ary, thresh):\n",
    "        \"\"\"Sigmoid transformation for turbulence (from finrl-papertrading-demo.ipynb).\"\"\"\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x * np.e)) - 0.5\n",
    "        return sigmoid(ary / thresh) * thresh\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get current state from Alpaca API (Production format: 301 features).\n",
    "        \n",
    "        State vector: [cash(1)] + [prices(30)] + [stocks(30)] + [tech_indicators(240)]\n",
    "        NO turbulence in state vector!\n",
    "        \n",
    "        Returns state vector with current prices, holdings, and technical indicators.\n",
    "        \"\"\"\n",
    "        # Fetch latest OHLCV data and append to CSV\n",
    "        df_new, price, tech, turbulence = fetch_and_append_data(\n",
    "            self.alpaca,  # Pass Alpaca REST API for OHLCV bars\n",
    "            self.config['DATA_CSV']\n",
    "        )\n",
    "        \n",
    "        if price is None:\n",
    "            print(\"âš ï¸  Failed to fetch data, using cached values\")\n",
    "            price = self.price\n",
    "            turbulence = 0\n",
    "            tech = np.zeros(len(TECH_INDICATORS) * self.config['STOCK_DIM'])\n",
    "        \n",
    "        # Calculate turbulence boolean (used for trading logic, NOT in state)\n",
    "        turbulence_bool = 1 if turbulence >= self.config['TURBULENCE_THRESH'] else 0\n",
    "        \n",
    "        # Scale technical indicators\n",
    "        tech_scaled = tech * 2 ** -7\n",
    "        \n",
    "        # Get current positions from Alpaca\n",
    "        positions = self.alpaca.list_positions()\n",
    "        stocks = np.zeros(len(self.tickers))\n",
    "        for position in positions:\n",
    "            if position.symbol in self.tickers:\n",
    "                ind = self.tickers.index(position.symbol)\n",
    "                stocks[ind] = abs(int(float(position.qty)))\n",
    "        \n",
    "        # Update instance variables\n",
    "        self.stocks = stocks\n",
    "        self.price = price\n",
    "        self.turbulence_bool = turbulence_bool\n",
    "        self.cash = float(self.alpaca.get_account().cash)\n",
    "        \n",
    "        # Build state vector (Production format: NO turbulence!)\n",
    "        # Model expects: 1 (cash) + 30 (prices) + 30 (stocks) + 240 (tech) = 301 features\n",
    "        amount = np.array(self.cash * (2 ** -12), dtype=np.float32)\n",
    "        scale = np.array(2 ** -6, dtype=np.float32)\n",
    "        \n",
    "        state = np.hstack((\n",
    "            amount,\n",
    "            price * scale,\n",
    "            stocks * scale,\n",
    "            tech_scaled,\n",
    "        )).astype(np.float32)\n",
    "        \n",
    "        # Handle NaN/Inf\n",
    "        state[np.isnan(state)] = 0.0\n",
    "        state[np.isinf(state)] = 0.0\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def submit_order(self, qty, stock, side, resp):\n",
    "        \"\"\"Submit order to Alpaca (from finrl-papertrading-demo.ipynb).\"\"\"\n",
    "        if qty > 0:\n",
    "            try:\n",
    "                self.alpaca.submit_order(stock, qty, side, \"market\", \"day\")\n",
    "                print(f\"   âœ“ {side.upper()} {qty} {stock}\")\n",
    "                resp.append(True)\n",
    "            except Exception as e:\n",
    "                print(f\"   âœ— {side.upper()} {qty} {stock} failed: {e}\")\n",
    "                resp.append(False)\n",
    "        else:\n",
    "            resp.append(True)\n",
    "    \n",
    "    def trade(self):\n",
    "        \"\"\"\n",
    "        Execute trading decision using SB3 PPO model.\n",
    "        From finrl-papertrading-demo.ipynb trade() logic.\n",
    "        \"\"\"\n",
    "        state = self.get_state()\n",
    "        \n",
    "        # Get action from SB3 PPO model\n",
    "        action = self.model.predict(state.reshape(1, -1), deterministic=True)[0]\n",
    "        action = action.flatten()\n",
    "        \n",
    "        # Scale actions\n",
    "        action = (action * self.config['HMAX']).astype(int)\n",
    "        \n",
    "        # Log action statistics\n",
    "        print(f\"\\nðŸ“Š Action Stats:\")\n",
    "        print(f\"   Range: [{np.min(action):.2f}, {np.max(action):.2f}]\")\n",
    "        print(f\"   Mean(abs): {np.mean(np.abs(action)):.2f}\")\n",
    "        print(f\"   ðŸ’µ Cash: ${self.cash:,.2f}, Turbulence: {self.turbulence_bool}\")\n",
    "        \n",
    "        # Display signals\n",
    "        min_action = self.config['MIN_ACTION_THRESHOLD']\n",
    "        sell_signals = [(self.tickers[i], action[i]) for i in range(len(action)) if action[i] < -min_action]\n",
    "        buy_signals = [(self.tickers[i], action[i]) for i in range(len(action)) if action[i] > min_action]\n",
    "        \n",
    "        print(f\"   ðŸ“‰ SELL ({len(sell_signals)}): {sell_signals[:5]}\")\n",
    "        print(f\"   ðŸ“ˆ BUY ({len(buy_signals)}): {buy_signals[:5]}\")\n",
    "        \n",
    "        # Execute trades (from finrl-papertrading-demo.ipynb)\n",
    "        self.stocks_cd += 1\n",
    "        \n",
    "        if self.turbulence_bool == 0:\n",
    "            threads = []\n",
    "            \n",
    "            # SELL orders\n",
    "            for index in np.where(action < -min_action)[0]:\n",
    "                sell_num_shares = min(self.stocks[index], -action[index])\n",
    "                qty = abs(int(sell_num_shares))\n",
    "                respSO = []\n",
    "                t = threading.Thread(\n",
    "                    target=lambda q=qty, s=self.tickers[index]: self.submit_order(q, s, 'sell', respSO)\n",
    "                )\n",
    "                t.start()\n",
    "                threads.append(t)\n",
    "                self.stocks_cd[index] = 0\n",
    "            \n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            \n",
    "            # Update cash\n",
    "            self.cash = float(self.alpaca.get_account().cash)\n",
    "            \n",
    "            threads = []\n",
    "            \n",
    "            # BUY orders\n",
    "            for index in np.where(action > min_action)[0]:\n",
    "                tmp_cash = max(0, self.cash)\n",
    "                buy_num_shares = min(tmp_cash // self.price[index], abs(int(action[index])))\n",
    "                qty = abs(int(buy_num_shares)) if not np.isnan(buy_num_shares) else 0\n",
    "                respSO = []\n",
    "                t = threading.Thread(\n",
    "                    target=lambda q=qty, s=self.tickers[index]: self.submit_order(q, s, 'buy', respSO)\n",
    "                )\n",
    "                t.start()\n",
    "                threads.append(t)\n",
    "                self.stocks_cd[index] = 0\n",
    "            \n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            \n",
    "        else:\n",
    "            # High turbulence - liquidate all positions\n",
    "            print(\"âš ï¸  HIGH TURBULENCE - Liquidating all positions\")\n",
    "            threads = []\n",
    "            positions = self.alpaca.list_positions()\n",
    "            \n",
    "            for position in positions:\n",
    "                side = 'sell' if position.side == 'long' else 'buy'\n",
    "                qty = abs(int(float(position.qty)))\n",
    "                respSO = []\n",
    "                t = threading.Thread(\n",
    "                    target=lambda q=qty, sym=position.symbol, s=side: self.submit_order(q, sym, s, respSO)\n",
    "                )\n",
    "                t.start()\n",
    "                threads.append(t)\n",
    "            \n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            \n",
    "            self.stocks_cd[:] = 0\n",
    "        \n",
    "        # Log trade\n",
    "        portfolio_value = self.cash + np.sum(self.stocks * self.price)\n",
    "        trade_log = {\n",
    "            'timestamp': datetime.utcnow(),\n",
    "            'cycle': self.cycle,\n",
    "            'portfolio_value': portfolio_value,\n",
    "            'cash': self.cash,\n",
    "            'action': action.tolist(),\n",
    "        }\n",
    "        self.trading_history.append(trade_log)\n",
    "    \n",
    "    def check_and_finetune(self):\n",
    "        \"\"\"\n",
    "        Check if it's time to fine-tune and execute if needed.\n",
    "        From realtime_finetune.py fine-tuning logic.\n",
    "        \"\"\"\n",
    "        current_time = datetime.utcnow()\n",
    "        time_since_finetune = (current_time - self.last_finetune).total_seconds() / 3600\n",
    "        \n",
    "        if time_since_finetune >= self.config['FINETUNE_INTERVAL_HOURS']:\n",
    "            print(f\"\\nâ° Time to fine-tune (last: {time_since_finetune:.1f}h ago)\")\n",
    "            \n",
    "            self.model, ft_result = finetune_model_with_csv_data(\n",
    "                self.model,\n",
    "                self.config['DATA_CSV'],\n",
    "                self.config\n",
    "            )\n",
    "            \n",
    "            if ft_result:\n",
    "                self.finetune_history.append(ft_result)\n",
    "                self.last_finetune = current_time\n",
    "                \n",
    "                # Save results\n",
    "                output_dir = Path(self.config['OUTPUT_DIR'])\n",
    "                output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                pd.DataFrame(self.finetune_history).to_csv(\n",
    "                    output_dir / 'finetune_history.csv',\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                if ft_result['accepted']:\n",
    "                    self.finetune_count += 1\n",
    "                    self.model_version = f'finetuned_v{self.finetune_count}'\n",
    "                    model_path = output_dir / f'model_cycle_{self.cycle}.zip'\n",
    "                    self.model.save(str(model_path))\n",
    "                    print(f\"ðŸ’¾ Saved fine-tuned model: {model_path}\")\n",
    "    \n",
    "    def square_off_all_positions(self):\n",
    "        \"\"\"Liquidate all positions before market close.\"\"\"\n",
    "        print(\"\\nðŸ”š Squaring off all positions before market close...\")\n",
    "        positions = self.alpaca.list_positions()\n",
    "        \n",
    "        if len(positions) == 0:\n",
    "            print(\"   No positions to square off\")\n",
    "            return\n",
    "        \n",
    "        threads = []\n",
    "        for position in positions:\n",
    "            side = 'sell' if position.side == 'long' else 'buy'\n",
    "            qty = abs(int(float(position.qty)))\n",
    "            respSO = []\n",
    "            print(f\"   Closing {qty} {position.symbol} ({side})\")\n",
    "            t = threading.Thread(\n",
    "                target=lambda q=qty, sym=position.symbol, s=side: self.submit_order(q, sym, s, respSO)\n",
    "            )\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "        \n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        print(\"âœ“ All positions squared off\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Main trading loop with fine-tuning.\n",
    "        From finrl-papertrading-demo.ipynb + realtime_finetune.py.\n",
    "        \"\"\"\n",
    "        # Wait for market to open\n",
    "        clock = self.alpaca.get_clock()\n",
    "        if not clock.is_open:\n",
    "            time_to_open = (clock.next_open.replace(tzinfo=timezone.utc) - clock.timestamp.replace(tzinfo=timezone.utc)).total_seconds()\n",
    "            print(f\"â° Market closed - waiting {int(time_to_open/60)} minutes for market open...\")\n",
    "            time.sleep(time_to_open)\n",
    "        \n",
    "        # Wait 15 minutes after market open before first trade\n",
    "        clock = self.alpaca.get_clock()\n",
    "        print(f\"âœ… Market opened - waiting 15 minutes before first trade...\")\n",
    "        time.sleep(15 * 60)  # Wait 15 minutes\n",
    "        \n",
    "        print(f\"âœ… Starting paper trading with fine-tuning (Model: {self.model_version})\")\n",
    "        \n",
    "        output_dir = Path(self.config['OUTPUT_DIR'])\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                self.cycle += 1\n",
    "                \n",
    "                # Check market status\n",
    "                clock = self.alpaca.get_clock()\n",
    "                closing_time = clock.next_close.replace(tzinfo=timezone.utc).timestamp()\n",
    "                curr_time = clock.timestamp.replace(tzinfo=timezone.utc).timestamp()\n",
    "                time_to_close = closing_time - curr_time\n",
    "                \n",
    "                # Square off positions 15 minutes before market close\n",
    "                if time_to_close < (15 * 60):  # 15 minutes = 900 seconds\n",
    "                    self.square_off_all_positions()\n",
    "                    print(\"ðŸ”š Market closing in <15 mins - stopping trading\")\n",
    "                    break\n",
    "                \n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"CYCLE {self.cycle} - {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                print(f\"Model: {self.model_version} | Time to close: {int(time_to_close/60)} mins\")\n",
    "                print(f\"{'='*80}\")\n",
    "                \n",
    "                # Execute trade\n",
    "                self.trade()\n",
    "                \n",
    "                # Check and fine-tune if needed\n",
    "                self.check_and_finetune()\n",
    "                \n",
    "                # Save trading history\n",
    "                pd.DataFrame(self.trading_history).to_csv(\n",
    "                    output_dir / 'trading_history.csv',\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                # Wait for next interval (60 seconds)\n",
    "                time.sleep(60)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nâš ï¸  Interrupted by user\")\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TRADING SESSION SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total cycles: {self.cycle}\")\n",
    "        print(f\"Trading decisions: {len(self.trading_history)}\")\n",
    "        print(f\"Fine-tuning sessions: {len(self.finetune_history)}\")\n",
    "        \n",
    "        if self.trading_history:\n",
    "            final_value = self.trading_history[-1]['portfolio_value']\n",
    "            initial_value = self.trading_history[0]['portfolio_value']\n",
    "            total_return = (final_value - initial_value) / initial_value * 100\n",
    "            print(f\"\\nðŸ’° Portfolio Performance:\")\n",
    "            print(f\"   Initial: ${initial_value:,.2f}\")\n",
    "            print(f\"   Final: ${final_value:,.2f}\")\n",
    "            print(f\"   Return: {total_return:+.2f}%\")\n",
    "        \n",
    "        if self.finetune_history:\n",
    "            accepted = sum(1 for r in self.finetune_history if r['accepted'])\n",
    "            print(f\"\\nðŸ”„ Fine-tuning:\")\n",
    "            print(f\"   Accepted: {accepted}/{len(self.finetune_history)}\")\n",
    "            avg_improvement = np.mean([r['improvement'] for r in self.finetune_history])\n",
    "            print(f\"   Avg improvement: {avg_improvement:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Results saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "print(\"âœ“ AlpacaPaperTradingWithFineTuning class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd0839",
   "metadata": {},
   "source": [
    "# Part 7: Run Paper Trading Loop\n",
    "\n",
    "Initialize the trading system and start the paper trading loop with automatic fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paper trading with fine-tuning\n",
    "trader = AlpacaPaperTradingWithFineTuning(\n",
    "    config=CONFIG,\n",
    "    model_path=CONFIG['TRAINED_MODEL']\n",
    ")\n",
    "\n",
    "# Start trading loop\n",
    "print(\"\\nðŸš€ Starting Alpaca paper trading with real-time fine-tuning\")\n",
    "print(f\"   Fine-tune interval: {CONFIG['FINETUNE_INTERVAL_HOURS']} hours\")\n",
    "print(f\"   Lookback window: {CONFIG['FINETUNE_LOOKBACK_HOURS']} hours\")\n",
    "print(f\"   Acceptance threshold: {CONFIG['VALIDATION_THRESHOLD']*100}%\")\n",
    "\n",
    "trader.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7c841",
   "metadata": {},
   "source": [
    "# Part 8: Performance Analysis\n",
    "\n",
    "Visualize trading performance and fine-tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "output_dir = Path(CONFIG['OUTPUT_DIR'])\n",
    "trading_history_path = output_dir / 'trading_history.csv'\n",
    "finetune_history_path = output_dir / 'finetune_history.csv'\n",
    "\n",
    "if trading_history_path.exists():\n",
    "    df_trading = pd.read_csv(trading_history_path)\n",
    "    df_trading['timestamp'] = pd.to_datetime(df_trading['timestamp'])\n",
    "    \n",
    "    # Plot portfolio value\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Portfolio value over time\n",
    "    axes[0].plot(df_trading['timestamp'], df_trading['portfolio_value'], linewidth=2, color='blue')\n",
    "    axes[0].set_title('Portfolio Value Over Time', fontsize=16, fontweight='bold')\n",
    "    axes[0].set_xlabel('Time', fontsize=12)\n",
    "    axes[0].set_ylabel('Portfolio Value ($)', fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))\n",
    "    \n",
    "    # Cash vs holdings value\n",
    "    df_trading['holdings_value'] = df_trading['portfolio_value'] - df_trading['cash']\n",
    "    axes[1].plot(df_trading['timestamp'], df_trading['cash'], label='Cash', linewidth=2, color='green')\n",
    "    axes[1].plot(df_trading['timestamp'], df_trading['holdings_value'], label='Holdings', linewidth=2, color='orange')\n",
    "    axes[1].set_title('Cash vs Holdings Value', fontsize=16, fontweight='bold')\n",
    "    axes[1].set_xlabel('Time', fontsize=12)\n",
    "    axes[1].set_ylabel('Value ($)', fontsize=12)\n",
    "    axes[1].legend(fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'trading_performance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance metrics\n",
    "    initial_value = df_trading['portfolio_value'].iloc[0]\n",
    "    final_value = df_trading['portfolio_value'].iloc[-1]\n",
    "    total_return = (final_value - initial_value) / initial_value * 100\n",
    "    max_value = df_trading['portfolio_value'].max()\n",
    "    min_value = df_trading['portfolio_value'].min()\n",
    "    volatility = df_trading['portfolio_value'].pct_change().std() * np.sqrt(252 * 6.5 * 60)  # Annualized\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Initial Portfolio Value: ${initial_value:,.2f}\")\n",
    "    print(f\"Final Portfolio Value:   ${final_value:,.2f}\")\n",
    "    print(f\"Total Return:            {total_return:+.2f}%\")\n",
    "    print(f\"Max Value:               ${max_value:,.2f}\")\n",
    "    print(f\"Min Value:               ${min_value:,.2f}\")\n",
    "    print(f\"Volatility (annualized): {volatility:.2%}\")\n",
    "    print(f\"Total Cycles:            {len(df_trading)}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"âš ï¸  No trading history found\")\n",
    "\n",
    "# Load and visualize fine-tuning results\n",
    "if finetune_history_path.exists():\n",
    "    df_finetune = pd.read_csv(finetune_history_path)\n",
    "    df_finetune['timestamp'] = pd.to_datetime(df_finetune['timestamp'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Performance improvement\n",
    "    colors = ['green' if x else 'red' for x in df_finetune['accepted']]\n",
    "    axes[0].bar(range(len(df_finetune)), df_finetune['improvement'], color=colors, alpha=0.7)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    axes[0].set_title('Fine-Tuning Performance Improvement', fontsize=16, fontweight='bold')\n",
    "    axes[0].set_xlabel('Fine-Tuning Session', fontsize=12)\n",
    "    axes[0].set_ylabel('Improvement (%)', fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Acceptance rate\n",
    "    cumulative_accepted = df_finetune['accepted'].cumsum()\n",
    "    cumulative_total = range(1, len(df_finetune) + 1)\n",
    "    acceptance_rate = [a/t * 100 for a, t in zip(cumulative_accepted, cumulative_total)]\n",
    "    \n",
    "    axes[1].plot(cumulative_total, acceptance_rate, marker='o', linewidth=2, markersize=8, color='purple')\n",
    "    axes[1].set_title('Cumulative Fine-Tuning Acceptance Rate', fontsize=16, fontweight='bold')\n",
    "    axes[1].set_xlabel('Fine-Tuning Session', fontsize=12)\n",
    "    axes[1].set_ylabel('Acceptance Rate (%)', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'finetune_performance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Fine-tuning summary\n",
    "    accepted_count = df_finetune['accepted'].sum()\n",
    "    total_count = len(df_finetune)\n",
    "    avg_improvement = df_finetune['improvement'].mean()\n",
    "    max_improvement = df_finetune['improvement'].max()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINE-TUNING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Sessions:       {total_count}\")\n",
    "    print(f\"Accepted:             {accepted_count} ({accepted_count/total_count*100:.1f}%)\")\n",
    "    print(f\"Rejected:             {total_count - accepted_count}\")\n",
    "    print(f\"Avg Improvement:      {avg_improvement:+.2f}%\")\n",
    "    print(f\"Max Improvement:      {max_improvement:+.2f}%\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"âš ï¸  No fine-tuning history found\")\n",
    "\n",
    "print(f\"\\nâœ“ All results saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
